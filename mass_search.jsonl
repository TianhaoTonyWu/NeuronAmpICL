{"task": "task1356_xlsum_title_generation.json", "shot": 0, "num": 3064, "correct": 0, "acc": 0.0}
{"task": "task1356_xlsum_title_generation.json", "shot": 5, "num": 3064, "correct": 3, "acc": 0.0009791122715404699}
{"task": "task893_gap_fill_the_blank_coreference_resolution.json", "shot": 0, "num": 50, "correct": 0, "acc": 0.0}
{"task": "task893_gap_fill_the_blank_coreference_resolution.json", "shot": 5, "num": 50, "correct": 29, "acc": 0.58}
{"task": "task641_esnli_classification.json", "shot": 0, "num": 99, "correct": 0, "acc": 0.0}
{"task": "task641_esnli_classification.json", "shot": 5, "num": 99, "correct": 32, "acc": 0.32323232323232326}
{"task": "task1529_scitail1.1_classification.json", "shot": 0, "num": 2530, "correct": 0, "acc": 0.0}
{"task": "task1529_scitail1.1_classification.json", "shot": 5, "num": 2530, "correct": 1612, "acc": 0.6371541501976284}
{"task": "task202_mnli_contradiction_classification.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task202_mnli_contradiction_classification.json", "shot": 5, "num": 3249, "correct": 740, "acc": 0.22776238842720836}
{"task": "task670_ambigqa_question_generation.json", "shot": 0, "num": 2375, "correct": 0, "acc": 0.0}
{"task": "task670_ambigqa_question_generation.json", "shot": 5, "num": 2375, "correct": 0, "acc": 0.0}
{"task": "task1393_superglue_copa_text_completion.json", "shot": 0, "num": 248, "correct": 56, "acc": 0.22580645161290322}
{"task": "task1393_superglue_copa_text_completion.json", "shot": 5, "num": 248, "correct": 144, "acc": 0.5806451612903226}
{"task": "task1344_glue_entailment_classification.json", "shot": 0, "num": 1240, "correct": 0, "acc": 0.0}
{"task": "task1344_glue_entailment_classification.json", "shot": 5, "num": 1240, "correct": 785, "acc": 0.6330645161290323}
{"task": "task288_gigaword_summarization.json", "shot": 0, "num": 967, "correct": 0, "acc": 0.0}
{"task": "task288_gigaword_summarization.json", "shot": 5, "num": 967, "correct": 2, "acc": 0.002068252326783868}
{"task": "task1387_anli_r3_entailment.json", "shot": 0, "num": 597, "correct": 53, "acc": 0.08877721943048576}
{"task": "task1387_anli_r3_entailment.json", "shot": 5, "num": 597, "correct": 247, "acc": 0.4137353433835846}
{"task": "task1664_winobias_text_generation.json", "shot": 0, "num": 194, "correct": 0, "acc": 0.0}
{"task": "task1664_winobias_text_generation.json", "shot": 5, "num": 194, "correct": 93, "acc": 0.4793814432989691}
{"task": "task1161_coda19_title_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1161_coda19_title_generation.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task880_schema_guided_dstc8_classification.json", "shot": 0, "num": 671, "correct": 0, "acc": 0.0}
{"task": "task880_schema_guided_dstc8_classification.json", "shot": 5, "num": 671, "correct": 164, "acc": 0.2444113263785395}
{"task": "task738_perspectrum_classification.json", "shot": 0, "num": 3201, "correct": 0, "acc": 0.0}
{"task": "task738_perspectrum_classification.json", "shot": 5, "num": 3201, "correct": 2232, "acc": 0.697282099343955}
{"task": "task1439_doqa_cooking_isanswerable.json", "shot": 0, "num": 1265, "correct": 0, "acc": 0.0}
{"task": "task1439_doqa_cooking_isanswerable.json", "shot": 5, "num": 1265, "correct": 507, "acc": 0.4007905138339921}
{"task": "task645_summarization.json", "shot": 0, "num": 1000, "correct": 2, "acc": 0.002}
{"task": "task645_summarization.json", "shot": 5, "num": 1000, "correct": 834, "acc": 0.834}
{"task": "task619_ohsumed_abstract_title_generation.json", "shot": 0, "num": 630, "correct": 0, "acc": 0.0}
{"task": "task619_ohsumed_abstract_title_generation.json", "shot": 5, "num": 630, "correct": 2, "acc": 0.0031746031746031746}
{"task": "task1728_web_nlg_data_to_text.json", "shot": 0, "num": 3248, "correct": 4, "acc": 0.0012315270935960591}
{"task": "task1728_web_nlg_data_to_text.json", "shot": 5, "num": 3248, "correct": 15, "acc": 0.004618226600985222}
{"task": "task1640_aqa1.0_answerable_unanswerable_question_classification.json", "shot": 0, "num": 1324, "correct": 0, "acc": 0.0}
{"task": "task1640_aqa1.0_answerable_unanswerable_question_classification.json", "shot": 5, "num": 1324, "correct": 813, "acc": 0.6140483383685801}
{"task": "task648_answer_generation.json", "shot": 0, "num": 141, "correct": 0, "acc": 0.0}
{"task": "task648_answer_generation.json", "shot": 5, "num": 141, "correct": 59, "acc": 0.41843971631205673}
{"task": "task242_tweetqa_classification.json", "shot": 0, "num": 2997, "correct": 3, "acc": 0.001001001001001001}
{"task": "task242_tweetqa_classification.json", "shot": 5, "num": 2997, "correct": 2502, "acc": 0.8348348348348348}
{"task": "task620_ohsumed_medical_subject_headings_answer_generation.json", "shot": 0, "num": 631, "correct": 0, "acc": 0.0}
{"task": "task620_ohsumed_medical_subject_headings_answer_generation.json", "shot": 5, "num": 631, "correct": 0, "acc": 0.0}
{"task": "task1159_bard_analogical_reasoning_containers.json", "shot": 0, "num": 349, "correct": 0, "acc": 0.0}
{"task": "task1159_bard_analogical_reasoning_containers.json", "shot": 5, "num": 349, "correct": 208, "acc": 0.5959885386819485}
{"task": "task500_scruples_anecdotes_title_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task500_scruples_anecdotes_title_generation.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task890_gcwd_classification.json", "shot": 0, "num": 99, "correct": 0, "acc": 0.0}
{"task": "task890_gcwd_classification.json", "shot": 5, "num": 99, "correct": 63, "acc": 0.6363636363636364}
{"task": "task039_qasc_find_overlapping_words.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task039_qasc_find_overlapping_words.json", "shot": 5, "num": 3250, "correct": 320, "acc": 0.09846153846153846}
{"task": "task1154_bard_analogical_reasoning_travel.json", "shot": 0, "num": 402, "correct": 0, "acc": 0.0}
{"task": "task1154_bard_analogical_reasoning_travel.json", "shot": 5, "num": 402, "correct": 96, "acc": 0.23880597014925373}
{"task": "task1612_sick_label_classification.json", "shot": 0, "num": 900, "correct": 0, "acc": 0.0}
{"task": "task1612_sick_label_classification.json", "shot": 5, "num": 900, "correct": 469, "acc": 0.5211111111111111}
{"task": "task1442_doqa_movies_isanswerable.json", "shot": 0, "num": 940, "correct": 0, "acc": 0.0}
{"task": "task1442_doqa_movies_isanswerable.json", "shot": 5, "num": 940, "correct": 572, "acc": 0.6085106382978723}
{"task": "task233_iirc_link_exists_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task233_iirc_link_exists_classification.json", "shot": 5, "num": 3250, "correct": 1874, "acc": 0.5766153846153846}
{"task": "task936_defeasible_nli_snli_classification.json", "shot": 0, "num": 3250, "correct": 2091, "acc": 0.6433846153846153}
{"task": "task936_defeasible_nli_snli_classification.json", "shot": 5, "num": 3250, "correct": 1945, "acc": 0.5984615384615385}
{"task": "task1386_anli_r2_entailment.json", "shot": 0, "num": 497, "correct": 97, "acc": 0.19517102615694165}
{"task": "task1386_anli_r2_entailment.json", "shot": 5, "num": 497, "correct": 202, "acc": 0.40643863179074446}
{"task": "task1152_bard_analogical_reasoning_causation.json", "shot": 0, "num": 102, "correct": 12, "acc": 0.11764705882352941}
{"task": "task1152_bard_analogical_reasoning_causation.json", "shot": 5, "num": 102, "correct": 37, "acc": 0.3627450980392157}
{"task": "task290_tellmewhy_question_answerability.json", "shot": 0, "num": 1741, "correct": 1, "acc": 0.0005743825387708214}
{"task": "task290_tellmewhy_question_answerability.json", "shot": 5, "num": 1741, "correct": 1136, "acc": 0.652498564043653}
{"task": "task304_numeric_fused_head_resolution.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task304_numeric_fused_head_resolution.json", "shot": 5, "num": 3250, "correct": 450, "acc": 0.13846153846153847}
{"task": "task760_msr_sqa_long_text_generation.json", "shot": 5, "num": 13, "correct": 0, "acc": 0.0}
{"task": "task760_msr_sqa_long_text_generation.json", "shot": 0, "num": 13, "correct": 0, "acc": 0.0}
{"task": "task035_winogrande_question_modification_person.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task035_winogrande_question_modification_person.json", "shot": 5, "num": 3250, "correct": 61, "acc": 0.01876923076923077}
{"task": "task569_recipe_nlg_text_generation.json", "shot": 0, "num": 3245, "correct": 0, "acc": 0.0}
{"task": "task569_recipe_nlg_text_generation.json", "shot": 5, "num": 3245, "correct": 92, "acc": 0.02835130970724191}
{"task": "task391_causal_relationship.json", "shot": 0, "num": 1142, "correct": 0, "acc": 0.0}
{"task": "task391_causal_relationship.json", "shot": 5, "num": 1142, "correct": 677, "acc": 0.5928196147110333}
{"task": "task891_gap_coreference_resolution.json", "shot": 0, "num": 99, "correct": 0, "acc": 0.0}
{"task": "task891_gap_coreference_resolution.json", "shot": 5, "num": 99, "correct": 53, "acc": 0.5353535353535354}
{"task": "task1586_scifact_title_generation.json", "shot": 0, "num": 2500, "correct": 0, "acc": 0.0}
{"task": "task1586_scifact_title_generation.json", "shot": 5, "num": 2500, "correct": 1, "acc": 0.0004}
{"task": "task602_wikitext-103_answer_generation.json", "shot": 0, "num": 42, "correct": 0, "acc": 0.0}
{"task": "task602_wikitext-103_answer_generation.json", "shot": 5, "num": 42, "correct": 3, "acc": 0.07142857142857142}
{"task": "task1195_disflqa_disfluent_to_fluent_conversion.json", "shot": 0, "num": 3247, "correct": 0, "acc": 0.0}
{"task": "task1195_disflqa_disfluent_to_fluent_conversion.json", "shot": 5, "num": 3247, "correct": 1041, "acc": 0.3206036341238066}
{"task": "task1409_dart_text_generation.json", "shot": 0, "num": 3241, "correct": 1, "acc": 0.000308546744831842}
{"task": "task1409_dart_text_generation.json", "shot": 5, "num": 3241, "correct": 22, "acc": 0.006788028386300524}
{"task": "task033_winogrande_answer_generation.json", "shot": 0, "num": 3250, "correct": 1243, "acc": 0.38246153846153846}
{"task": "task033_winogrande_answer_generation.json", "shot": 5, "num": 3250, "correct": 1534, "acc": 0.472}
{"task": "task1407_dart_question_generation.json", "shot": 0, "num": 1565, "correct": 0, "acc": 0.0}
{"task": "task1407_dart_question_generation.json", "shot": 5, "num": 1565, "correct": 0, "acc": 0.0}
{"task": "task402_grailqa_paraphrase_generation.json", "shot": 0, "num": 1523, "correct": 0, "acc": 0.0}
{"task": "task402_grailqa_paraphrase_generation.json", "shot": 5, "num": 1523, "correct": 0, "acc": 0.0}
{"task": "task201_mnli_neutral_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task201_mnli_neutral_classification.json", "shot": 5, "num": 3250, "correct": 778, "acc": 0.2393846153846154}
{"task": "task520_aquamuse_answer_given_in_passage.json", "shot": 0, "num": 500, "correct": 0, "acc": 0.0}
{"task": "task520_aquamuse_answer_given_in_passage.json", "shot": 5, "num": 500, "correct": 399, "acc": 0.798}
{"task": "task892_gap_reverse_coreference_resolution.json", "shot": 0, "num": 97, "correct": 0, "acc": 0.0}
{"task": "task892_gap_reverse_coreference_resolution.json", "shot": 5, "num": 97, "correct": 55, "acc": 0.5670103092783505}
{"task": "task828_copa_commonsense_cause_effect.json", "shot": 0, "num": 498, "correct": 0, "acc": 0.0}
{"task": "task828_copa_commonsense_cause_effect.json", "shot": 5, "num": 498, "correct": 270, "acc": 0.5421686746987951}
{"task": "task769_qed_summarization.json", "shot": 0, "num": 500, "correct": 0, "acc": 0.0}
{"task": "task769_qed_summarization.json", "shot": 5, "num": 500, "correct": 414, "acc": 0.828}
{"task": "task1155_bard_analogical_reasoning_trash_or_treasure.json", "shot": 0, "num": 273, "correct": 0, "acc": 0.0}
{"task": "task1155_bard_analogical_reasoning_trash_or_treasure.json", "shot": 5, "num": 273, "correct": 220, "acc": 0.8058608058608059}
{"task": "task1385_anli_r1_entailment.json", "shot": 0, "num": 495, "correct": 86, "acc": 0.17373737373737375}
{"task": "task1385_anli_r1_entailment.json", "shot": 5, "num": 495, "correct": 193, "acc": 0.3898989898989899}
{"task": "task1531_daily_dialog_type_classification.json", "shot": 0, "num": 249, "correct": 0, "acc": 0.0}
{"task": "task1531_daily_dialog_type_classification.json", "shot": 5, "num": 249, "correct": 114, "acc": 0.4578313253012048}
{"task": "task1516_imppres_naturallanguageinference.json", "shot": 0, "num": 355, "correct": 0, "acc": 0.0}
{"task": "task1516_imppres_naturallanguageinference.json", "shot": 5, "num": 355, "correct": 135, "acc": 0.38028169014084506}
{"task": "task1394_meta_woz_task_classification.json", "shot": 0, "num": 112, "correct": 0, "acc": 0.0}
{"task": "task1394_meta_woz_task_classification.json", "shot": 5, "num": 112, "correct": 76, "acc": 0.6785714285714286}
{"task": "task401_numeric_fused_head_reference.json", "shot": 0, "num": 1873, "correct": 26, "acc": 0.013881473571809931}
{"task": "task401_numeric_fused_head_reference.json", "shot": 5, "num": 1873, "correct": 476, "acc": 0.25413774693005875}
{"task": "task1598_nyc_long_text_generation.json", "shot": 0, "num": 3247, "correct": 0, "acc": 0.0}
{"task": "task1598_nyc_long_text_generation.json", "shot": 5, "num": 3247, "correct": 0, "acc": 0.0}
{"task": "task1615_sick_tclassify_b_relation_a.json", "shot": 0, "num": 899, "correct": 0, "acc": 0.0}
{"task": "task1615_sick_tclassify_b_relation_a.json", "shot": 5, "num": 899, "correct": 550, "acc": 0.6117908787541713}
{"task": "task970_sherliic_causal_relationship.json", "shot": 0, "num": 1179, "correct": 0, "acc": 0.0}
{"task": "task970_sherliic_causal_relationship.json", "shot": 5, "num": 1179, "correct": 758, "acc": 0.6429177268871925}
{"task": "task1390_wscfixed_coreference.json", "shot": 0, "num": 324, "correct": 149, "acc": 0.45987654320987653}
{"task": "task1390_wscfixed_coreference.json", "shot": 5, "num": 324, "correct": 173, "acc": 0.5339506172839507}
{"task": "task199_mnli_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task199_mnli_classification.json", "shot": 5, "num": 3250, "correct": 2068, "acc": 0.6363076923076924}
{"task": "task034_winogrande_question_modification_object.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task034_winogrande_question_modification_object.json", "shot": 5, "num": 3250, "correct": 226, "acc": 0.06953846153846154}
{"task": "task133_winowhy_reason_plausibility_detection.json", "shot": 0, "num": 1428, "correct": 0, "acc": 0.0}
{"task": "task133_winowhy_reason_plausibility_detection.json", "shot": 5, "num": 1428, "correct": 712, "acc": 0.49859943977591037}
{"task": "task226_english_language_answer_relevance_classification.json", "shot": 0, "num": 239, "correct": 0, "acc": 0.0}
{"task": "task226_english_language_answer_relevance_classification.json", "shot": 5, "num": 239, "correct": 121, "acc": 0.5062761506276151}
{"task": "task510_reddit_tifu_title_summarization.json", "shot": 0, "num": 3250, "correct": 1, "acc": 0.0003076923076923077}
{"task": "task510_reddit_tifu_title_summarization.json", "shot": 5, "num": 3250, "correct": 10, "acc": 0.003076923076923077}
{"task": "task935_defeasible_nli_atomic_classification.json", "shot": 0, "num": 3250, "correct": 1353, "acc": 0.4163076923076923}
{"task": "task935_defeasible_nli_atomic_classification.json", "shot": 5, "num": 3250, "correct": 2039, "acc": 0.6273846153846154}
{"task": "task349_squad2.0_answerable_unanswerable_question_classification.json", "shot": 0, "num": 3249, "correct": 1, "acc": 0.0003077870113881194}
{"task": "task349_squad2.0_answerable_unanswerable_question_classification.json", "shot": 5, "num": 3249, "correct": 2055, "acc": 0.6325023084025854}
{"task": "task1157_bard_analogical_reasoning_rooms_for_containers.json", "shot": 0, "num": 484, "correct": 0, "acc": 0.0}
{"task": "task1157_bard_analogical_reasoning_rooms_for_containers.json", "shot": 5, "num": 484, "correct": 168, "acc": 0.34710743801652894}
{"task": "task937_defeasible_nli_social_classification.json", "shot": 0, "num": 3250, "correct": 293, "acc": 0.09015384615384615}
{"task": "task937_defeasible_nli_social_classification.json", "shot": 5, "num": 3250, "correct": 2017, "acc": 0.6206153846153846}
{"task": "task743_eurlex_summarization.json", "shot": 0, "num": 325, "correct": 0, "acc": 0.0}
{"task": "task743_eurlex_summarization.json", "shot": 5, "num": 325, "correct": 0, "acc": 0.0}
{"task": "task1388_cb_entailment.json", "shot": 0, "num": 151, "correct": 16, "acc": 0.10596026490066225}
{"task": "task1388_cb_entailment.json", "shot": 5, "num": 151, "correct": 77, "acc": 0.5099337748344371}
{"task": "task671_ambigqa_text_generation.json", "shot": 0, "num": 2375, "correct": 0, "acc": 0.0}
{"task": "task671_ambigqa_text_generation.json", "shot": 5, "num": 2375, "correct": 1, "acc": 0.0004210526315789474}
{"task": "task121_zest_text_modification.json", "shot": 0, "num": 64, "correct": 0, "acc": 0.0}
{"task": "task121_zest_text_modification.json", "shot": 5, "num": 64, "correct": 2, "acc": 0.03125}
{"task": "task1345_glue_qqp_question_paraprashing.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1345_glue_qqp_question_paraprashing.json", "shot": 5, "num": 3250, "correct": 1, "acc": 0.0003076923076923077}
{"task": "task330_gap_answer_generation.json", "shot": 0, "num": 1980, "correct": 0, "acc": 0.0}
{"task": "task330_gap_answer_generation.json", "shot": 5, "num": 1980, "correct": 943, "acc": 0.4762626262626263}
{"task": "task1342_amazon_us_reviews_title.json", "shot": 0, "num": 3109, "correct": 0, "acc": 0.0}
{"task": "task1342_amazon_us_reviews_title.json", "shot": 5, "num": 3109, "correct": 34, "acc": 0.010935992280476037}
{"task": "task329_gap_classification.json", "shot": 0, "num": 2225, "correct": 0, "acc": 0.0}
{"task": "task329_gap_classification.json", "shot": 5, "num": 2225, "correct": 1029, "acc": 0.46247191011235955}
{"task": "task281_points_of_correspondence.json", "shot": 0, "num": 738, "correct": 0, "acc": 0.0}
{"task": "task281_points_of_correspondence.json", "shot": 5, "num": 738, "correct": 3, "acc": 0.0040650406504065045}
{"task": "task036_qasc_topic_word_to_generate_related_fact.json", "shot": 0, "num": 461, "correct": 0, "acc": 0.0}
{"task": "task036_qasc_topic_word_to_generate_related_fact.json", "shot": 5, "num": 461, "correct": 0, "acc": 0.0}
{"task": "task1554_scitail_classification.json", "shot": 0, "num": 3248, "correct": 0, "acc": 0.0}
{"task": "task1554_scitail_classification.json", "shot": 5, "num": 3248, "correct": 2043, "acc": 0.6290024630541872}
{"task": "task050_multirc_answerability.json", "shot": 0, "num": 2956, "correct": 1810, "acc": 0.6123139377537212}
{"task": "task050_multirc_answerability.json", "shot": 5, "num": 2956, "correct": 1953, "acc": 0.6606901217861976}
{"task": "task362_spolin_yesand_prompt_response_sub_classification.json", "shot": 0, "num": 3198, "correct": 0, "acc": 0.0}
{"task": "task362_spolin_yesand_prompt_response_sub_classification.json", "shot": 5, "num": 3198, "correct": 1646, "acc": 0.5146966854283928}
{"task": "task1557_jfleg_answer_generation.json", "shot": 0, "num": 377, "correct": 0, "acc": 0.0}
{"task": "task1557_jfleg_answer_generation.json", "shot": 5, "num": 377, "correct": 49, "acc": 0.129973474801061}
{"task": "task249_enhanced_wsc_pronoun_disambiguation.json", "shot": 0, "num": 342, "correct": 0, "acc": 0.0}
{"task": "task249_enhanced_wsc_pronoun_disambiguation.json", "shot": 5, "num": 342, "correct": 186, "acc": 0.543859649122807}
{"task": "task957_e2e_nlg_text_generation_generate.json", "shot": 0, "num": 1081, "correct": 0, "acc": 0.0}
{"task": "task957_e2e_nlg_text_generation_generate.json", "shot": 5, "num": 1081, "correct": 1, "acc": 0.0009250693802035153}
{"task": "task418_persent_title_generation.json", "shot": 0, "num": 1670, "correct": 0, "acc": 0.0}
{"task": "task418_persent_title_generation.json", "shot": 5, "num": 1670, "correct": 0, "acc": 0.0}
{"task": "task614_glucose_cause_event_detection.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task614_glucose_cause_event_detection.json", "shot": 5, "num": 3249, "correct": 10, "acc": 0.0030778701138811943}
{"task": "task677_ollie_sentence_answer_generation.json", "shot": 0, "num": 2255, "correct": 0, "acc": 0.0}
{"task": "task677_ollie_sentence_answer_generation.json", "shot": 5, "num": 2255, "correct": 6, "acc": 0.0026607538802660754}
{"task": "task220_rocstories_title_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task220_rocstories_title_classification.json", "shot": 5, "num": 3250, "correct": 1492, "acc": 0.4590769230769231}
{"task": "task1631_openpi_answer_generation.json", "shot": 0, "num": 1495, "correct": 0, "acc": 0.0}
{"task": "task1631_openpi_answer_generation.json", "shot": 5, "num": 1495, "correct": 1013, "acc": 0.6775919732441471}
{"task": "task232_iirc_link_number_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task232_iirc_link_number_classification.json", "shot": 5, "num": 3250, "correct": 1891, "acc": 0.5818461538461538}
{"task": "task1391_winogrande_easy_answer_generation.json", "shot": 0, "num": 3246, "correct": 1436, "acc": 0.4423906346272335}
{"task": "task1391_winogrande_easy_answer_generation.json", "shot": 5, "num": 3246, "correct": 1778, "acc": 0.547751078250154}
{"task": "task1358_xlsum_title_generation.json", "shot": 0, "num": 3064, "correct": 0, "acc": 0.0}
{"task": "task1358_xlsum_title_generation.json", "shot": 5, "num": 3064, "correct": 7, "acc": 0.0022845953002610967}
{"task": "task1533_daily_dialog_formal_classification.json", "shot": 0, "num": 3081, "correct": 0, "acc": 0.0}
{"task": "task1533_daily_dialog_formal_classification.json", "shot": 5, "num": 3081, "correct": 1459, "acc": 0.4735475494969166}
{"task": "task1156_bard_analogical_reasoning_tools.json", "shot": 0, "num": 329, "correct": 0, "acc": 0.0}
{"task": "task1156_bard_analogical_reasoning_tools.json", "shot": 5, "num": 329, "correct": 152, "acc": 0.46200607902735563}
{"task": "task1659_title_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1659_title_generation.json", "shot": 5, "num": 3250, "correct": 465, "acc": 0.14307692307692307}
{"task": "task1624_disfl_qa_question_yesno_classification.json", "shot": 0, "num": 750, "correct": 0, "acc": 0.0}
{"task": "task1624_disfl_qa_question_yesno_classification.json", "shot": 5, "num": 750, "correct": 402, "acc": 0.536}
{"task": "task1158_bard_analogical_reasoning_manipulating_items.json", "shot": 0, "num": 188, "correct": 0, "acc": 0.0}
{"task": "task1158_bard_analogical_reasoning_manipulating_items.json", "shot": 5, "num": 188, "correct": 49, "acc": 0.26063829787234044}
{"task": "task827_copa_commonsense_reasoning.json", "shot": 0, "num": 498, "correct": 0, "acc": 0.0}
{"task": "task827_copa_commonsense_reasoning.json", "shot": 5, "num": 498, "correct": 319, "acc": 0.6405622489959839}
{"task": "task1153_bard_analogical_reasoning_affordance.json", "shot": 0, "num": 1009, "correct": 61, "acc": 0.06045589692765114}
{"task": "task1153_bard_analogical_reasoning_affordance.json", "shot": 5, "num": 1009, "correct": 284, "acc": 0.2814667988107037}
{"task": "task393_plausible_result_generation.json", "shot": 0, "num": 50, "correct": 0, "acc": 0.0}
{"task": "task393_plausible_result_generation.json", "shot": 5, "num": 50, "correct": 0, "acc": 0.0}
{"task": "task879_schema_guided_dstc8_classification.json", "shot": 0, "num": 1153, "correct": 77, "acc": 0.06678230702515178}
{"task": "task879_schema_guided_dstc8_classification.json", "shot": 5, "num": 1153, "correct": 516, "acc": 0.44752818733738076}
{"task": "task613_politifact_text_generation.json", "shot": 0, "num": 3250, "correct": 26, "acc": 0.008}
{"task": "task613_politifact_text_generation.json", "shot": 5, "num": 3250, "correct": 125, "acc": 0.038461538461538464}
{"task": "task219_rocstories_title_answer_generation.json", "shot": 0, "num": 3250, "correct": 2, "acc": 0.0006153846153846154}
{"task": "task219_rocstories_title_answer_generation.json", "shot": 5, "num": 3250, "correct": 248, "acc": 0.07630769230769231}
{"task": "task190_snli_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task190_snli_classification.json", "shot": 5, "num": 3250, "correct": 1548, "acc": 0.4763076923076923}
{"task": "task200_mnli_entailment_classification.json", "shot": 0, "num": 2232, "correct": 0, "acc": 0.0}
{"task": "task200_mnli_entailment_classification.json", "shot": 5, "num": 2232, "correct": 1203, "acc": 0.5389784946236559}
{"task": "task1534_daily_dialog_question_classification.json", "shot": 0, "num": 3081, "correct": 0, "acc": 0.0}
{"task": "task1534_daily_dialog_question_classification.json", "shot": 5, "num": 3081, "correct": 1560, "acc": 0.5063291139240507}
{"task": "task1540_parsed_pdfs_summarization.json", "shot": 0, "num": 1500, "correct": 0, "acc": 0.0}
{"task": "task1540_parsed_pdfs_summarization.json", "shot": 5, "num": 1500, "correct": 9, "acc": 0.006}
{"task": "task442_com_qa_paraphrase_question_generation.json", "shot": 0, "num": 898, "correct": 0, "acc": 0.0}
{"task": "task442_com_qa_paraphrase_question_generation.json", "shot": 5, "num": 898, "correct": 10, "acc": 0.011135857461024499}
{"task": "task392_inverse_causal_relationship.json", "shot": 0, "num": 1302, "correct": 0, "acc": 0.0}
{"task": "task392_inverse_causal_relationship.json", "shot": 5, "num": 1302, "correct": 879, "acc": 0.6751152073732719}
{"task": "task1562_zest_text_modification.json", "shot": 0, "num": 71, "correct": 0, "acc": 0.0}
{"task": "task1562_zest_text_modification.json", "shot": 5, "num": 71, "correct": 0, "acc": 0.0}
{"task": "task640_esnli_classification.json", "shot": 0, "num": 50, "correct": 0, "acc": 0.0}
{"task": "task640_esnli_classification.json", "shot": 5, "num": 50, "correct": 26, "acc": 0.52}
{"task": "task1622_disfl_qa_text_modication.json", "shot": 0, "num": 998, "correct": 0, "acc": 0.0}
{"task": "task1622_disfl_qa_text_modication.json", "shot": 5, "num": 998, "correct": 311, "acc": 0.31162324649298595}
{"task": "task623_ohsumed_yes_no_answer_generation.json", "shot": 0, "num": 200, "correct": 0, "acc": 0.0}
{"task": "task623_ohsumed_yes_no_answer_generation.json", "shot": 5, "num": 200, "correct": 35, "acc": 0.175}
{"task": "task020_mctaco_span_based_question.json", "shot": 0, "num": 191, "correct": 9, "acc": 0.04712041884816754}
{"task": "task020_mctaco_span_based_question.json", "shot": 5, "num": 191, "correct": 145, "acc": 0.7591623036649214}
{"task": "task642_esnli_classification.json", "shot": 0, "num": 150, "correct": 1, "acc": 0.006666666666666667}
{"task": "task642_esnli_classification.json", "shot": 5, "num": 150, "correct": 66, "acc": 0.44}
{"task": "task102_commongen_sentence_generation.json", "shot": 0, "num": 2704, "correct": 0, "acc": 0.0}
{"task": "task102_commongen_sentence_generation.json", "shot": 5, "num": 2704, "correct": 0, "acc": 0.0}
{"task": "task547_alt_translation_entk_en.json", "shot": 0, "num": 300, "correct": 0, "acc": 0.0}
{"task": "task547_alt_translation_entk_en.json", "shot": 5, "num": 300, "correct": 49, "acc": 0.16333333333333333}
{"task": "task706_mmmlu_answer_generation_high_school_mathematics.json", "shot": 0, "num": 91, "correct": 0, "acc": 0.0}
{"task": "task706_mmmlu_answer_generation_high_school_mathematics.json", "shot": 5, "num": 91, "correct": 27, "acc": 0.2967032967032967}
{"task": "task1565_triviaqa_classification.json", "shot": 0, "num": 54, "correct": 0, "acc": 0.0}
{"task": "task1565_triviaqa_classification.json", "shot": 5, "num": 54, "correct": 28, "acc": 0.5185185185185185}
{"task": "task701_mmmlu_answer_generation_high_school_computer_science.json", "shot": 0, "num": 56, "correct": 10, "acc": 0.17857142857142858}
{"task": "task701_mmmlu_answer_generation_high_school_computer_science.json", "shot": 5, "num": 56, "correct": 22, "acc": 0.39285714285714285}
{"task": "task698_mmmlu_answer_generation_global_facts.json", "shot": 0, "num": 56, "correct": 16, "acc": 0.2857142857142857}
{"task": "task698_mmmlu_answer_generation_global_facts.json", "shot": 5, "num": 56, "correct": 23, "acc": 0.4107142857142857}
{"task": "task104_semeval_2019_task10_closed_vocabulary_mathematical_answer_generation.json", "shot": 0, "num": 342, "correct": 0, "acc": 0.0}
{"task": "task104_semeval_2019_task10_closed_vocabulary_mathematical_answer_generation.json", "shot": 5, "num": 342, "correct": 74, "acc": 0.21637426900584794}
{"task": "task926_coached_conv_pref_word_generation.json", "shot": 0, "num": 248, "correct": 0, "acc": 0.0}
{"task": "task926_coached_conv_pref_word_generation.json", "shot": 5, "num": 248, "correct": 189, "acc": 0.7620967741935484}
{"task": "task326_jigsaw_classification_obscene.json", "shot": 0, "num": 3192, "correct": 0, "acc": 0.0}
{"task": "task326_jigsaw_classification_obscene.json", "shot": 5, "num": 3192, "correct": 1627, "acc": 0.5097117794486216}
{"task": "task1498_24hour_to_12hour_clock.json", "shot": 0, "num": 98, "correct": 0, "acc": 0.0}
{"task": "task1498_24hour_to_12hour_clock.json", "shot": 5, "num": 98, "correct": 78, "acc": 0.7959183673469388}
{"task": "task1731_quartz_question_answering.json", "shot": 0, "num": 1340, "correct": 0, "acc": 0.0}
{"task": "task1731_quartz_question_answering.json", "shot": 5, "num": 1340, "correct": 661, "acc": 0.49328358208955225}
{"task": "task1453_person_entity_extraction_btc_corpus.json", "shot": 0, "num": 131, "correct": 6, "acc": 0.04580152671755725}
{"task": "task1453_person_entity_extraction_btc_corpus.json", "shot": 5, "num": 131, "correct": 102, "acc": 0.7786259541984732}
{"task": "task1399_obqa_answer_generation.json", "shot": 0, "num": 884, "correct": 0, "acc": 0.0}
{"task": "task1399_obqa_answer_generation.json", "shot": 5, "num": 884, "correct": 36, "acc": 0.04072398190045249}
{"task": "task1286_openbookqa_question_answering.json", "shot": 0, "num": 2724, "correct": 0, "acc": 0.0}
{"task": "task1286_openbookqa_question_answering.json", "shot": 5, "num": 2724, "correct": 1284, "acc": 0.4713656387665198}
{"task": "task165_mcscript_question_answering_commonsense.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task165_mcscript_question_answering_commonsense.json", "shot": 5, "num": 3250, "correct": 1211, "acc": 0.3726153846153846}
{"task": "task610_conllpp_ner.json", "shot": 0, "num": 1860, "correct": 0, "acc": 0.0}
{"task": "task610_conllpp_ner.json", "shot": 5, "num": 1860, "correct": 75, "acc": 0.04032258064516129}
{"task": "task864_asdiv_singleop_question_answering.json", "shot": 0, "num": 470, "correct": 0, "acc": 0.0}
{"task": "task864_asdiv_singleop_question_answering.json", "shot": 5, "num": 470, "correct": 331, "acc": 0.7042553191489361}
{"task": "task385_socialiqa_incorrect_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task385_socialiqa_incorrect_answer_generation.json", "shot": 5, "num": 3250, "correct": 976, "acc": 0.30030769230769233}
{"task": "task1608_xquad_en_answer_generation.json", "shot": 0, "num": 593, "correct": 152, "acc": 0.2563237774030354}
{"task": "task1608_xquad_en_answer_generation.json", "shot": 5, "num": 593, "correct": 289, "acc": 0.4873524451939292}
{"task": "task337_hateeval_classification_individual_en.json", "shot": 0, "num": 1499, "correct": 0, "acc": 0.0}
{"task": "task337_hateeval_classification_individual_en.json", "shot": 5, "num": 1499, "correct": 628, "acc": 0.418945963975984}
{"task": "task563_discofuse_answer_generation.json", "shot": 0, "num": 500, "correct": 0, "acc": 0.0}
{"task": "task563_discofuse_answer_generation.json", "shot": 5, "num": 500, "correct": 443, "acc": 0.886}
{"task": "task023_cosmosqa_question_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task023_cosmosqa_question_generation.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task607_sbic_intentional_offense_binary_classification.json", "shot": 0, "num": 866, "correct": 0, "acc": 0.0}
{"task": "task607_sbic_intentional_offense_binary_classification.json", "shot": 5, "num": 866, "correct": 269, "acc": 0.31062355658198615}
{"task": "task513_argument_stance_classification.json", "shot": 0, "num": 84, "correct": 4, "acc": 0.047619047619047616}
{"task": "task513_argument_stance_classification.json", "shot": 5, "num": 84, "correct": 51, "acc": 0.6071428571428571}
{"task": "task512_twitter_emotion_classification.json", "shot": 0, "num": 1715, "correct": 75, "acc": 0.043731778425655975}
{"task": "task512_twitter_emotion_classification.json", "shot": 5, "num": 1715, "correct": 660, "acc": 0.3848396501457726}
{"task": "task269_csrg_counterfactual_story_generation.json", "shot": 0, "num": 3200, "correct": 0, "acc": 0.0}
{"task": "task269_csrg_counterfactual_story_generation.json", "shot": 5, "num": 3200, "correct": 3, "acc": 0.0009375}
{"task": "task599_cuad_question_generation.json", "shot": 0, "num": 204, "correct": 0, "acc": 0.0}
{"task": "task599_cuad_question_generation.json", "shot": 5, "num": 204, "correct": 0, "acc": 0.0}
{"task": "task353_casino_classification_negotiation_elicit_pref.json", "shot": 0, "num": 375, "correct": 0, "acc": 0.0}
{"task": "task353_casino_classification_negotiation_elicit_pref.json", "shot": 5, "num": 375, "correct": 168, "acc": 0.448}
{"task": "task118_semeval_2019_task10_open_vocabulary_mathematical_answer_generation.json", "shot": 0, "num": 122, "correct": 0, "acc": 0.0}
{"task": "task118_semeval_2019_task10_open_vocabulary_mathematical_answer_generation.json", "shot": 5, "num": 122, "correct": 23, "acc": 0.1885245901639344}
{"task": "task742_lhoestq_answer_generation_frequency.json", "shot": 0, "num": 52, "correct": 0, "acc": 0.0}
{"task": "task742_lhoestq_answer_generation_frequency.json", "shot": 5, "num": 52, "correct": 20, "acc": 0.38461538461538464}
{"task": "task1447_drug_extraction_ade.json", "shot": 0, "num": 1359, "correct": 19, "acc": 0.013980868285504048}
{"task": "task1447_drug_extraction_ade.json", "shot": 5, "num": 1359, "correct": 1230, "acc": 0.9050772626931567}
{"task": "task1216_atomic_classification_causes.json", "shot": 0, "num": 350, "correct": 0, "acc": 0.0}
{"task": "task1216_atomic_classification_causes.json", "shot": 5, "num": 350, "correct": 245, "acc": 0.7}
{"task": "task344_hybridqa_answer_generation.json", "shot": 0, "num": 3248, "correct": 16, "acc": 0.0049261083743842365}
{"task": "task344_hybridqa_answer_generation.json", "shot": 5, "num": 3248, "correct": 110, "acc": 0.033866995073891626}
{"task": "task184_break_generate_question.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task184_break_generate_question.json", "shot": 5, "num": 3249, "correct": 145, "acc": 0.04462911665127731}
{"task": "task001_quoref_question_generation.json", "shot": 0, "num": 2113, "correct": 0, "acc": 0.0}
{"task": "task001_quoref_question_generation.json", "shot": 5, "num": 2113, "correct": 0, "acc": 0.0}
{"task": "task002_quoref_answer_generation.json", "shot": 0, "num": 3250, "correct": 22, "acc": 0.00676923076923077}
{"task": "task002_quoref_answer_generation.json", "shot": 5, "num": 3250, "correct": 1465, "acc": 0.45076923076923076}
{"task": "task267_concatenate_and_reverse_all_elements_from_index_i_to_j.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task267_concatenate_and_reverse_all_elements_from_index_i_to_j.json", "shot": 5, "num": 3250, "correct": 8, "acc": 0.0024615384615384616}
{"task": "task707_mmmlu_answer_generation_high_school_microeconomics.json", "shot": 0, "num": 89, "correct": 11, "acc": 0.12359550561797752}
{"task": "task707_mmmlu_answer_generation_high_school_microeconomics.json", "shot": 5, "num": 89, "correct": 36, "acc": 0.4044943820224719}
{"task": "task341_winomt_classification_gender_anti.json", "shot": 0, "num": 789, "correct": 0, "acc": 0.0}
{"task": "task341_winomt_classification_gender_anti.json", "shot": 5, "num": 789, "correct": 441, "acc": 0.55893536121673}
{"task": "task153_tomqa_find_location_hard_clean.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task153_tomqa_find_location_hard_clean.json", "shot": 5, "num": 3250, "correct": 1227, "acc": 0.37753846153846154}
{"task": "task693_mmmlu_answer_generation_conceptual_physics.json", "shot": 0, "num": 89, "correct": 8, "acc": 0.0898876404494382}
{"task": "task693_mmmlu_answer_generation_conceptual_physics.json", "shot": 5, "num": 89, "correct": 38, "acc": 0.42696629213483145}
{"task": "task667_mmmlu_answer_generation_business_ethics.json", "shot": 0, "num": 49, "correct": 7, "acc": 0.14285714285714285}
{"task": "task667_mmmlu_answer_generation_business_ethics.json", "shot": 5, "num": 49, "correct": 31, "acc": 0.6326530612244898}
{"task": "task119_semeval_2019_task10_geometric_mathematical_answer_generation.json", "shot": 0, "num": 66, "correct": 0, "acc": 0.0}
{"task": "task119_semeval_2019_task10_geometric_mathematical_answer_generation.json", "shot": 5, "num": 66, "correct": 17, "acc": 0.25757575757575757}
{"task": "task176_break_decompose_questions.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task176_break_decompose_questions.json", "shot": 5, "num": 3250, "correct": 10, "acc": 0.003076923076923077}
{"task": "task488_extract_all_alphabetical_elements_from_list_in_order.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task488_extract_all_alphabetical_elements_from_list_in_order.json", "shot": 5, "num": 3250, "correct": 900, "acc": 0.27692307692307694}
{"task": "task845_pubmedqa_question_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task845_pubmedqa_question_generation.json", "shot": 5, "num": 3250, "correct": 2, "acc": 0.0006153846153846154}
{"task": "task298_storycloze_correct_end_classification.json", "shot": 0, "num": 1885, "correct": 0, "acc": 0.0}
{"task": "task298_storycloze_correct_end_classification.json", "shot": 5, "num": 1885, "correct": 1391, "acc": 0.7379310344827587}
{"task": "task751_svamp_subtraction_question_answering.json", "shot": 0, "num": 266, "correct": 0, "acc": 0.0}
{"task": "task751_svamp_subtraction_question_answering.json", "shot": 5, "num": 266, "correct": 102, "acc": 0.38345864661654133}
{"task": "task704_mmmlu_answer_generation_high_school_government_and_politics.json", "shot": 0, "num": 87, "correct": 31, "acc": 0.3563218390804598}
{"task": "task704_mmmlu_answer_generation_high_school_government_and_politics.json", "shot": 5, "num": 87, "correct": 46, "acc": 0.5287356321839081}
{"task": "task1549_wiqa_answer_generation_missing_step.json", "shot": 0, "num": 46, "correct": 0, "acc": 0.0}
{"task": "task1549_wiqa_answer_generation_missing_step.json", "shot": 5, "num": 46, "correct": 8, "acc": 0.17391304347826086}
{"task": "task1188_count_max_freq_char.json", "shot": 0, "num": 98, "correct": 0, "acc": 0.0}
{"task": "task1188_count_max_freq_char.json", "shot": 5, "num": 98, "correct": 27, "acc": 0.2755102040816326}
{"task": "task211_logic2text_classification.json", "shot": 0, "num": 3000, "correct": 12, "acc": 0.004}
{"task": "task211_logic2text_classification.json", "shot": 5, "num": 3000, "correct": 1958, "acc": 0.6526666666666666}
{"task": "task403_creak_commonsense_inference.json", "shot": 0, "num": 3004, "correct": 392, "acc": 0.13049267643142476}
{"task": "task403_creak_commonsense_inference.json", "shot": 5, "num": 3004, "correct": 2437, "acc": 0.8112516644474035}
{"task": "task339_record_answer_generation.json", "shot": 0, "num": 3249, "correct": 3, "acc": 0.0009233610341643582}
{"task": "task339_record_answer_generation.json", "shot": 5, "num": 3249, "correct": 1286, "acc": 0.39581409664512157}
{"task": "task575_air_dialogue_classification.json", "shot": 0, "num": 1977, "correct": 392, "acc": 0.19828022255943348}
{"task": "task575_air_dialogue_classification.json", "shot": 5, "num": 1977, "correct": 1150, "acc": 0.5816894284269094}
{"task": "task470_mrqa_question_generation.json", "shot": 0, "num": 1528, "correct": 0, "acc": 0.0}
{"task": "task470_mrqa_question_generation.json", "shot": 5, "num": 1528, "correct": 3, "acc": 0.001963350785340314}
{"task": "task390_torque_text_span_selection.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task390_torque_text_span_selection.json", "shot": 5, "num": 3250, "correct": 188, "acc": 0.057846153846153846}
{"task": "task820_protoqa_answer_generation.json", "shot": 0, "num": 2500, "correct": 0, "acc": 0.0}
{"task": "task820_protoqa_answer_generation.json", "shot": 5, "num": 2500, "correct": 2, "acc": 0.0008}
{"task": "task064_all_elements_except_first_i.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task064_all_elements_except_first_i.json", "shot": 5, "num": 3250, "correct": 162, "acc": 0.049846153846153846}
{"task": "task846_pubmedqa_classification.json", "shot": 0, "num": 3250, "correct": 19, "acc": 0.005846153846153846}
{"task": "task846_pubmedqa_classification.json", "shot": 5, "num": 3250, "correct": 1957, "acc": 0.6021538461538462}
{"task": "task300_storycloze_order_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task300_storycloze_order_generation.json", "shot": 5, "num": 3250, "correct": 33, "acc": 0.010153846153846154}
{"task": "task130_scan_structured_text_generation_command_action_long.json", "shot": 0, "num": 1960, "correct": 0, "acc": 0.0}
{"task": "task130_scan_structured_text_generation_command_action_long.json", "shot": 5, "num": 1960, "correct": 0, "acc": 0.0}
{"task": "task1296_wiki_hop_question_answering.json", "shot": 5, "num": 3248, "correct": 1, "acc": 0.0003078817733990148}
{"task": "task1296_wiki_hop_question_answering.json", "shot": 0, "num": 3248, "correct": 0, "acc": 0.0}
{"task": "task170_hotpotqa_answer_generation.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task170_hotpotqa_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task918_coqa_answer_generation.json", "shot": 0, "num": 248, "correct": 0, "acc": 0.0}
{"task": "task918_coqa_answer_generation.json", "shot": 5, "num": 248, "correct": 39, "acc": 0.15725806451612903}
{"task": "task1383_quarel_write_incorrect_answer.json", "shot": 0, "num": 399, "correct": 100, "acc": 0.2506265664160401}
{"task": "task1383_quarel_write_incorrect_answer.json", "shot": 5, "num": 399, "correct": 125, "acc": 0.3132832080200501}
{"task": "task309_race_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task309_race_answer_generation.json", "shot": 5, "num": 3250, "correct": 1399, "acc": 0.43046153846153845}
{"task": "task1600_smcalflow_sentence_generation.json", "shot": 0, "num": 515, "correct": 0, "acc": 0.0}
{"task": "task1600_smcalflow_sentence_generation.json", "shot": 5, "num": 515, "correct": 1, "acc": 0.001941747572815534}
{"task": "task1209_atomic_classification_objectuse.json", "shot": 0, "num": 3247, "correct": 4, "acc": 0.0012319063751154912}
{"task": "task1209_atomic_classification_objectuse.json", "shot": 5, "num": 3247, "correct": 2206, "acc": 0.6793963658761935}
{"task": "task909_dialogre_prevalent_speakers.json", "shot": 0, "num": 217, "correct": 0, "acc": 0.0}
{"task": "task909_dialogre_prevalent_speakers.json", "shot": 5, "num": 217, "correct": 80, "acc": 0.3686635944700461}
{"task": "task636_extract_and_sort_unique_alphabets_in_a_list.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task636_extract_and_sort_unique_alphabets_in_a_list.json", "shot": 5, "num": 3250, "correct": 385, "acc": 0.11846153846153847}
{"task": "task354_casino_classification_negotiation_no_need.json", "shot": 0, "num": 192, "correct": 2, "acc": 0.010416666666666666}
{"task": "task354_casino_classification_negotiation_no_need.json", "shot": 5, "num": 192, "correct": 98, "acc": 0.5104166666666666}
{"task": "task497_extract_all_numbers_from_list_in_order.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task497_extract_all_numbers_from_list_in_order.json", "shot": 5, "num": 3250, "correct": 331, "acc": 0.10184615384615385}
{"task": "task475_yelp_polarity_classification.json", "shot": 0, "num": 3250, "correct": 66, "acc": 0.020307692307692308}
{"task": "task475_yelp_polarity_classification.json", "shot": 5, "num": 3250, "correct": 2892, "acc": 0.8898461538461538}
{"task": "task964_librispeech_asr_text_auto_completion.json", "shot": 0, "num": 75, "correct": 0, "acc": 0.0}
{"task": "task964_librispeech_asr_text_auto_completion.json", "shot": 5, "num": 75, "correct": 1, "acc": 0.013333333333333334}
{"task": "task066_timetravel_binary_consistency_classification.json", "shot": 0, "num": 3250, "correct": 156, "acc": 0.048}
{"task": "task066_timetravel_binary_consistency_classification.json", "shot": 5, "num": 3250, "correct": 2360, "acc": 0.7261538461538461}
{"task": "task405_narrativeqa_question_generation.json", "shot": 5, "num": 785, "correct": 0, "acc": 0.0}
{"task": "task405_narrativeqa_question_generation.json", "shot": 0, "num": 785, "correct": 0, "acc": 0.0}
{"task": "task357_casino_classification_negotiation_small_talk.json", "shot": 0, "num": 1038, "correct": 2, "acc": 0.0019267822736030828}
{"task": "task357_casino_classification_negotiation_small_talk.json", "shot": 5, "num": 1038, "correct": 491, "acc": 0.4730250481695568}
{"task": "task108_contextualabusedetection_classification.json", "shot": 0, "num": 1998, "correct": 40, "acc": 0.02002002002002002}
{"task": "task108_contextualabusedetection_classification.json", "shot": 5, "num": 1998, "correct": 1192, "acc": 0.5965965965965966}
{"task": "task702_mmmlu_answer_generation_high_school_european_history.json", "shot": 0, "num": 85, "correct": 0, "acc": 0.0}
{"task": "task702_mmmlu_answer_generation_high_school_european_history.json", "shot": 5, "num": 85, "correct": 24, "acc": 0.2823529411764706}
{"task": "task595_mocha_answer_generation.json", "shot": 0, "num": 2475, "correct": 0, "acc": 0.0}
{"task": "task595_mocha_answer_generation.json", "shot": 5, "num": 2475, "correct": 71, "acc": 0.028686868686868688}
{"task": "task347_hybridqa_incorrect_answer_generation.json", "shot": 0, "num": 3248, "correct": 0, "acc": 0.0}
{"task": "task347_hybridqa_incorrect_answer_generation.json", "shot": 5, "num": 3248, "correct": 106, "acc": 0.03263546798029557}
{"task": "task550_discofuse_sentence_generation.json", "shot": 0, "num": 300, "correct": 0, "acc": 0.0}
{"task": "task550_discofuse_sentence_generation.json", "shot": 5, "num": 300, "correct": 0, "acc": 0.0}
{"task": "task223_quartz_explanation_generation.json", "shot": 0, "num": 1931, "correct": 0, "acc": 0.0}
{"task": "task223_quartz_explanation_generation.json", "shot": 5, "num": 1931, "correct": 6, "acc": 0.0031071983428275505}
{"task": "task364_regard_social_impact_classification.json", "shot": 0, "num": 139, "correct": 22, "acc": 0.15827338129496402}
{"task": "task364_regard_social_impact_classification.json", "shot": 5, "num": 139, "correct": 99, "acc": 0.7122302158273381}
{"task": "task1294_wiki_qa_answer_verification.json", "shot": 0, "num": 1098, "correct": 610, "acc": 0.5555555555555556}
{"task": "task1294_wiki_qa_answer_verification.json", "shot": 5, "num": 1098, "correct": 674, "acc": 0.6138433515482696}
{"task": "task924_event2mind_word_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task924_event2mind_word_generation.json", "shot": 5, "num": 3250, "correct": 100, "acc": 0.03076923076923077}
{"task": "task521_trivia_question_classification.json", "shot": 0, "num": 3246, "correct": 32, "acc": 0.009858287122612447}
{"task": "task521_trivia_question_classification.json", "shot": 5, "num": 3246, "correct": 1694, "acc": 0.5218730745532963}
{"task": "task063_first_i_elements.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task063_first_i_elements.json", "shot": 5, "num": 3250, "correct": 900, "acc": 0.27692307692307694}
{"task": "task1289_trec_classification.json", "shot": 0, "num": 2688, "correct": 253, "acc": 0.09412202380952381}
{"task": "task1289_trec_classification.json", "shot": 5, "num": 2688, "correct": 1125, "acc": 0.4185267857142857}
{"task": "task076_splash_correcting_sql_mistake.json", "shot": 0, "num": 1049, "correct": 0, "acc": 0.0}
{"task": "task076_splash_correcting_sql_mistake.json", "shot": 5, "num": 1049, "correct": 44, "acc": 0.04194470924690181}
{"task": "task164_mcscript_question_answering_text.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task164_mcscript_question_answering_text.json", "shot": 5, "num": 3250, "correct": 1030, "acc": 0.3169230769230769}
{"task": "task397_semeval_2018_task1_tweet_anger_detection.json", "shot": 0, "num": 1301, "correct": 80, "acc": 0.061491160645657184}
{"task": "task397_semeval_2018_task1_tweet_anger_detection.json", "shot": 5, "num": 1301, "correct": 910, "acc": 0.6994619523443505}
{"task": "task506_position_of_all_alphabetical_elements_in_list.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task506_position_of_all_alphabetical_elements_in_list.json", "shot": 5, "num": 3250, "correct": 41, "acc": 0.012615384615384615}
{"task": "task1713_convai3_sentence_generation.json", "shot": 0, "num": 91, "correct": 0, "acc": 0.0}
{"task": "task1713_convai3_sentence_generation.json", "shot": 5, "num": 91, "correct": 0, "acc": 0.0}
{"task": "task625_xlwic_true_or_false_answer_generation.json", "shot": 0, "num": 72, "correct": 3, "acc": 0.041666666666666664}
{"task": "task625_xlwic_true_or_false_answer_generation.json", "shot": 5, "num": 72, "correct": 25, "acc": 0.3472222222222222}
{"task": "task129_scan_long_text_generation_action_command_short.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task129_scan_long_text_generation_action_command_short.json", "shot": 5, "num": 3249, "correct": 6, "acc": 0.0018467220683287165}
{"task": "task1089_check_monotonic_array.json", "shot": 0, "num": 80, "correct": 0, "acc": 0.0}
{"task": "task1089_check_monotonic_array.json", "shot": 5, "num": 80, "correct": 52, "acc": 0.65}
{"task": "task585_preposition_classification.json", "shot": 0, "num": 463, "correct": 0, "acc": 0.0}
{"task": "task585_preposition_classification.json", "shot": 5, "num": 463, "correct": 79, "acc": 0.17062634989200864}
{"task": "task457_matres_conditional_classification.json", "shot": 0, "num": 315, "correct": 0, "acc": 0.0}
{"task": "task457_matres_conditional_classification.json", "shot": 5, "num": 315, "correct": 187, "acc": 0.5936507936507937}
{"task": "task1705_ljspeech_classification.json", "shot": 0, "num": 50, "correct": 0, "acc": 0.0}
{"task": "task1705_ljspeech_classification.json", "shot": 5, "num": 50, "correct": 25, "acc": 0.5}
{"task": "task694_mmmlu_answer_generation_econometrics.json", "shot": 0, "num": 64, "correct": 1, "acc": 0.015625}
{"task": "task694_mmmlu_answer_generation_econometrics.json", "shot": 5, "num": 64, "correct": 18, "acc": 0.28125}
{"task": "task1486_cell_extraction_anem_dataset.json", "shot": 0, "num": 62, "correct": 0, "acc": 0.0}
{"task": "task1486_cell_extraction_anem_dataset.json", "shot": 5, "num": 62, "correct": 34, "acc": 0.5483870967741935}
{"task": "task318_stereoset_classification_gender.json", "shot": 0, "num": 362, "correct": 0, "acc": 0.0}
{"task": "task318_stereoset_classification_gender.json", "shot": 5, "num": 362, "correct": 201, "acc": 0.5552486187845304}
{"task": "task179_participant_extraction.json", "shot": 0, "num": 212, "correct": 46, "acc": 0.2169811320754717}
{"task": "task179_participant_extraction.json", "shot": 5, "num": 212, "correct": 109, "acc": 0.5141509433962265}
{"task": "task140_detoxifying-lms_classification_style.json", "shot": 0, "num": 195, "correct": 0, "acc": 0.0}
{"task": "task140_detoxifying-lms_classification_style.json", "shot": 5, "num": 195, "correct": 18, "acc": 0.09230769230769231}
{"task": "task1573_samsum_classification.json", "shot": 0, "num": 154, "correct": 32, "acc": 0.2077922077922078}
{"task": "task1573_samsum_classification.json", "shot": 5, "num": 154, "correct": 74, "acc": 0.4805194805194805}
{"task": "task161_count_words_containing_letter.json", "shot": 0, "num": 3214, "correct": 0, "acc": 0.0}
{"task": "task161_count_words_containing_letter.json", "shot": 5, "num": 3214, "correct": 788, "acc": 0.24517734909769756}
{"task": "task1657_gooaq_question_generation.json", "shot": 0, "num": 52, "correct": 0, "acc": 0.0}
{"task": "task1657_gooaq_question_generation.json", "shot": 5, "num": 52, "correct": 2, "acc": 0.038461538461538464}
{"task": "task720_mmmlu_answer_generation_marketing.json", "shot": 0, "num": 89, "correct": 49, "acc": 0.550561797752809}
{"task": "task720_mmmlu_answer_generation_marketing.json", "shot": 5, "num": 89, "correct": 67, "acc": 0.7528089887640449}
{"task": "task1501_dstc3_answer_generation.json", "shot": 0, "num": 725, "correct": 1, "acc": 0.001379310344827586}
{"task": "task1501_dstc3_answer_generation.json", "shot": 5, "num": 725, "correct": 324, "acc": 0.44689655172413795}
{"task": "task1331_reverse_array.json", "shot": 0, "num": 98, "correct": 0, "acc": 0.0}
{"task": "task1331_reverse_array.json", "shot": 5, "num": 98, "correct": 0, "acc": 0.0}
{"task": "task719_mmmlu_answer_generation_management.json", "shot": 0, "num": 58, "correct": 12, "acc": 0.20689655172413793}
{"task": "task719_mmmlu_answer_generation_management.json", "shot": 5, "num": 58, "correct": 39, "acc": 0.6724137931034483}
{"task": "task631_dbpedia_14_incorrect_answer_generation.json", "shot": 0, "num": 929, "correct": 0, "acc": 0.0}
{"task": "task631_dbpedia_14_incorrect_answer_generation.json", "shot": 5, "num": 929, "correct": 23, "acc": 0.024757804090419805}
{"task": "task1483_chemical_extraction_chemprot_dataset.json", "shot": 0, "num": 142, "correct": 0, "acc": 0.0}
{"task": "task1483_chemical_extraction_chemprot_dataset.json", "shot": 5, "num": 142, "correct": 64, "acc": 0.4507042253521127}
{"task": "task1568_propara_classification.json", "shot": 0, "num": 81, "correct": 0, "acc": 0.0}
{"task": "task1568_propara_classification.json", "shot": 5, "num": 81, "correct": 26, "acc": 0.32098765432098764}
{"task": "task458_matres_negation_classification.json", "shot": 0, "num": 387, "correct": 38, "acc": 0.09819121447028424}
{"task": "task458_matres_negation_classification.json", "shot": 5, "num": 387, "correct": 214, "acc": 0.5529715762273901}
{"task": "task098_conala_list_intersection.json", "shot": 0, "num": 2499, "correct": 0, "acc": 0.0}
{"task": "task098_conala_list_intersection.json", "shot": 5, "num": 2499, "correct": 176, "acc": 0.07042817126850741}
{"task": "task1724_civil_comments_insult_classification.json", "shot": 0, "num": 500, "correct": 8, "acc": 0.016}
{"task": "task1724_civil_comments_insult_classification.json", "shot": 5, "num": 500, "correct": 286, "acc": 0.572}
{"task": "task1427_country_region_in_world.json", "shot": 0, "num": 119, "correct": 1, "acc": 0.008403361344537815}
{"task": "task1427_country_region_in_world.json", "shot": 5, "num": 119, "correct": 63, "acc": 0.5294117647058824}
{"task": "task1670_md_gender_bias_text_modification.json", "shot": 0, "num": 500, "correct": 0, "acc": 0.0}
{"task": "task1670_md_gender_bias_text_modification.json", "shot": 5, "num": 500, "correct": 9, "acc": 0.018}
{"task": "task586_amazonfood_polarity_classification.json", "shot": 0, "num": 3250, "correct": 4, "acc": 0.0012307692307692308}
{"task": "task586_amazonfood_polarity_classification.json", "shot": 5, "num": 3250, "correct": 3043, "acc": 0.9363076923076923}
{"task": "task413_mickey_en_sentence_perturbation_generation.json", "shot": 0, "num": 3241, "correct": 0, "acc": 0.0}
{"task": "task413_mickey_en_sentence_perturbation_generation.json", "shot": 5, "num": 3241, "correct": 0, "acc": 0.0}
{"task": "task1310_amazonreview_rating_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1310_amazonreview_rating_classification.json", "shot": 5, "num": 3250, "correct": 1861, "acc": 0.5726153846153846}
{"task": "task834_mathdataset_classification.json", "shot": 0, "num": 1992, "correct": 0, "acc": 0.0}
{"task": "task834_mathdataset_classification.json", "shot": 5, "num": 1992, "correct": 1361, "acc": 0.6832329317269076}
{"task": "task735_mmmlu_answer_generation_us_foreign_policy.json", "shot": 0, "num": 56, "correct": 15, "acc": 0.26785714285714285}
{"task": "task735_mmmlu_answer_generation_us_foreign_policy.json", "shot": 5, "num": 56, "correct": 34, "acc": 0.6071428571428571}
{"task": "task686_mmmlu_answer_generation_college_biology.json", "shot": 0, "num": 81, "correct": 18, "acc": 0.2222222222222222}
{"task": "task686_mmmlu_answer_generation_college_biology.json", "shot": 5, "num": 81, "correct": 37, "acc": 0.4567901234567901}
{"task": "task600_find_the_longest_common_substring_in_two_strings.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task600_find_the_longest_common_substring_in_two_strings.json", "shot": 5, "num": 3250, "correct": 230, "acc": 0.07076923076923076}
{"task": "task956_leetcode_420_strong_password_check.json", "shot": 0, "num": 2599, "correct": 0, "acc": 0.0}
{"task": "task956_leetcode_420_strong_password_check.json", "shot": 5, "num": 2599, "correct": 173, "acc": 0.06656406310119277}
{"task": "task736_mmmlu_answer_generation_virology.json", "shot": 0, "num": 85, "correct": 19, "acc": 0.2235294117647059}
{"task": "task736_mmmlu_answer_generation_virology.json", "shot": 5, "num": 85, "correct": 33, "acc": 0.38823529411764707}
{"task": "task674_google_wellformed_query_sentence_generation.json", "shot": 0, "num": 1151, "correct": 0, "acc": 0.0}
{"task": "task674_google_wellformed_query_sentence_generation.json", "shot": 5, "num": 1151, "correct": 232, "acc": 0.20156385751520417}
{"task": "task492_mwsc_incorrect_answer_generation.json", "shot": 0, "num": 50, "correct": 0, "acc": 0.0}
{"task": "task492_mwsc_incorrect_answer_generation.json", "shot": 5, "num": 50, "correct": 23, "acc": 0.46}
{"task": "task342_winomt_classification_profession_pro.json", "shot": 0, "num": 789, "correct": 0, "acc": 0.0}
{"task": "task342_winomt_classification_profession_pro.json", "shot": 5, "num": 789, "correct": 589, "acc": 0.7465145754119138}
{"task": "task388_torque_token_classification.json", "shot": 0, "num": 1134, "correct": 0, "acc": 0.0}
{"task": "task388_torque_token_classification.json", "shot": 5, "num": 1134, "correct": 7, "acc": 0.006172839506172839}
{"task": "task431_senteval_object_count.json", "shot": 0, "num": 3249, "correct": 8, "acc": 0.0024622960911049553}
{"task": "task431_senteval_object_count.json", "shot": 5, "num": 3249, "correct": 1929, "acc": 0.5937211449676824}
{"task": "task629_dbpedia_14_classification.json", "shot": 0, "num": 1500, "correct": 0, "acc": 0.0}
{"task": "task629_dbpedia_14_classification.json", "shot": 5, "num": 1500, "correct": 384, "acc": 0.256}
{"task": "task1596_event2mind_text_generation_2.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1596_event2mind_text_generation_2.json", "shot": 5, "num": 3250, "correct": 35, "acc": 0.010769230769230769}
{"task": "task756_find_longert_substring_and_return_all_unique_alphabets_in_it.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task756_find_longert_substring_and_return_all_unique_alphabets_in_it.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1359_numer_sense_answer_generation.json", "shot": 0, "num": 2985, "correct": 0, "acc": 0.0}
{"task": "task1359_numer_sense_answer_generation.json", "shot": 5, "num": 2985, "correct": 869, "acc": 0.2911222780569514}
{"task": "task109_smsspamcollection_spamsmsdetection.json", "shot": 0, "num": 533, "correct": 20, "acc": 0.0375234521575985}
{"task": "task109_smsspamcollection_spamsmsdetection.json", "shot": 5, "num": 533, "correct": 399, "acc": 0.7485928705440901}
{"task": "task1205_atomic_classification_isafter.json", "shot": 0, "num": 3247, "correct": 66, "acc": 0.020326455189405606}
{"task": "task1205_atomic_classification_isafter.json", "shot": 5, "num": 3247, "correct": 1974, "acc": 0.607945796119495}
{"task": "task574_air_dialogue_sentence_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task574_air_dialogue_sentence_generation.json", "shot": 5, "num": 3250, "correct": 5, "acc": 0.0015384615384615385}
{"task": "task1424_mathqa_probability.json", "shot": 0, "num": 286, "correct": 0, "acc": 0.0}
{"task": "task1424_mathqa_probability.json", "shot": 5, "num": 286, "correct": 59, "acc": 0.2062937062937063}
{"task": "task849_pubmedqa_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task849_pubmedqa_answer_generation.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task907_dialogre_identify_relationships.json", "shot": 0, "num": 32, "correct": 0, "acc": 0.0}
{"task": "task907_dialogre_identify_relationships.json", "shot": 5, "num": 32, "correct": 14, "acc": 0.4375}
{"task": "task376_reverse_order_of_words.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task376_reverse_order_of_words.json", "shot": 5, "num": 3250, "correct": 17, "acc": 0.005230769230769231}
{"task": "task379_agnews_topic_classification.json", "shot": 0, "num": 3249, "correct": 6, "acc": 0.0018467220683287165}
{"task": "task379_agnews_topic_classification.json", "shot": 5, "num": 3249, "correct": 2295, "acc": 0.7063711911357341}
{"task": "task690_mmmlu_answer_generation_college_medicine.json", "shot": 0, "num": 63, "correct": 13, "acc": 0.20634920634920634}
{"task": "task690_mmmlu_answer_generation_college_medicine.json", "shot": 5, "num": 63, "correct": 26, "acc": 0.4126984126984127}
{"task": "task713_mmmlu_answer_generation_human_aging.json", "shot": 0, "num": 88, "correct": 21, "acc": 0.23863636363636365}
{"task": "task713_mmmlu_answer_generation_human_aging.json", "shot": 5, "num": 88, "correct": 50, "acc": 0.5681818181818182}
{"task": "task1146_country_capital.json", "shot": 0, "num": 116, "correct": 0, "acc": 0.0}
{"task": "task1146_country_capital.json", "shot": 5, "num": 116, "correct": 87, "acc": 0.75}
{"task": "task609_sbic_potentially_offense_binary_classification.json", "shot": 0, "num": 1473, "correct": 0, "acc": 0.0}
{"task": "task609_sbic_potentially_offense_binary_classification.json", "shot": 5, "num": 1473, "correct": 720, "acc": 0.48879837067209775}
{"task": "task105_story_cloze-rocstories_sentence_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task105_story_cloze-rocstories_sentence_generation.json", "shot": 5, "num": 3250, "correct": 2, "acc": 0.0006153846153846154}
{"task": "task1208_atomic_classification_xreason.json", "shot": 0, "num": 366, "correct": 0, "acc": 0.0}
{"task": "task1208_atomic_classification_xreason.json", "shot": 5, "num": 366, "correct": 240, "acc": 0.6557377049180327}
{"task": "task724_mmmlu_answer_generation_moral_scenarios.json", "shot": 0, "num": 126, "correct": 0, "acc": 0.0}
{"task": "task724_mmmlu_answer_generation_moral_scenarios.json", "shot": 5, "num": 126, "correct": 35, "acc": 0.2777777777777778}
{"task": "task737_mmmlu_answer_generation_world_religions.json", "shot": 0, "num": 86, "correct": 36, "acc": 0.4186046511627907}
{"task": "task737_mmmlu_answer_generation_world_religions.json", "shot": 5, "num": 86, "correct": 58, "acc": 0.6744186046511628}
{"task": "task1341_msr_text_classification.json", "shot": 0, "num": 87, "correct": 37, "acc": 0.42528735632183906}
{"task": "task1341_msr_text_classification.json", "shot": 5, "num": 87, "correct": 51, "acc": 0.5862068965517241}
{"task": "task1211_atomic_classification_hassubevent.json", "shot": 0, "num": 3247, "correct": 926, "acc": 0.2851863258392362}
{"task": "task1211_atomic_classification_hassubevent.json", "shot": 5, "num": 3247, "correct": 1874, "acc": 0.5771481367416076}
{"task": "task870_msmarco_answer_generation.json", "shot": 5, "num": 367, "correct": 0, "acc": 0.0}
{"task": "task870_msmarco_answer_generation.json", "shot": 0, "num": 367, "correct": 0, "acc": 0.0}
{"task": "task1212_atomic_classification_hasproperty.json", "shot": 0, "num": 3247, "correct": 0, "acc": 0.0}
{"task": "task1212_atomic_classification_hasproperty.json", "shot": 5, "num": 3247, "correct": 2210, "acc": 0.680628272251309}
{"task": "task490_mwsc_options_generation.json", "shot": 0, "num": 50, "correct": 1, "acc": 0.02}
{"task": "task490_mwsc_options_generation.json", "shot": 5, "num": 50, "correct": 20, "acc": 0.4}
{"task": "task399_semeval_2018_task1_tweet_sadness_detection.json", "shot": 0, "num": 1450, "correct": 0, "acc": 0.0}
{"task": "task399_semeval_2018_task1_tweet_sadness_detection.json", "shot": 5, "num": 1450, "correct": 1079, "acc": 0.7441379310344828}
{"task": "task1204_atomic_classification_hinderedby.json", "shot": 0, "num": 3246, "correct": 0, "acc": 0.0}
{"task": "task1204_atomic_classification_hinderedby.json", "shot": 5, "num": 3246, "correct": 2021, "acc": 0.6226124460874923}
{"task": "task632_dbpedia_14_classification.json", "shot": 0, "num": 1000, "correct": 961, "acc": 0.961}
{"task": "task632_dbpedia_14_classification.json", "shot": 5, "num": 1000, "correct": 814, "acc": 0.814}
{"task": "task154_tomqa_find_location_hard_noise.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task154_tomqa_find_location_hard_noise.json", "shot": 5, "num": 3250, "correct": 1210, "acc": 0.3723076923076923}
{"task": "task471_haspart_answer_generation.json", "shot": 5, "num": 1548, "correct": 44, "acc": 0.028423772609819122}
{"task": "task471_haspart_answer_generation.json", "shot": 0, "num": 1548, "correct": 0, "acc": 0.0}
{"task": "task191_hotpotqa_question_generation.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task191_hotpotqa_question_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task160_replace_letter_in_a_sentence.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task160_replace_letter_in_a_sentence.json", "shot": 5, "num": 3250, "correct": 6, "acc": 0.0018461538461538461}
{"task": "task577_curiosity_dialogs_classification.json", "shot": 0, "num": 3006, "correct": 31, "acc": 0.010312707917498337}
{"task": "task577_curiosity_dialogs_classification.json", "shot": 5, "num": 3006, "correct": 1348, "acc": 0.44843646041250834}
{"task": "task1360_numer_sense_multiple_choice_qa_generation.json", "shot": 0, "num": 3116, "correct": 0, "acc": 0.0}
{"task": "task1360_numer_sense_multiple_choice_qa_generation.json", "shot": 5, "num": 3116, "correct": 817, "acc": 0.2621951219512195}
{"task": "task1446_farthest_integers.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1446_farthest_integers.json", "shot": 5, "num": 3250, "correct": 88, "acc": 0.02707692307692308}
{"task": "task328_jigsaw_classification_insult.json", "shot": 0, "num": 3107, "correct": 1, "acc": 0.000321853878339234}
{"task": "task328_jigsaw_classification_insult.json", "shot": 5, "num": 3107, "correct": 2083, "acc": 0.6704216285806244}
{"task": "task375_classify_type_of_sentence_in_debate.json", "shot": 0, "num": 200, "correct": 0, "acc": 0.0}
{"task": "task375_classify_type_of_sentence_in_debate.json", "shot": 5, "num": 200, "correct": 79, "acc": 0.395}
{"task": "task1338_peixian_equity_evaluation_corpus_sentiment_classifier.json", "shot": 0, "num": 3000, "correct": 0, "acc": 0.0}
{"task": "task1338_peixian_equity_evaluation_corpus_sentiment_classifier.json", "shot": 5, "num": 3000, "correct": 2676, "acc": 0.892}
{"task": "task319_stereoset_classification_profession.json", "shot": 0, "num": 1238, "correct": 0, "acc": 0.0}
{"task": "task319_stereoset_classification_profession.json", "shot": 5, "num": 1238, "correct": 760, "acc": 0.6138933764135702}
{"task": "task1604_ethos_text_classification.json", "shot": 0, "num": 499, "correct": 0, "acc": 0.0}
{"task": "task1604_ethos_text_classification.json", "shot": 5, "num": 499, "correct": 355, "acc": 0.7114228456913828}
{"task": "task1506_celebrity_minimal_dob_span.json", "shot": 0, "num": 115, "correct": 0, "acc": 0.0}
{"task": "task1506_celebrity_minimal_dob_span.json", "shot": 5, "num": 115, "correct": 101, "acc": 0.8782608695652174}
{"task": "task1451_drug_dose_extraction.json", "shot": 0, "num": 111, "correct": 0, "acc": 0.0}
{"task": "task1451_drug_dose_extraction.json", "shot": 5, "num": 111, "correct": 64, "acc": 0.5765765765765766}
{"task": "task128_scan_structured_text_generation_command_action_short.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task128_scan_structured_text_generation_command_action_short.json", "shot": 5, "num": 3250, "correct": 21, "acc": 0.006461538461538461}
{"task": "task871_msmarco_question_generation.json", "shot": 5, "num": 50, "correct": 0, "acc": 0.0}
{"task": "task871_msmarco_question_generation.json", "shot": 0, "num": 50, "correct": 0, "acc": 0.0}
{"task": "task296_storycloze_correct_end_classification.json", "shot": 0, "num": 942, "correct": 0, "acc": 0.0}
{"task": "task296_storycloze_correct_end_classification.json", "shot": 5, "num": 942, "correct": 509, "acc": 0.5403397027600849}
{"task": "task1602_webquestion_question_genreation.json", "shot": 0, "num": 1581, "correct": 0, "acc": 0.0}
{"task": "task1602_webquestion_question_genreation.json", "shot": 5, "num": 1581, "correct": 32, "acc": 0.02024035420619861}
{"task": "task1290_xsum_summarization.json", "shot": 0, "num": 3247, "correct": 0, "acc": 0.0}
{"task": "task1290_xsum_summarization.json", "shot": 5, "num": 3247, "correct": 0, "acc": 0.0}
{"task": "task666_mmmlu_answer_generation_astronomy.json", "shot": 0, "num": 84, "correct": 8, "acc": 0.09523809523809523}
{"task": "task666_mmmlu_answer_generation_astronomy.json", "shot": 5, "num": 84, "correct": 33, "acc": 0.39285714285714285}
{"task": "task850_synthetic_longest_palindrome.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task850_synthetic_longest_palindrome.json", "shot": 5, "num": 3250, "correct": 60, "acc": 0.018461538461538463}
{"task": "task1215_atomic_classification_capableof.json", "shot": 0, "num": 3246, "correct": 0, "acc": 0.0}
{"task": "task1215_atomic_classification_capableof.json", "shot": 5, "num": 3246, "correct": 1939, "acc": 0.5973505853357979}
{"task": "task517_emo_classify_emotion_of_dialogue.json", "shot": 0, "num": 3239, "correct": 9, "acc": 0.002778635381290522}
{"task": "task517_emo_classify_emotion_of_dialogue.json", "shot": 5, "num": 3239, "correct": 2548, "acc": 0.7866625501698055}
{"task": "task1423_mathqa_geometry.json", "shot": 0, "num": 1311, "correct": 0, "acc": 0.0}
{"task": "task1423_mathqa_geometry.json", "shot": 5, "num": 1311, "correct": 259, "acc": 0.19755911517925248}
{"task": "task278_stereoset_sentence_generation_antistereotype.json", "shot": 0, "num": 369, "correct": 0, "acc": 0.0}
{"task": "task278_stereoset_sentence_generation_antistereotype.json", "shot": 5, "num": 369, "correct": 4, "acc": 0.01084010840108401}
{"task": "task276_enhanced_wsc_classification.json", "shot": 0, "num": 729, "correct": 0, "acc": 0.0}
{"task": "task276_enhanced_wsc_classification.json", "shot": 5, "num": 729, "correct": 171, "acc": 0.2345679012345679}
{"task": "task905_hate_speech_offensive_classification.json", "shot": 0, "num": 3250, "correct": 9, "acc": 0.002769230769230769}
{"task": "task905_hate_speech_offensive_classification.json", "shot": 5, "num": 3250, "correct": 1089, "acc": 0.33507692307692305}
{"task": "task194_duorc_answer_generation.json", "shot": 5, "num": 3201, "correct": 18, "acc": 0.005623242736644799}
{"task": "task194_duorc_answer_generation.json", "shot": 0, "num": 3201, "correct": 11, "acc": 0.003436426116838488}
{"task": "task753_svamp_addition_question_answering.json", "shot": 0, "num": 98, "correct": 0, "acc": 0.0}
{"task": "task753_svamp_addition_question_answering.json", "shot": 5, "num": 98, "correct": 34, "acc": 0.3469387755102041}
{"task": "task268_casehold_legal_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task268_casehold_legal_answer_generation.json", "shot": 5, "num": 3250, "correct": 1038, "acc": 0.3193846153846154}
{"task": "task868_cfq_mcd1_explanation_to_sql.json", "shot": 0, "num": 479, "correct": 0, "acc": 0.0}
{"task": "task868_cfq_mcd1_explanation_to_sql.json", "shot": 5, "num": 479, "correct": 300, "acc": 0.6263048016701461}
{"task": "task343_winomt_classification_profession_anti.json", "shot": 0, "num": 789, "correct": 0, "acc": 0.0}
{"task": "task343_winomt_classification_profession_anti.json", "shot": 5, "num": 789, "correct": 499, "acc": 0.632446134347275}
{"task": "task310_race_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task310_race_classification.json", "shot": 5, "num": 3250, "correct": 628, "acc": 0.19323076923076923}
{"task": "task1656_gooaq_answer_generation.json", "shot": 0, "num": 55, "correct": 0, "acc": 0.0}
{"task": "task1656_gooaq_answer_generation.json", "shot": 5, "num": 55, "correct": 10, "acc": 0.18181818181818182}
{"task": "task351_winomt_classification_gender_identifiability_anti.json", "shot": 0, "num": 1577, "correct": 0, "acc": 0.0}
{"task": "task351_winomt_classification_gender_identifiability_anti.json", "shot": 5, "num": 1577, "correct": 671, "acc": 0.4254914394419784}
{"task": "task1336_peixian_equity_evaluation_corpus_gender_classifier.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1336_peixian_equity_evaluation_corpus_gender_classifier.json", "shot": 5, "num": 3250, "correct": 3089, "acc": 0.9504615384615385}
{"task": "task372_synthetic_palindrome_numbers.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task372_synthetic_palindrome_numbers.json", "shot": 5, "num": 3250, "correct": 20, "acc": 0.006153846153846154}
{"task": "task1400_obqa_incorrect_answer_generation.json", "shot": 0, "num": 884, "correct": 0, "acc": 0.0}
{"task": "task1400_obqa_incorrect_answer_generation.json", "shot": 5, "num": 884, "correct": 0, "acc": 0.0}
{"task": "task1087_two_number_sum.json", "shot": 0, "num": 100, "correct": 0, "acc": 0.0}
{"task": "task1087_two_number_sum.json", "shot": 5, "num": 100, "correct": 2, "acc": 0.02}
{"task": "task1517_limit_classfication.json", "shot": 0, "num": 998, "correct": 2, "acc": 0.002004008016032064}
{"task": "task1517_limit_classfication.json", "shot": 5, "num": 998, "correct": 611, "acc": 0.6122244488977956}
{"task": "task718_mmmlu_answer_generation_machine_learning.json", "shot": 0, "num": 63, "correct": 5, "acc": 0.07936507936507936}
{"task": "task718_mmmlu_answer_generation_machine_learning.json", "shot": 5, "num": 63, "correct": 18, "acc": 0.2857142857142857}
{"task": "task515_senteval_odd_word_out.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task515_senteval_odd_word_out.json", "shot": 5, "num": 3250, "correct": 1469, "acc": 0.452}
{"task": "task835_mathdataset_answer_generation.json", "shot": 0, "num": 1993, "correct": 0, "acc": 0.0}
{"task": "task835_mathdataset_answer_generation.json", "shot": 5, "num": 1993, "correct": 139, "acc": 0.06974410436527848}
{"task": "task509_collate_of_all_alphabetical_and_numerical_elements_in_list_separately.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task509_collate_of_all_alphabetical_and_numerical_elements_in_list_separately.json", "shot": 5, "num": 3250, "correct": 243, "acc": 0.07476923076923077}
{"task": "task150_afs_argument_quality_gun_control.json", "shot": 0, "num": 1068, "correct": 0, "acc": 0.0}
{"task": "task150_afs_argument_quality_gun_control.json", "shot": 5, "num": 1068, "correct": 806, "acc": 0.7546816479400749}
{"task": "task725_mmmlu_answer_generation_nutrition.json", "shot": 0, "num": 93, "correct": 15, "acc": 0.16129032258064516}
{"task": "task725_mmmlu_answer_generation_nutrition.json", "shot": 5, "num": 93, "correct": 42, "acc": 0.45161290322580644}
{"task": "task705_mmmlu_answer_generation_high_school_macroeconomics.json", "shot": 0, "num": 98, "correct": 20, "acc": 0.20408163265306123}
{"task": "task705_mmmlu_answer_generation_high_school_macroeconomics.json", "shot": 5, "num": 98, "correct": 38, "acc": 0.3877551020408163}
{"task": "task059_ropes_story_generation.json", "shot": 0, "num": 261, "correct": 0, "acc": 0.0}
{"task": "task059_ropes_story_generation.json", "shot": 5, "num": 261, "correct": 0, "acc": 0.0}
{"task": "task681_hope_edi_malayalam_text_classification.json", "shot": 0, "num": 985, "correct": 0, "acc": 0.0}
{"task": "task681_hope_edi_malayalam_text_classification.json", "shot": 5, "num": 985, "correct": 379, "acc": 0.3847715736040609}
{"task": "task270_csrg_counterfactual_context_generation.json", "shot": 0, "num": 3200, "correct": 0, "acc": 0.0}
{"task": "task270_csrg_counterfactual_context_generation.json", "shot": 5, "num": 3200, "correct": 22, "acc": 0.006875}
{"task": "task320_stereoset_classification_race.json", "shot": 0, "num": 1463, "correct": 0, "acc": 0.0}
{"task": "task320_stereoset_classification_race.json", "shot": 5, "num": 1463, "correct": 859, "acc": 0.5871496924128503}
{"task": "task1581_eqasc-perturbed_answer_generation.json", "shot": 0, "num": 410, "correct": 0, "acc": 0.0}
{"task": "task1581_eqasc-perturbed_answer_generation.json", "shot": 5, "num": 410, "correct": 295, "acc": 0.7195121951219512}
{"task": "task1364_hans_answer_generation.json", "shot": 0, "num": 1499, "correct": 0, "acc": 0.0}
{"task": "task1364_hans_answer_generation.json", "shot": 5, "num": 1499, "correct": 349, "acc": 0.23282188125416944}
{"task": "task1192_food_flavor_profile.json", "shot": 0, "num": 109, "correct": 43, "acc": 0.3944954128440367}
{"task": "task1192_food_flavor_profile.json", "shot": 5, "num": 109, "correct": 88, "acc": 0.8073394495412844}
{"task": "task1368_healthfact_sentence_generation.json", "shot": 0, "num": 1647, "correct": 0, "acc": 0.0}
{"task": "task1368_healthfact_sentence_generation.json", "shot": 5, "num": 1647, "correct": 0, "acc": 0.0}
{"task": "task089_swap_words_verification.json", "shot": 0, "num": 3248, "correct": 0, "acc": 0.0}
{"task": "task089_swap_words_verification.json", "shot": 5, "num": 3248, "correct": 364, "acc": 0.11206896551724138}
{"task": "task746_yelp_restaurant_review_classification.json", "shot": 0, "num": 518, "correct": 2, "acc": 0.003861003861003861}
{"task": "task746_yelp_restaurant_review_classification.json", "shot": 5, "num": 518, "correct": 495, "acc": 0.9555984555984556}
{"task": "task633_dbpedia_14_answer_generation.json", "shot": 0, "num": 2450, "correct": 0, "acc": 0.0}
{"task": "task633_dbpedia_14_answer_generation.json", "shot": 5, "num": 2450, "correct": 1858, "acc": 0.7583673469387755}
{"task": "task573_air_dialogue_classification.json", "shot": 0, "num": 1318, "correct": 0, "acc": 0.0}
{"task": "task573_air_dialogue_classification.json", "shot": 5, "num": 1318, "correct": 797, "acc": 0.6047040971168437}
{"task": "task1319_country_by_barcode_prefix.json", "shot": 0, "num": 51, "correct": 0, "acc": 0.0}
{"task": "task1319_country_by_barcode_prefix.json", "shot": 5, "num": 51, "correct": 7, "acc": 0.13725490196078433}
{"task": "task1572_samsum_summary.json", "shot": 0, "num": 100, "correct": 0, "acc": 0.0}
{"task": "task1572_samsum_summary.json", "shot": 5, "num": 100, "correct": 0, "acc": 0.0}
{"task": "task860_prost_mcq_generation.json", "shot": 0, "num": 102, "correct": 0, "acc": 0.0}
{"task": "task860_prost_mcq_generation.json", "shot": 5, "num": 102, "correct": 0, "acc": 0.0}
{"task": "task491_mwsc_answer_generation.json", "shot": 0, "num": 50, "correct": 0, "acc": 0.0}
{"task": "task491_mwsc_answer_generation.json", "shot": 5, "num": 50, "correct": 25, "acc": 0.5}
{"task": "task637_extract_and_sort_unique_digits_in_a_list.json", "shot": 0, "num": 3244, "correct": 0, "acc": 0.0}
{"task": "task637_extract_and_sort_unique_digits_in_a_list.json", "shot": 5, "num": 3244, "correct": 68, "acc": 0.02096177558569667}
{"task": "task868_mawps_singleop_question_answering.json", "shot": 0, "num": 479, "correct": 0, "acc": 0.0}
{"task": "task868_mawps_singleop_question_answering.json", "shot": 5, "num": 479, "correct": 300, "acc": 0.6263048016701461}
{"task": "task608_sbic_sexual_offense_binary_classification.json", "shot": 0, "num": 1858, "correct": 0, "acc": 0.0}
{"task": "task608_sbic_sexual_offense_binary_classification.json", "shot": 5, "num": 1858, "correct": 591, "acc": 0.31808396124865446}
{"task": "task1607_ethos_text_classification.json", "shot": 0, "num": 86, "correct": 3, "acc": 0.03488372093023256}
{"task": "task1607_ethos_text_classification.json", "shot": 5, "num": 86, "correct": 48, "acc": 0.5581395348837209}
{"task": "task045_miscellaneous_sentence_paraphrasing.json", "shot": 0, "num": 96, "correct": 0, "acc": 0.0}
{"task": "task045_miscellaneous_sentence_paraphrasing.json", "shot": 5, "num": 96, "correct": 0, "acc": 0.0}
{"task": "task103_facts2story_long_text_generation.json", "shot": 0, "num": 3221, "correct": 0, "acc": 0.0}
{"task": "task103_facts2story_long_text_generation.json", "shot": 5, "num": 3221, "correct": 0, "acc": 0.0}
{"task": "task1448_disease_entity_extraction_ncbi_dataset.json", "shot": 0, "num": 76, "correct": 0, "acc": 0.0}
{"task": "task1448_disease_entity_extraction_ncbi_dataset.json", "shot": 5, "num": 76, "correct": 49, "acc": 0.6447368421052632}
{"task": "task065_timetravel_consistent_sentence_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task065_timetravel_consistent_sentence_classification.json", "shot": 5, "num": 3250, "correct": 1339, "acc": 0.412}
{"task": "task1488_sarcasmdetection_headline_classification.json", "shot": 0, "num": 252, "correct": 0, "acc": 0.0}
{"task": "task1488_sarcasmdetection_headline_classification.json", "shot": 5, "num": 252, "correct": 155, "acc": 0.6150793650793651}
{"task": "task1548_wiqa_binary_classification.json", "shot": 0, "num": 296, "correct": 0, "acc": 0.0}
{"task": "task1548_wiqa_binary_classification.json", "shot": 5, "num": 296, "correct": 159, "acc": 0.5371621621621622}
{"task": "task455_swag_context_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task455_swag_context_generation.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1499_dstc3_summarization.json", "shot": 0, "num": 1130, "correct": 0, "acc": 0.0}
{"task": "task1499_dstc3_summarization.json", "shot": 5, "num": 1130, "correct": 0, "acc": 0.0}
{"task": "task047_miscellaneous_answering_science_questions.json", "shot": 0, "num": 126, "correct": 0, "acc": 0.0}
{"task": "task047_miscellaneous_answering_science_questions.json", "shot": 5, "num": 126, "correct": 86, "acc": 0.6825396825396826}
{"task": "task596_mocha_question_generation.json", "shot": 0, "num": 2475, "correct": 0, "acc": 0.0}
{"task": "task596_mocha_question_generation.json", "shot": 5, "num": 2475, "correct": 40, "acc": 0.01616161616161616}
{"task": "task099_reverse_elements_between_index_i_and_j.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task099_reverse_elements_between_index_i_and_j.json", "shot": 5, "num": 3250, "correct": 53, "acc": 0.016307692307692308}
{"task": "task1403_check_validity_date_mmddyyyy.json", "shot": 0, "num": 98, "correct": 0, "acc": 0.0}
{"task": "task1403_check_validity_date_mmddyyyy.json", "shot": 5, "num": 98, "correct": 66, "acc": 0.673469387755102}
{"task": "task852_synthetic_multiply_odds.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task852_synthetic_multiply_odds.json", "shot": 5, "num": 3250, "correct": 1, "acc": 0.0003076923076923077}
{"task": "task1584_evalution_meronym_classification.json", "shot": 0, "num": 540, "correct": 0, "acc": 0.0}
{"task": "task1584_evalution_meronym_classification.json", "shot": 5, "num": 540, "correct": 449, "acc": 0.8314814814814815}
{"task": "task148_afs_argument_quality_gay_marriage.json", "shot": 0, "num": 631, "correct": 0, "acc": 0.0}
{"task": "task148_afs_argument_quality_gay_marriage.json", "shot": 5, "num": 631, "correct": 373, "acc": 0.5911251980982567}
{"task": "task1567_propara_question_generation.json", "shot": 0, "num": 100, "correct": 0, "acc": 0.0}
{"task": "task1567_propara_question_generation.json", "shot": 5, "num": 100, "correct": 3, "acc": 0.03}
{"task": "task1213_atomic_classification_desires.json", "shot": 0, "num": 2600, "correct": 0, "acc": 0.0}
{"task": "task1213_atomic_classification_desires.json", "shot": 5, "num": 2600, "correct": 1358, "acc": 0.5223076923076924}
{"task": "task851_synthetic_multiply_evens.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task851_synthetic_multiply_evens.json", "shot": 5, "num": 3250, "correct": 1, "acc": 0.0003076923076923077}
{"task": "task903_deceptive_opinion_spam_classification.json", "shot": 0, "num": 795, "correct": 318, "acc": 0.4}
{"task": "task903_deceptive_opinion_spam_classification.json", "shot": 5, "num": 795, "correct": 612, "acc": 0.769811320754717}
{"task": "task1729_personachat_generate_next.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1729_personachat_generate_next.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task124_conala_pair_averages.json", "shot": 0, "num": 2500, "correct": 0, "acc": 0.0}
{"task": "task124_conala_pair_averages.json", "shot": 5, "num": 2500, "correct": 2, "acc": 0.0008}
{"task": "task523_find_if_numbers_or_alphabets_are_more_in_list.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task523_find_if_numbers_or_alphabets_are_more_in_list.json", "shot": 5, "num": 3250, "correct": 1463, "acc": 0.4501538461538461}
{"task": "task1203_atomic_classification_xreact.json", "shot": 0, "num": 3247, "correct": 201, "acc": 0.06190329534955343}
{"task": "task1203_atomic_classification_xreact.json", "shot": 5, "num": 3247, "correct": 1978, "acc": 0.6091777024946105}
{"task": "task371_synthetic_product_of_list.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task371_synthetic_product_of_list.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task833_poem_sentiment_classification.json", "shot": 0, "num": 142, "correct": 24, "acc": 0.16901408450704225}
{"task": "task833_poem_sentiment_classification.json", "shot": 5, "num": 142, "correct": 127, "acc": 0.8943661971830986}
{"task": "task700_mmmlu_answer_generation_high_school_chemistry.json", "shot": 0, "num": 87, "correct": 4, "acc": 0.04597701149425287}
{"task": "task700_mmmlu_answer_generation_high_school_chemistry.json", "shot": 5, "num": 87, "correct": 22, "acc": 0.25287356321839083}
{"task": "task1333_check_validity_date_ddmmyyyy.json", "shot": 0, "num": 89, "correct": 0, "acc": 0.0}
{"task": "task1333_check_validity_date_ddmmyyyy.json", "shot": 5, "num": 89, "correct": 45, "acc": 0.5056179775280899}
{"task": "task1678_mathqa_answer_selection.json", "shot": 0, "num": 2236, "correct": 0, "acc": 0.0}
{"task": "task1678_mathqa_answer_selection.json", "shot": 5, "num": 2236, "correct": 469, "acc": 0.20974955277280857}
{"task": "task107_splash_question_to_sql.json", "shot": 0, "num": 1016, "correct": 0, "acc": 0.0}
{"task": "task107_splash_question_to_sql.json", "shot": 5, "num": 1016, "correct": 13, "acc": 0.012795275590551181}
{"task": "task710_mmmlu_answer_generation_high_school_statistics.json", "shot": 0, "num": 88, "correct": 8, "acc": 0.09090909090909091}
{"task": "task710_mmmlu_answer_generation_high_school_statistics.json", "shot": 5, "num": 88, "correct": 20, "acc": 0.22727272727272727}
{"task": "task731_mmmlu_answer_generation_professional_psychology.json", "shot": 0, "num": 111, "correct": 26, "acc": 0.23423423423423423}
{"task": "task731_mmmlu_answer_generation_professional_psychology.json", "shot": 5, "num": 111, "correct": 45, "acc": 0.40540540540540543}
{"task": "task110_logic2text_sentence_generation.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task110_logic2text_sentence_generation.json", "shot": 5, "num": 3249, "correct": 145, "acc": 0.04462911665127731}
{"task": "task285_imdb_answer_generation.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task285_imdb_answer_generation.json", "shot": 5, "num": 3249, "correct": 2409, "acc": 0.7414589104339797}
{"task": "task151_tomqa_find_location_easy_clean.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task151_tomqa_find_location_easy_clean.json", "shot": 5, "num": 3250, "correct": 1925, "acc": 0.5923076923076923}
{"task": "task1480_gene_extraction_jnlpba_dataset.json", "shot": 0, "num": 600, "correct": 0, "acc": 0.0}
{"task": "task1480_gene_extraction_jnlpba_dataset.json", "shot": 5, "num": 600, "correct": 204, "acc": 0.34}
{"task": "task715_mmmlu_answer_generation_international_law.json", "shot": 0, "num": 68, "correct": 10, "acc": 0.14705882352941177}
{"task": "task715_mmmlu_answer_generation_international_law.json", "shot": 5, "num": 68, "correct": 37, "acc": 0.5441176470588235}
{"task": "task1196_atomic_classification_oeffect.json", "shot": 0, "num": 3247, "correct": 1509, "acc": 0.46473668001231905}
{"task": "task1196_atomic_classification_oeffect.json", "shot": 5, "num": 3247, "correct": 1449, "acc": 0.4462580843855867}
{"task": "task168_strategyqa_question_decomposition.json", "shot": 0, "num": 1511, "correct": 0, "acc": 0.0}
{"task": "task168_strategyqa_question_decomposition.json", "shot": 5, "num": 1511, "correct": 0, "acc": 0.0}
{"task": "task192_hotpotqa_sentence_generation.json", "shot": 0, "num": 2500, "correct": 0, "acc": 0.0}
{"task": "task192_hotpotqa_sentence_generation.json", "shot": 5, "num": 2500, "correct": 36, "acc": 0.0144}
{"task": "task695_mmmlu_answer_generation_electrical_engineering.json", "shot": 0, "num": 82, "correct": 1, "acc": 0.012195121951219513}
{"task": "task695_mmmlu_answer_generation_electrical_engineering.json", "shot": 5, "num": 82, "correct": 27, "acc": 0.32926829268292684}
{"task": "task679_hope_edi_english_text_classification.json", "shot": 0, "num": 1919, "correct": 0, "acc": 0.0}
{"task": "task679_hope_edi_english_text_classification.json", "shot": 5, "num": 1919, "correct": 1365, "acc": 0.7113079729025534}
{"task": "task178_quartz_question_answering.json", "shot": 0, "num": 1923, "correct": 0, "acc": 0.0}
{"task": "task178_quartz_question_answering.json", "shot": 5, "num": 1923, "correct": 1083, "acc": 0.5631825273010921}
{"task": "task459_matres_static_classification.json", "shot": 0, "num": 49, "correct": 14, "acc": 0.2857142857142857}
{"task": "task459_matres_static_classification.json", "shot": 5, "num": 49, "correct": 24, "acc": 0.4897959183673469}
{"task": "task1703_ljspeech_textmodification.json", "shot": 0, "num": 250, "correct": 0, "acc": 0.0}
{"task": "task1703_ljspeech_textmodification.json", "shot": 5, "num": 250, "correct": 45, "acc": 0.18}
{"task": "task387_semeval_2018_task3_irony_classification.json", "shot": 0, "num": 515, "correct": 0, "acc": 0.0}
{"task": "task387_semeval_2018_task3_irony_classification.json", "shot": 5, "num": 515, "correct": 160, "acc": 0.3106796116504854}
{"task": "task469_mrqa_answer_generation.json", "shot": 0, "num": 2264, "correct": 24, "acc": 0.01060070671378092}
{"task": "task469_mrqa_answer_generation.json", "shot": 5, "num": 2264, "correct": 922, "acc": 0.40724381625441697}
{"task": "task668_extreme_abstract_summarization.json", "shot": 0, "num": 1611, "correct": 0, "acc": 0.0}
{"task": "task668_extreme_abstract_summarization.json", "shot": 5, "num": 1611, "correct": 0, "acc": 0.0}
{"task": "task1322_country_government_type.json", "shot": 0, "num": 119, "correct": 0, "acc": 0.0}
{"task": "task1322_country_government_type.json", "shot": 5, "num": 119, "correct": 34, "acc": 0.2857142857142857}
{"task": "task294_storycommonsense_motiv_text_generation.json", "shot": 0, "num": 3250, "correct": 157, "acc": 0.04830769230769231}
{"task": "task294_storycommonsense_motiv_text_generation.json", "shot": 5, "num": 3250, "correct": 422, "acc": 0.12984615384615383}
{"task": "task088_identify_typo_verification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task088_identify_typo_verification.json", "shot": 5, "num": 3250, "correct": 2361, "acc": 0.7264615384615385}
{"task": "task1202_atomic_classification_xneed.json", "shot": 0, "num": 3247, "correct": 263, "acc": 0.08099784416384355}
{"task": "task1202_atomic_classification_xneed.json", "shot": 5, "num": 3247, "correct": 1729, "acc": 0.532491530643671}
{"task": "task580_socialiqa_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task580_socialiqa_answer_generation.json", "shot": 5, "num": 3250, "correct": 1539, "acc": 0.4735384615384615}
{"task": "task522_news_editorial_summary.json", "shot": 5, "num": 82, "correct": 0, "acc": 0.0}
{"task": "task522_news_editorial_summary.json", "shot": 0, "num": 82, "correct": 0, "acc": 0.0}
{"task": "task1482_gene_extraction_chemprot_dataset.json", "shot": 0, "num": 139, "correct": 2, "acc": 0.014388489208633094}
{"task": "task1482_gene_extraction_chemprot_dataset.json", "shot": 5, "num": 139, "correct": 76, "acc": 0.5467625899280576}
{"task": "task1595_event2mind_text_generation_1.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task1595_event2mind_text_generation_1.json", "shot": 5, "num": 3249, "correct": 97, "acc": 0.029855340104647583}
{"task": "task1284_hrngo_informativeness_classification.json", "shot": 0, "num": 889, "correct": 0, "acc": 0.0}
{"task": "task1284_hrngo_informativeness_classification.json", "shot": 5, "num": 889, "correct": 593, "acc": 0.6670416197975253}
{"task": "task672_amazon_and_yelp_summarization_dataset_summarization.json", "shot": 5, "num": 80, "correct": 0, "acc": 0.0}
{"task": "task672_amazon_and_yelp_summarization_dataset_summarization.json", "shot": 0, "num": 80, "correct": 0, "acc": 0.0}
{"task": "task672_nummersense.json", "shot": 5, "num": 80, "correct": 0, "acc": 0.0}
{"task": "task672_nummersense.json", "shot": 0, "num": 80, "correct": 0, "acc": 0.0}
{"task": "task1434_head_qa_classification.json", "shot": 0, "num": 1264, "correct": 0, "acc": 0.0}
{"task": "task1434_head_qa_classification.json", "shot": 5, "num": 1264, "correct": 520, "acc": 0.41139240506329117}
{"task": "task1318_country_national_dish.json", "shot": 0, "num": 82, "correct": 0, "acc": 0.0}
{"task": "task1318_country_national_dish.json", "shot": 5, "num": 82, "correct": 13, "acc": 0.15853658536585366}
{"task": "task1147_country_currency.json", "shot": 0, "num": 116, "correct": 0, "acc": 0.0}
{"task": "task1147_country_currency.json", "shot": 5, "num": 116, "correct": 55, "acc": 0.47413793103448276}
{"task": "task025_cosmosqa_incorrect_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task025_cosmosqa_incorrect_answer_generation.json", "shot": 5, "num": 3250, "correct": 1, "acc": 0.0003076923076923077}
{"task": "task1727_wiqa_what_is_the_effect.json", "shot": 0, "num": 3249, "correct": 62, "acc": 0.019082794706063406}
{"task": "task1727_wiqa_what_is_the_effect.json", "shot": 5, "num": 3249, "correct": 1349, "acc": 0.4152046783625731}
{"task": "task311_race_question_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task311_race_question_generation.json", "shot": 5, "num": 3250, "correct": 4, "acc": 0.0012307692307692308}
{"task": "task1314_country_abbreviation.json", "shot": 0, "num": 119, "correct": 2, "acc": 0.01680672268907563}
{"task": "task1314_country_abbreviation.json", "shot": 5, "num": 119, "correct": 104, "acc": 0.8739495798319328}
{"task": "task126_scan_structured_text_generation_command_action_all.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task126_scan_structured_text_generation_command_action_all.json", "shot": 5, "num": 3250, "correct": 19, "acc": 0.005846153846153846}
{"task": "task365_synthetic_remove_vowels.json", "shot": 0, "num": 3234, "correct": 0, "acc": 0.0}
{"task": "task365_synthetic_remove_vowels.json", "shot": 5, "num": 3234, "correct": 230, "acc": 0.07111935683364255}
{"task": "task345_hybridqa_answer_generation.json", "shot": 0, "num": 3248, "correct": 0, "acc": 0.0}
{"task": "task345_hybridqa_answer_generation.json", "shot": 5, "num": 3248, "correct": 0, "acc": 0.0}
{"task": "task1292_yelp_review_full_text_categorization.json", "shot": 0, "num": 3247, "correct": 0, "acc": 0.0}
{"task": "task1292_yelp_review_full_text_categorization.json", "shot": 5, "num": 3247, "correct": 2047, "acc": 0.6304280874653526}
{"task": "task861_prost_mcq_answers_generation.json", "shot": 0, "num": 423, "correct": 0, "acc": 0.0}
{"task": "task861_prost_mcq_answers_generation.json", "shot": 5, "num": 423, "correct": 253, "acc": 0.5981087470449172}
{"task": "task382_hybridqa_answer_generation.json", "shot": 0, "num": 3248, "correct": 0, "acc": 0.0}
{"task": "task382_hybridqa_answer_generation.json", "shot": 5, "num": 3248, "correct": 1107, "acc": 0.34082512315270935}
{"task": "task855_conv_ai_2_classification.json", "shot": 0, "num": 60, "correct": 0, "acc": 0.0}
{"task": "task855_conv_ai_2_classification.json", "shot": 5, "num": 60, "correct": 51, "acc": 0.85}
{"task": "task069_abductivenli_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task069_abductivenli_classification.json", "shot": 5, "num": 3250, "correct": 1643, "acc": 0.5055384615384615}
{"task": "task1660_super_glue_question_generation.json", "shot": 0, "num": 947, "correct": 0, "acc": 0.0}
{"task": "task1660_super_glue_question_generation.json", "shot": 5, "num": 947, "correct": 7, "acc": 0.007391763463569166}
{"task": "task664_mmmlu_answer_generation_abstract_algebra.json", "shot": 0, "num": 56, "correct": 0, "acc": 0.0}
{"task": "task664_mmmlu_answer_generation_abstract_algebra.json", "shot": 5, "num": 56, "correct": 18, "acc": 0.32142857142857145}
{"task": "task1309_amazonreview_summary_classification.json", "shot": 0, "num": 3250, "correct": 1213, "acc": 0.37323076923076925}
{"task": "task1309_amazonreview_summary_classification.json", "shot": 5, "num": 3250, "correct": 2289, "acc": 0.7043076923076923}
{"task": "task097_conala_remove_duplicates.json", "shot": 0, "num": 2492, "correct": 0, "acc": 0.0}
{"task": "task097_conala_remove_duplicates.json", "shot": 5, "num": 2492, "correct": 24, "acc": 0.009630818619582664}
{"task": "task1418_bless_semantic_relation_classification.json", "shot": 0, "num": 1173, "correct": 0, "acc": 0.0}
{"task": "task1418_bless_semantic_relation_classification.json", "shot": 5, "num": 1173, "correct": 291, "acc": 0.24808184143222506}
{"task": "task209_stancedetection_classification.json", "shot": 0, "num": 763, "correct": 0, "acc": 0.0}
{"task": "task209_stancedetection_classification.json", "shot": 5, "num": 763, "correct": 380, "acc": 0.4980340760157274}
{"task": "task1199_atomic_classification_xattr.json", "shot": 0, "num": 3247, "correct": 1181, "acc": 0.3637203572528488}
{"task": "task1199_atomic_classification_xattr.json", "shot": 5, "num": 3247, "correct": 1814, "acc": 0.5586695411148753}
{"task": "task1603_smcalflow_sentence_generation.json", "shot": 0, "num": 1390, "correct": 0, "acc": 0.0}
{"task": "task1603_smcalflow_sentence_generation.json", "shot": 5, "num": 1390, "correct": 0, "acc": 0.0}
{"task": "task1326_qa_zre_question_generation_from_answer.json", "shot": 0, "num": 2765, "correct": 2, "acc": 0.0007233273056057866}
{"task": "task1326_qa_zre_question_generation_from_answer.json", "shot": 5, "num": 2765, "correct": 25, "acc": 0.009041591320072333}
{"task": "task630_dbpedia_14_classification.json", "shot": 0, "num": 500, "correct": 434, "acc": 0.868}
{"task": "task630_dbpedia_14_classification.json", "shot": 5, "num": 500, "correct": 421, "acc": 0.842}
{"task": "task138_detoxifying-lms_classification_fluency.json", "shot": 0, "num": 195, "correct": 0, "acc": 0.0}
{"task": "task138_detoxifying-lms_classification_fluency.json", "shot": 5, "num": 195, "correct": 16, "acc": 0.08205128205128205}
{"task": "task862_asdiv_multidiv_question_answering.json", "shot": 0, "num": 189, "correct": 0, "acc": 0.0}
{"task": "task862_asdiv_multidiv_question_answering.json", "shot": 5, "num": 189, "correct": 100, "acc": 0.5291005291005291}
{"task": "task1347_glue_sts-b_similarity_classification.json", "shot": 0, "num": 1500, "correct": 0, "acc": 0.0}
{"task": "task1347_glue_sts-b_similarity_classification.json", "shot": 5, "num": 1500, "correct": 213, "acc": 0.142}
{"task": "task1340_msr_text_compression_compression.json", "shot": 0, "num": 2467, "correct": 0, "acc": 0.0}
{"task": "task1340_msr_text_compression_compression.json", "shot": 5, "num": 2467, "correct": 5, "acc": 0.002026753141467369}
{"task": "task1645_medical_question_pair_dataset_text_classification.json", "shot": 0, "num": 1474, "correct": 0, "acc": 0.0}
{"task": "task1645_medical_question_pair_dataset_text_classification.json", "shot": 5, "num": 1474, "correct": 803, "acc": 0.5447761194029851}
{"task": "task169_strategyqa_sentence_generation.json", "shot": 0, "num": 2122, "correct": 0, "acc": 0.0}
{"task": "task169_strategyqa_sentence_generation.json", "shot": 5, "num": 2122, "correct": 0, "acc": 0.0}
{"task": "task921_code_x_glue_information_retreival.json", "shot": 0, "num": 187, "correct": 0, "acc": 0.0}
{"task": "task921_code_x_glue_information_retreival.json", "shot": 5, "num": 187, "correct": 67, "acc": 0.3582887700534759}
{"task": "task934_turk_simplification.json", "shot": 0, "num": 1178, "correct": 0, "acc": 0.0}
{"task": "task934_turk_simplification.json", "shot": 5, "num": 1178, "correct": 0, "acc": 0.0}
{"task": "task321_stereoset_classification_religion.json", "shot": 0, "num": 114, "correct": 0, "acc": 0.0}
{"task": "task321_stereoset_classification_religion.json", "shot": 5, "num": 114, "correct": 68, "acc": 0.5964912280701754}
{"task": "task673_google_wellformed_query_classification.json", "shot": 0, "num": 2659, "correct": 0, "acc": 0.0}
{"task": "task673_google_wellformed_query_classification.json", "shot": 5, "num": 2659, "correct": 1420, "acc": 0.5340353516359534}
{"task": "task022_cosmosqa_passage_inappropriate_binary.json", "shot": 0, "num": 65, "correct": 0, "acc": 0.0}
{"task": "task022_cosmosqa_passage_inappropriate_binary.json", "shot": 5, "num": 65, "correct": 37, "acc": 0.5692307692307692}
{"task": "task904_hate_speech_offensive_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task904_hate_speech_offensive_classification.json", "shot": 5, "num": 3250, "correct": 2459, "acc": 0.7566153846153846}
{"task": "task101_reverse_and_concatenate_all_elements_from_index_i_to_j.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task101_reverse_and_concatenate_all_elements_from_index_i_to_j.json", "shot": 5, "num": 3250, "correct": 33, "acc": 0.010153846153846154}
{"task": "task277_stereoset_sentence_generation_stereotype.json", "shot": 0, "num": 495, "correct": 0, "acc": 0.0}
{"task": "task277_stereoset_sentence_generation_stereotype.json", "shot": 5, "num": 495, "correct": 56, "acc": 0.11313131313131314}
{"task": "task083_babi_t1_single_supporting_fact_answer_generation.json", "shot": 0, "num": 496, "correct": 0, "acc": 0.0}
{"task": "task083_babi_t1_single_supporting_fact_answer_generation.json", "shot": 5, "num": 496, "correct": 339, "acc": 0.6834677419354839}
{"task": "task823_peixian-rtgender_sentiment_analysis.json", "shot": 0, "num": 2500, "correct": 0, "acc": 0.0}
{"task": "task823_peixian-rtgender_sentiment_analysis.json", "shot": 5, "num": 2500, "correct": 1439, "acc": 0.5756}
{"task": "task369_synthetic_remove_odds.json", "shot": 0, "num": 3247, "correct": 0, "acc": 0.0}
{"task": "task369_synthetic_remove_odds.json", "shot": 5, "num": 3247, "correct": 190, "acc": 0.05851555281798583}
{"task": "task1412_web_questions_question_answering.json", "shot": 0, "num": 1886, "correct": 0, "acc": 0.0}
{"task": "task1412_web_questions_question_answering.json", "shot": 5, "num": 1886, "correct": 217, "acc": 0.11505832449628844}
{"task": "task1431_head_qa_answer_generation.json", "shot": 0, "num": 1265, "correct": 0, "acc": 0.0}
{"task": "task1431_head_qa_answer_generation.json", "shot": 5, "num": 1265, "correct": 480, "acc": 0.3794466403162055}
{"task": "task380_boolq_yes_no_question.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task380_boolq_yes_no_question.json", "shot": 5, "num": 3250, "correct": 2524, "acc": 0.7766153846153846}
{"task": "task1520_qa_srl_answer_generation.json", "shot": 0, "num": 417, "correct": 0, "acc": 0.0}
{"task": "task1520_qa_srl_answer_generation.json", "shot": 5, "num": 417, "correct": 80, "acc": 0.19184652278177458}
{"task": "task1285_kpa_keypoint_matching.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1285_kpa_keypoint_matching.json", "shot": 5, "num": 3250, "correct": 819, "acc": 0.252}
{"task": "task1564_triviaqa_answer_generation.json", "shot": 0, "num": 55, "correct": 0, "acc": 0.0}
{"task": "task1564_triviaqa_answer_generation.json", "shot": 5, "num": 55, "correct": 13, "acc": 0.23636363636363636}
{"task": "task1186_nne_hrngo_classification.json", "shot": 0, "num": 1585, "correct": 0, "acc": 0.0}
{"task": "task1186_nne_hrngo_classification.json", "shot": 5, "num": 1585, "correct": 826, "acc": 0.5211356466876972}
{"task": "task070_abductivenli_incorrect_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task070_abductivenli_incorrect_classification.json", "shot": 5, "num": 3250, "correct": 833, "acc": 0.2563076923076923}
{"task": "task589_amazonfood_summary_text_generation.json", "shot": 0, "num": 3250, "correct": 2, "acc": 0.0006153846153846154}
{"task": "task589_amazonfood_summary_text_generation.json", "shot": 5, "num": 3250, "correct": 36, "acc": 0.011076923076923076}
{"task": "task081_piqa_wrong_answer_generation.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task081_piqa_wrong_answer_generation.json", "shot": 5, "num": 3249, "correct": 1, "acc": 0.0003077870113881194}
{"task": "task697_mmmlu_answer_generation_formal_logic.json", "shot": 0, "num": 70, "correct": 3, "acc": 0.04285714285714286}
{"task": "task697_mmmlu_answer_generation_formal_logic.json", "shot": 5, "num": 70, "correct": 14, "acc": 0.2}
{"task": "task1332_check_leap_year.json", "shot": 0, "num": 100, "correct": 0, "acc": 0.0}
{"task": "task1332_check_leap_year.json", "shot": 5, "num": 100, "correct": 40, "acc": 0.4}
{"task": "task157_count_vowels_and_consonants.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task157_count_vowels_and_consonants.json", "shot": 5, "num": 3250, "correct": 224, "acc": 0.06892307692307692}
{"task": "task1594_yahoo_answers_topics_question_generation.json", "shot": 0, "num": 1082, "correct": 0, "acc": 0.0}
{"task": "task1594_yahoo_answers_topics_question_generation.json", "shot": 5, "num": 1082, "correct": 5, "acc": 0.0046210720887245845}
{"task": "task227_clariq_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task227_clariq_classification.json", "shot": 5, "num": 3250, "correct": 2128, "acc": 0.6547692307692308}
{"task": "task1422_mathqa_physics.json", "shot": 0, "num": 3212, "correct": 0, "acc": 0.0}
{"task": "task1422_mathqa_physics.json", "shot": 5, "num": 3212, "correct": 635, "acc": 0.1976961394769614}
{"task": "task1339_peixian_equity_evaluation_corpus_text_completion.json", "shot": 0, "num": 887, "correct": 0, "acc": 0.0}
{"task": "task1339_peixian_equity_evaluation_corpus_text_completion.json", "shot": 5, "num": 887, "correct": 61, "acc": 0.06877113866967305}
{"task": "task843_financial_phrasebank_classification.json", "shot": 0, "num": 903, "correct": 385, "acc": 0.4263565891472868}
{"task": "task843_financial_phrasebank_classification.json", "shot": 5, "num": 903, "correct": 697, "acc": 0.7718715393133998}
{"task": "task1485_organ_extraction_anem_dataset.json", "shot": 0, "num": 70, "correct": 0, "acc": 0.0}
{"task": "task1485_organ_extraction_anem_dataset.json", "shot": 5, "num": 70, "correct": 57, "acc": 0.8142857142857143}
{"task": "task087_new_operator_addsub_arithmetic.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task087_new_operator_addsub_arithmetic.json", "shot": 5, "num": 3250, "correct": 3, "acc": 0.0009230769230769231}
{"task": "task370_synthetic_remove_divisible_by_3.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task370_synthetic_remove_divisible_by_3.json", "shot": 5, "num": 3249, "correct": 133, "acc": 0.04093567251461988}
{"task": "task386_semeval_2018_task3_irony_detection.json", "shot": 0, "num": 913, "correct": 0, "acc": 0.0}
{"task": "task386_semeval_2018_task3_irony_detection.json", "shot": 5, "num": 913, "correct": 406, "acc": 0.44468784227820374}
{"task": "task1419_mathqa_gain.json", "shot": 0, "num": 3209, "correct": 0, "acc": 0.0}
{"task": "task1419_mathqa_gain.json", "shot": 5, "num": 3209, "correct": 548, "acc": 0.17076971019009038}
{"task": "task587_amazonfood_polarity_correction_classification.json", "shot": 0, "num": 3185, "correct": 1174, "acc": 0.36860282574568287}
{"task": "task587_amazonfood_polarity_correction_classification.json", "shot": 5, "num": 3185, "correct": 2388, "acc": 0.7497645211930927}
{"task": "task359_casino_classification_negotiation_vouch_fair.json", "shot": 0, "num": 437, "correct": 0, "acc": 0.0}
{"task": "task359_casino_classification_negotiation_vouch_fair.json", "shot": 5, "num": 437, "correct": 248, "acc": 0.5675057208237986}
{"task": "task770_pawsx_english_text_modification.json", "shot": 0, "num": 126, "correct": 0, "acc": 0.0}
{"task": "task770_pawsx_english_text_modification.json", "shot": 5, "num": 126, "correct": 1, "acc": 0.007936507936507936}
{"task": "task496_semeval_answer_generation.json", "shot": 0, "num": 2398, "correct": 1, "acc": 0.0004170141784820684}
{"task": "task496_semeval_answer_generation.json", "shot": 5, "num": 2398, "correct": 2050, "acc": 0.8548790658882403}
{"task": "task060_ropes_question_generation.json", "shot": 0, "num": 807, "correct": 0, "acc": 0.0}
{"task": "task060_ropes_question_generation.json", "shot": 5, "num": 807, "correct": 0, "acc": 0.0}
{"task": "task026_drop_question_generation.json", "shot": 0, "num": 3054, "correct": 0, "acc": 0.0}
{"task": "task026_drop_question_generation.json", "shot": 5, "num": 3054, "correct": 0, "acc": 0.0}
{"task": "task699_mmmlu_answer_generation_high_school_biology.json", "shot": 0, "num": 92, "correct": 23, "acc": 0.25}
{"task": "task699_mmmlu_answer_generation_high_school_biology.json", "shot": 5, "num": 92, "correct": 43, "acc": 0.4673913043478261}
{"task": "task1510_evalution_relation_extraction.json", "shot": 0, "num": 3247, "correct": 0, "acc": 0.0}
{"task": "task1510_evalution_relation_extraction.json", "shot": 5, "num": 3247, "correct": 2346, "acc": 0.7225130890052356}
{"task": "task247_dream_answer_generation.json", "shot": 0, "num": 3057, "correct": 0, "acc": 0.0}
{"task": "task247_dream_answer_generation.json", "shot": 5, "num": 3057, "correct": 2080, "acc": 0.6804056264311417}
{"task": "task734_mmmlu_answer_generation_sociology.json", "shot": 0, "num": 87, "correct": 18, "acc": 0.20689655172413793}
{"task": "task734_mmmlu_answer_generation_sociology.json", "shot": 5, "num": 87, "correct": 46, "acc": 0.5287356321839081}
{"task": "task291_semeval_2020_task4_commonsense_validation.json", "shot": 0, "num": 2998, "correct": 0, "acc": 0.0}
{"task": "task291_semeval_2020_task4_commonsense_validation.json", "shot": 5, "num": 2998, "correct": 1661, "acc": 0.5540360240160107}
{"task": "task1585_root09_hypernym_generation.json", "shot": 5, "num": 282, "correct": 27, "acc": 0.09574468085106383}
{"task": "task1585_root09_hypernym_generation.json", "shot": 0, "num": 282, "correct": 0, "acc": 0.0}
{"task": "task308_jeopardy_answer_generation_all.json", "shot": 0, "num": 3250, "correct": 316, "acc": 0.09723076923076923}
{"task": "task308_jeopardy_answer_generation_all.json", "shot": 5, "num": 3250, "correct": 1275, "acc": 0.3923076923076923}
{"task": "task132_dais_text_modification.json", "shot": 0, "num": 2499, "correct": 0, "acc": 0.0}
{"task": "task132_dais_text_modification.json", "shot": 5, "num": 2499, "correct": 1937, "acc": 0.7751100440176071}
{"task": "task709_mmmlu_answer_generation_high_school_psychology.json", "shot": 0, "num": 105, "correct": 29, "acc": 0.2761904761904762}
{"task": "task709_mmmlu_answer_generation_high_school_psychology.json", "shot": 5, "num": 105, "correct": 64, "acc": 0.6095238095238096}
{"task": "task090_equation_learner_algebra.json", "shot": 0, "num": 2159, "correct": 0, "acc": 0.0}
{"task": "task090_equation_learner_algebra.json", "shot": 5, "num": 2159, "correct": 61, "acc": 0.02825382121352478}
{"task": "task205_remove_even_elements.json", "shot": 0, "num": 2500, "correct": 0, "acc": 0.0}
{"task": "task205_remove_even_elements.json", "shot": 5, "num": 2500, "correct": 107, "acc": 0.0428}
{"task": "task1217_atomic_answer_generation.json", "shot": 0, "num": 2218, "correct": 3, "acc": 0.001352569882777277}
{"task": "task1217_atomic_answer_generation.json", "shot": 5, "num": 2218, "correct": 3, "acc": 0.001352569882777277}
{"task": "task302_record_classification.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task302_record_classification.json", "shot": 5, "num": 3249, "correct": 916, "acc": 0.28193290243151736}
{"task": "task859_prost_question_generation.json", "shot": 0, "num": 47, "correct": 0, "acc": 0.0}
{"task": "task859_prost_question_generation.json", "shot": 5, "num": 47, "correct": 31, "acc": 0.6595744680851063}
{"task": "task821_protoqa_question_generation.json", "shot": 0, "num": 2500, "correct": 0, "acc": 0.0}
{"task": "task821_protoqa_question_generation.json", "shot": 5, "num": 2500, "correct": 3, "acc": 0.0012}
{"task": "task323_jigsaw_classification_sexually_explicit.json", "shot": 0, "num": 3185, "correct": 0, "acc": 0.0}
{"task": "task323_jigsaw_classification_sexually_explicit.json", "shot": 5, "num": 3185, "correct": 1976, "acc": 0.6204081632653061}
{"task": "task356_casino_classification_negotiation_self_need.json", "shot": 0, "num": 962, "correct": 0, "acc": 0.0}
{"task": "task356_casino_classification_negotiation_self_need.json", "shot": 5, "num": 962, "correct": 525, "acc": 0.5457380457380457}
{"task": "task1421_mathqa_other.json", "shot": 0, "num": 1135, "correct": 0, "acc": 0.0}
{"task": "task1421_mathqa_other.json", "shot": 5, "num": 1135, "correct": 206, "acc": 0.1814977973568282}
{"task": "task1308_amazonreview_category_classification.json", "shot": 0, "num": 3250, "correct": 682, "acc": 0.20984615384615385}
{"task": "task1308_amazonreview_category_classification.json", "shot": 5, "num": 3250, "correct": 2106, "acc": 0.648}
{"task": "task617_amazonreview_category_text_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task617_amazonreview_category_text_generation.json", "shot": 5, "num": 3250, "correct": 647, "acc": 0.19907692307692307}
{"task": "task847_pubmedqa_question_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task847_pubmedqa_question_generation.json", "shot": 5, "num": 3250, "correct": 1, "acc": 0.0003076923076923077}
{"task": "task906_dialogre_identify_names.json", "shot": 0, "num": 139, "correct": 0, "acc": 0.0}
{"task": "task906_dialogre_identify_names.json", "shot": 5, "num": 139, "correct": 37, "acc": 0.26618705035971224}
{"task": "task346_hybridqa_classification.json", "shot": 0, "num": 3249, "correct": 3, "acc": 0.0009233610341643582}
{"task": "task346_hybridqa_classification.json", "shot": 5, "num": 3249, "correct": 1596, "acc": 0.49122807017543857}
{"task": "task061_ropes_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task061_ropes_answer_generation.json", "shot": 5, "num": 3250, "correct": 1580, "acc": 0.48615384615384616}
{"task": "task228_arc_answer_generation_easy.json", "shot": 0, "num": 2597, "correct": 404, "acc": 0.1555641124374278}
{"task": "task228_arc_answer_generation_easy.json", "shot": 5, "num": 2597, "correct": 1763, "acc": 0.6788602233346168}
{"task": "task582_naturalquestion_answer_generation.json", "shot": 0, "num": 3250, "correct": 130, "acc": 0.04}
{"task": "task582_naturalquestion_answer_generation.json", "shot": 5, "num": 3250, "correct": 469, "acc": 0.1443076923076923}
{"task": "task1509_evalution_antonyms.json", "shot": 0, "num": 276, "correct": 3, "acc": 0.010869565217391304}
{"task": "task1509_evalution_antonyms.json", "shot": 5, "num": 276, "correct": 71, "acc": 0.2572463768115942}
{"task": "task1605_ethos_text_classification.json", "shot": 0, "num": 146, "correct": 0, "acc": 0.0}
{"task": "task1605_ethos_text_classification.json", "shot": 5, "num": 146, "correct": 65, "acc": 0.4452054794520548}
{"task": "task1541_agnews_classification.json", "shot": 0, "num": 2000, "correct": 0, "acc": 0.0}
{"task": "task1541_agnews_classification.json", "shot": 5, "num": 2000, "correct": 1253, "acc": 0.6265}
{"task": "task516_senteval_conjoints_inversion.json", "shot": 0, "num": 3250, "correct": 1, "acc": 0.0003076923076923077}
{"task": "task516_senteval_conjoints_inversion.json", "shot": 5, "num": 3250, "correct": 1665, "acc": 0.5123076923076924}
{"task": "task564_discofuse_classification.json", "shot": 0, "num": 500, "correct": 0, "acc": 0.0}
{"task": "task564_discofuse_classification.json", "shot": 5, "num": 500, "correct": 40, "acc": 0.08}
{"task": "task696_mmmlu_answer_generation_elementary_mathematics.json", "shot": 0, "num": 97, "correct": 7, "acc": 0.07216494845360824}
{"task": "task696_mmmlu_answer_generation_elementary_mathematics.json", "shot": 5, "num": 97, "correct": 29, "acc": 0.29896907216494845}
{"task": "task245_check_presence_in_set_intersection.json", "shot": 0, "num": 1933, "correct": 0, "acc": 0.0}
{"task": "task245_check_presence_in_set_intersection.json", "shot": 5, "num": 1933, "correct": 556, "acc": 0.2876357992757372}
{"task": "task303_record_incorrect_answer_generation.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task303_record_incorrect_answer_generation.json", "shot": 5, "num": 3249, "correct": 1, "acc": 0.0003077870113881194}
{"task": "task373_synthetic_round_tens_place.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task373_synthetic_round_tens_place.json", "shot": 5, "num": 3250, "correct": 282, "acc": 0.08676923076923077}
{"task": "task1198_atomic_classification_owant.json", "shot": 0, "num": 3247, "correct": 555, "acc": 0.1709270095472744}
{"task": "task1198_atomic_classification_owant.json", "shot": 5, "num": 3247, "correct": 1543, "acc": 0.47520788420080073}
{"task": "task568_circa_question_generation.json", "shot": 0, "num": 3103, "correct": 0, "acc": 0.0}
{"task": "task568_circa_question_generation.json", "shot": 5, "num": 3103, "correct": 5, "acc": 0.0016113438607798904}
{"task": "task142_odd-man-out_classification_no_category.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task142_odd-man-out_classification_no_category.json", "shot": 5, "num": 3250, "correct": 904, "acc": 0.27815384615384614}
{"task": "task085_unnatural_addsub_arithmetic.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task085_unnatural_addsub_arithmetic.json", "shot": 5, "num": 3250, "correct": 1, "acc": 0.0003076923076923077}
{"task": "task869_cfq_mcd1_sql_to_explanation.json", "shot": 0, "num": 99, "correct": 0, "acc": 0.0}
{"task": "task869_cfq_mcd1_sql_to_explanation.json", "shot": 5, "num": 99, "correct": 56, "acc": 0.5656565656565656}
{"task": "task1429_evalution_semantic_relation_classification.json", "shot": 0, "num": 397, "correct": 0, "acc": 0.0}
{"task": "task1429_evalution_semantic_relation_classification.json", "shot": 5, "num": 397, "correct": 65, "acc": 0.163727959697733}
{"task": "task861_asdiv_addsub_question_answering.json", "shot": 0, "num": 423, "correct": 0, "acc": 0.0}
{"task": "task861_asdiv_addsub_question_answering.json", "shot": 5, "num": 423, "correct": 253, "acc": 0.5981087470449172}
{"task": "task590_amazonfood_summary_correction_classification.json", "shot": 0, "num": 3196, "correct": 1539, "acc": 0.48153942428035046}
{"task": "task590_amazonfood_summary_correction_classification.json", "shot": 5, "num": 3196, "correct": 1624, "acc": 0.5081351689612015}
{"task": "task152_tomqa_find_location_easy_noise.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task152_tomqa_find_location_easy_noise.json", "shot": 5, "num": 3250, "correct": 1976, "acc": 0.608}
{"task": "task1725_civil_comments_severtoxicity_classification.json", "shot": 0, "num": 498, "correct": 90, "acc": 0.18072289156626506}
{"task": "task1725_civil_comments_severtoxicity_classification.json", "shot": 5, "num": 498, "correct": 237, "acc": 0.4759036144578313}
{"task": "task593_sciq_explanation_generation.json", "shot": 0, "num": 1450, "correct": 0, "acc": 0.0}
{"task": "task593_sciq_explanation_generation.json", "shot": 5, "num": 1450, "correct": 21, "acc": 0.014482758620689656}
{"task": "task711_mmmlu_answer_generation_high_school_us_history.json", "shot": 0, "num": 87, "correct": 0, "acc": 0.0}
{"task": "task711_mmmlu_answer_generation_high_school_us_history.json", "shot": 5, "num": 87, "correct": 33, "acc": 0.3793103448275862}
{"task": "task1508_wordnet_antonyms.json", "shot": 5, "num": 1689, "correct": 1081, "acc": 0.6400236826524571}
{"task": "task1508_wordnet_antonyms.json", "shot": 0, "num": 1689, "correct": 41, "acc": 0.02427471876850207}
{"task": "task1401_obqa_sentence_generation.json", "shot": 0, "num": 879, "correct": 0, "acc": 0.0}
{"task": "task1401_obqa_sentence_generation.json", "shot": 5, "num": 879, "correct": 11, "acc": 0.012514220705346985}
{"task": "task578_curiosity_dialogs_answer_generation.json", "shot": 0, "num": 1749, "correct": 0, "acc": 0.0}
{"task": "task578_curiosity_dialogs_answer_generation.json", "shot": 5, "num": 1749, "correct": 935, "acc": 0.5345911949685535}
{"task": "task1583_bless_meronym_classification.json", "shot": 5, "num": 741, "correct": 581, "acc": 0.7840755735492577}
{"task": "task1583_bless_meronym_classification.json", "shot": 0, "num": 741, "correct": 0, "acc": 0.0}
{"task": "task965_librispeech_asr_missing_word_prediction.json", "shot": 0, "num": 75, "correct": 0, "acc": 0.0}
{"task": "task965_librispeech_asr_missing_word_prediction.json", "shot": 5, "num": 75, "correct": 15, "acc": 0.2}
{"task": "task208_combinations_of_list.json", "shot": 0, "num": 2493, "correct": 0, "acc": 0.0}
{"task": "task208_combinations_of_list.json", "shot": 5, "num": 2493, "correct": 98, "acc": 0.03931006819093462}
{"task": "task1088_array_of_products.json", "shot": 0, "num": 75, "correct": 0, "acc": 0.0}
{"task": "task1088_array_of_products.json", "shot": 5, "num": 75, "correct": 0, "acc": 0.0}
{"task": "task1502_hatexplain_classification.json", "shot": 0, "num": 2764, "correct": 0, "acc": 0.0}
{"task": "task1502_hatexplain_classification.json", "shot": 5, "num": 2764, "correct": 987, "acc": 0.35709117221418235}
{"task": "task046_miscellaneous_question_typing.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task046_miscellaneous_question_typing.json", "shot": 5, "num": 3249, "correct": 388, "acc": 0.11942136041859033}
{"task": "task301_record_question_generation.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task301_record_question_generation.json", "shot": 5, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task685_mmmlu_answer_generation_clinical_knowledge.json", "shot": 0, "num": 89, "correct": 23, "acc": 0.25842696629213485}
{"task": "task685_mmmlu_answer_generation_clinical_knowledge.json", "shot": 5, "num": 89, "correct": 49, "acc": 0.550561797752809}
{"task": "task1190_add_integer_to_list.json", "shot": 0, "num": 98, "correct": 0, "acc": 0.0}
{"task": "task1190_add_integer_to_list.json", "shot": 5, "num": 98, "correct": 0, "acc": 0.0}
{"task": "task717_mmmlu_answer_generation_logical_fallacies.json", "shot": 0, "num": 85, "correct": 24, "acc": 0.2823529411764706}
{"task": "task717_mmmlu_answer_generation_logical_fallacies.json", "shot": 5, "num": 85, "correct": 46, "acc": 0.5411764705882353}
{"task": "task287_casehold_legal_incorrect_answer_generation.json", "shot": 0, "num": 3248, "correct": 0, "acc": 0.0}
{"task": "task287_casehold_legal_incorrect_answer_generation.json", "shot": 5, "num": 3248, "correct": 0, "acc": 0.0}
{"task": "task1389_hellaswag_completion.json", "shot": 0, "num": 3247, "correct": 0, "acc": 0.0}
{"task": "task1389_hellaswag_completion.json", "shot": 5, "num": 3247, "correct": 719, "acc": 0.22143517092700954}
{"task": "task1582_bless_hypernym_generation.json", "shot": 0, "num": 80, "correct": 0, "acc": 0.0}
{"task": "task1582_bless_hypernym_generation.json", "shot": 5, "num": 80, "correct": 10, "acc": 0.125}
{"task": "task675_google_wellformed_query_sentence_generation.json", "shot": 0, "num": 1168, "correct": 0, "acc": 0.0}
{"task": "task675_google_wellformed_query_sentence_generation.json", "shot": 5, "num": 1168, "correct": 260, "acc": 0.2226027397260274}
{"task": "task229_arc_answer_generation_hard.json", "shot": 0, "num": 1295, "correct": 131, "acc": 0.10115830115830116}
{"task": "task229_arc_answer_generation_hard.json", "shot": 5, "num": 1295, "correct": 652, "acc": 0.5034749034749034}
{"task": "task741_lhoestq_answer_generation_place.json", "shot": 0, "num": 58, "correct": 25, "acc": 0.43103448275862066}
{"task": "task741_lhoestq_answer_generation_place.json", "shot": 5, "num": 58, "correct": 32, "acc": 0.5517241379310345}
{"task": "task560_alt_translation_en_entk.json", "shot": 0, "num": 300, "correct": 0, "acc": 0.0}
{"task": "task560_alt_translation_en_entk.json", "shot": 5, "num": 300, "correct": 37, "acc": 0.12333333333333334}
{"task": "task902_deceptive_opinion_spam_classification.json", "shot": 0, "num": 795, "correct": 5, "acc": 0.006289308176100629}
{"task": "task902_deceptive_opinion_spam_classification.json", "shot": 5, "num": 795, "correct": 787, "acc": 0.989937106918239}
{"task": "task856_conv_ai_2_classification.json", "shot": 0, "num": 59, "correct": 0, "acc": 0.0}
{"task": "task856_conv_ai_2_classification.json", "shot": 5, "num": 59, "correct": 13, "acc": 0.22033898305084745}
{"task": "task068_abductivenli_incorrect_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task068_abductivenli_incorrect_answer_generation.json", "shot": 5, "num": 3250, "correct": 7, "acc": 0.0021538461538461538}
{"task": "task727_mmmlu_answer_generation_prehistory.json", "shot": 0, "num": 94, "correct": 27, "acc": 0.2872340425531915}
{"task": "task727_mmmlu_answer_generation_prehistory.json", "shot": 5, "num": 94, "correct": 52, "acc": 0.5531914893617021}
{"task": "task567_circa_text_generation.json", "shot": 0, "num": 1439, "correct": 0, "acc": 0.0}
{"task": "task567_circa_text_generation.json", "shot": 5, "num": 1439, "correct": 30, "acc": 0.020847810979847115}
{"task": "task923_event2mind_classifier.json", "shot": 0, "num": 2250, "correct": 0, "acc": 0.0}
{"task": "task923_event2mind_classifier.json", "shot": 5, "num": 2250, "correct": 860, "acc": 0.38222222222222224}
{"task": "task507_position_of_all_numerical_elements_in_list.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task507_position_of_all_numerical_elements_in_list.json", "shot": 5, "num": 3250, "correct": 43, "acc": 0.01323076923076923}
{"task": "task687_mmmlu_answer_generation_college_chemistry.json", "shot": 0, "num": 55, "correct": 0, "acc": 0.0}
{"task": "task687_mmmlu_answer_generation_college_chemistry.json", "shot": 5, "num": 55, "correct": 16, "acc": 0.2909090909090909}
{"task": "task112_asset_simple_sentence_identification.json", "shot": 0, "num": 3250, "correct": 788, "acc": 0.24246153846153845}
{"task": "task112_asset_simple_sentence_identification.json", "shot": 5, "num": 3250, "correct": 1868, "acc": 0.5747692307692308}
{"task": "task606_sum_of_all_numbers_in_list_between_positions_i_and_j.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task606_sum_of_all_numbers_in_list_between_positions_i_and_j.json", "shot": 5, "num": 3250, "correct": 188, "acc": 0.057846153846153846}
{"task": "task866_mawps_multidiv_question_answering.json", "shot": 0, "num": 299, "correct": 0, "acc": 0.0}
{"task": "task866_mawps_multidiv_question_answering.json", "shot": 5, "num": 299, "correct": 105, "acc": 0.3511705685618729}
{"task": "task684_online_privacy_policy_text_information_type_generation.json", "shot": 0, "num": 1540, "correct": 0, "acc": 0.0}
{"task": "task684_online_privacy_policy_text_information_type_generation.json", "shot": 5, "num": 1540, "correct": 569, "acc": 0.36948051948051946}
{"task": "task398_semeval_2018_task1_tweet_joy_detection.json", "shot": 0, "num": 898, "correct": 164, "acc": 0.18262806236080179}
{"task": "task398_semeval_2018_task1_tweet_joy_detection.json", "shot": 5, "num": 898, "correct": 608, "acc": 0.6770601336302895}
{"task": "task576_curiosity_dialogs_answer_generation.json", "shot": 0, "num": 3232, "correct": 0, "acc": 0.0}
{"task": "task576_curiosity_dialogs_answer_generation.json", "shot": 5, "num": 3232, "correct": 406, "acc": 0.12561881188118812}
{"task": "task867_mawps_multiop_question_answering.json", "shot": 0, "num": 390, "correct": 0, "acc": 0.0}
{"task": "task867_mawps_multiop_question_answering.json", "shot": 5, "num": 390, "correct": 39, "acc": 0.1}
{"task": "task317_crows-pairs_classification_stereotype_type.json", "shot": 0, "num": 750, "correct": 0, "acc": 0.0}
{"task": "task317_crows-pairs_classification_stereotype_type.json", "shot": 5, "num": 750, "correct": 420, "acc": 0.56}
{"task": "task283_dream_incorrect_answer_generation.json", "shot": 0, "num": 3056, "correct": 0, "acc": 0.0}
{"task": "task283_dream_incorrect_answer_generation.json", "shot": 5, "num": 3056, "correct": 3, "acc": 0.000981675392670157}
{"task": "task1194_kth_largest_element.json", "shot": 0, "num": 100, "correct": 0, "acc": 0.0}
{"task": "task1194_kth_largest_element.json", "shot": 5, "num": 100, "correct": 6, "acc": 0.06}
{"task": "task591_sciq_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task591_sciq_answer_generation.json", "shot": 5, "num": 3250, "correct": 1146, "acc": 0.3526153846153846}
{"task": "task1288_glue_mrpc_paraphrasing.json", "shot": 0, "num": 2036, "correct": 0, "acc": 0.0}
{"task": "task1288_glue_mrpc_paraphrasing.json", "shot": 5, "num": 2036, "correct": 1377, "acc": 0.6763261296660118}
{"task": "task1704_ljspeech_textmodification.json", "shot": 0, "num": 250, "correct": 0, "acc": 0.0}
{"task": "task1704_ljspeech_textmodification.json", "shot": 5, "num": 250, "correct": 43, "acc": 0.172}
{"task": "task726_mmmlu_answer_generation_philosophy.json", "shot": 0, "num": 93, "correct": 29, "acc": 0.3118279569892473}
{"task": "task726_mmmlu_answer_generation_philosophy.json", "shot": 5, "num": 93, "correct": 44, "acc": 0.4731182795698925}
{"task": "task611_mutual_multi_turn_dialogue.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task611_mutual_multi_turn_dialogue.json", "shot": 5, "num": 3250, "correct": 636, "acc": 0.1956923076923077}
{"task": "task167_strategyqa_question_generation.json", "shot": 0, "num": 1177, "correct": 0, "acc": 0.0}
{"task": "task167_strategyqa_question_generation.json", "shot": 5, "num": 1177, "correct": 0, "acc": 0.0}
{"task": "task193_duorc_question_generation.json", "shot": 0, "num": 2400, "correct": 0, "acc": 0.0}
{"task": "task193_duorc_question_generation.json", "shot": 5, "num": 2400, "correct": 0, "acc": 0.0}
{"task": "task1293_kilt_tasks_hotpotqa_question_answering.json", "shot": 0, "num": 3121, "correct": 1, "acc": 0.0003204101249599487}
{"task": "task1293_kilt_tasks_hotpotqa_question_answering.json", "shot": 5, "num": 3121, "correct": 391, "acc": 0.12528035885933997}
{"task": "task1712_poki_classification.json", "shot": 0, "num": 3224, "correct": 0, "acc": 0.0}
{"task": "task1712_poki_classification.json", "shot": 5, "num": 3224, "correct": 2051, "acc": 0.636166253101737}
{"task": "task335_hateeval_classification_aggresive_en.json", "shot": 0, "num": 1499, "correct": 0, "acc": 0.0}
{"task": "task335_hateeval_classification_aggresive_en.json", "shot": 5, "num": 1499, "correct": 334, "acc": 0.2228152101400934}
{"task": "task166_clariq_sentence_generation.json", "shot": 0, "num": 116, "correct": 0, "acc": 0.0}
{"task": "task166_clariq_sentence_generation.json", "shot": 5, "num": 116, "correct": 0, "acc": 0.0}
{"task": "task1193_food_course_classification.json", "shot": 0, "num": 110, "correct": 7, "acc": 0.06363636363636363}
{"task": "task1193_food_course_classification.json", "shot": 5, "num": 110, "correct": 73, "acc": 0.6636363636363637}
{"task": "task1505_root09_semantic_relation_classification.json", "shot": 5, "num": 1587, "correct": 553, "acc": 0.3484562066792691}
{"task": "task1505_root09_semantic_relation_classification.json", "shot": 0, "num": 1587, "correct": 0, "acc": 0.0}
{"task": "task043_essential_terms_answering_incomplete_questions.json", "shot": 0, "num": 852, "correct": 482, "acc": 0.5657276995305164}
{"task": "task043_essential_terms_answering_incomplete_questions.json", "shot": 5, "num": 852, "correct": 394, "acc": 0.4624413145539906}
{"task": "task1730_personachat_choose_next.json", "shot": 0, "num": 3248, "correct": 0, "acc": 0.0}
{"task": "task1730_personachat_choose_next.json", "shot": 5, "num": 3248, "correct": 900, "acc": 0.2770935960591133}
{"task": "task504_count_all_alphabetical_elements_in_list.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task504_count_all_alphabetical_elements_in_list.json", "shot": 5, "num": 3250, "correct": 413, "acc": 0.1270769230769231}
{"task": "task925_coached_conv_pref_classifier.json", "shot": 0, "num": 249, "correct": 0, "acc": 0.0}
{"task": "task925_coached_conv_pref_classifier.json", "shot": 5, "num": 249, "correct": 123, "acc": 0.4939759036144578}
{"task": "task566_circa_classification.json", "shot": 0, "num": 2738, "correct": 179, "acc": 0.06537618699780862}
{"task": "task566_circa_classification.json", "shot": 5, "num": 2738, "correct": 2030, "acc": 0.7414170927684441}
{"task": "task721_mmmlu_answer_generation_medical_genetics.json", "shot": 0, "num": 57, "correct": 16, "acc": 0.2807017543859649}
{"task": "task721_mmmlu_answer_generation_medical_genetics.json", "shot": 5, "num": 57, "correct": 37, "acc": 0.6491228070175439}
{"task": "task750_aqua_multiple_choice_answering.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task750_aqua_multiple_choice_answering.json", "shot": 5, "num": 3250, "correct": 688, "acc": 0.21169230769230768}
{"task": "task114_is_the_given_word_longest.json", "shot": 0, "num": 3250, "correct": 45, "acc": 0.013846153846153847}
{"task": "task114_is_the_given_word_longest.json", "shot": 5, "num": 3250, "correct": 1453, "acc": 0.4470769230769231}
{"task": "task1150_delete_max_min.json", "shot": 0, "num": 75, "correct": 0, "acc": 0.0}
{"task": "task1150_delete_max_min.json", "shot": 5, "num": 75, "correct": 0, "acc": 0.0}
{"task": "task1149_item_check_edible.json", "shot": 0, "num": 60, "correct": 0, "acc": 0.0}
{"task": "task1149_item_check_edible.json", "shot": 5, "num": 60, "correct": 51, "acc": 0.85}
{"task": "task888_reviews_classification.json", "shot": 0, "num": 250, "correct": 3, "acc": 0.012}
{"task": "task888_reviews_classification.json", "shot": 5, "num": 250, "correct": 230, "acc": 0.92}
{"task": "task1551_every_ith_element_from_kth_element.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1551_every_ith_element_from_kth_element.json", "shot": 5, "num": 3250, "correct": 93, "acc": 0.028615384615384615}
{"task": "task692_mmmlu_answer_generation_computer_security.json", "shot": 0, "num": 57, "correct": 15, "acc": 0.2631578947368421}
{"task": "task692_mmmlu_answer_generation_computer_security.json", "shot": 5, "num": 57, "correct": 25, "acc": 0.43859649122807015}
{"task": "task1669_md_gender_bias_text_modification.json", "shot": 0, "num": 500, "correct": 0, "acc": 0.0}
{"task": "task1669_md_gender_bias_text_modification.json", "shot": 5, "num": 500, "correct": 7, "acc": 0.014}
{"task": "task1355_sent_comp_summarization.json", "shot": 0, "num": 500, "correct": 0, "acc": 0.0}
{"task": "task1355_sent_comp_summarization.json", "shot": 5, "num": 500, "correct": 0, "acc": 0.0}
{"task": "task740_lhoestq_answer_generation_quantity.json", "shot": 0, "num": 59, "correct": 5, "acc": 0.0847457627118644}
{"task": "task740_lhoestq_answer_generation_quantity.json", "shot": 5, "num": 59, "correct": 35, "acc": 0.5932203389830508}
{"task": "task730_mmmlu_answer_generation_professional_medicine.json", "shot": 0, "num": 92, "correct": 24, "acc": 0.2608695652173913}
{"task": "task730_mmmlu_answer_generation_professional_medicine.json", "shot": 5, "num": 92, "correct": 35, "acc": 0.3804347826086957}
{"task": "task518_emo_different_dialogue_emotions.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task518_emo_different_dialogue_emotions.json", "shot": 5, "num": 3250, "correct": 1691, "acc": 0.5203076923076924}
{"task": "task453_swag_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task453_swag_answer_generation.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1425_country_iso_numeric.json", "shot": 0, "num": 124, "correct": 0, "acc": 0.0}
{"task": "task1425_country_iso_numeric.json", "shot": 5, "num": 124, "correct": 20, "acc": 0.16129032258064516}
{"task": "task848_pubmedqa_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task848_pubmedqa_classification.json", "shot": 5, "num": 3250, "correct": 1397, "acc": 0.4298461538461538}
{"task": "task1361_movierationales_classification.json", "shot": 5, "num": 998, "correct": 7, "acc": 0.0070140280561122245}
{"task": "task1361_movierationales_classification.json", "shot": 0, "num": 998, "correct": 2, "acc": 0.002004008016032064}
{"task": "task363_sst2_polarity_classification.json", "shot": 0, "num": 3248, "correct": 10, "acc": 0.003078817733990148}
{"task": "task363_sst2_polarity_classification.json", "shot": 5, "num": 3248, "correct": 2825, "acc": 0.8697660098522167}
{"task": "task1201_atomic_classification_xintent.json", "shot": 0, "num": 3247, "correct": 41, "acc": 0.012627040344933785}
{"task": "task1201_atomic_classification_xintent.json", "shot": 5, "num": 3247, "correct": 1834, "acc": 0.5648290729904527}
{"task": "task1711_poki_text_generation.json", "shot": 0, "num": 3221, "correct": 0, "acc": 0.0}
{"task": "task1711_poki_text_generation.json", "shot": 5, "num": 3221, "correct": 0, "acc": 0.0}
{"task": "task1384_deal_or_no_dialog_classification.json", "shot": 0, "num": 2105, "correct": 243, "acc": 0.1154394299287411}
{"task": "task1384_deal_or_no_dialog_classification.json", "shot": 5, "num": 2105, "correct": 1626, "acc": 0.7724465558194774}
{"task": "task649_race_blank_question_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task649_race_blank_question_generation.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1723_civil_comments_sexuallyexplicit_classification.json", "shot": 0, "num": 498, "correct": 11, "acc": 0.02208835341365462}
{"task": "task1723_civil_comments_sexuallyexplicit_classification.json", "shot": 5, "num": 498, "correct": 200, "acc": 0.40160642570281124}
{"task": "task461_qasper_question_generation.json", "shot": 0, "num": 1307, "correct": 0, "acc": 0.0}
{"task": "task461_qasper_question_generation.json", "shot": 5, "num": 1307, "correct": 4, "acc": 0.00306044376434583}
{"task": "task514_argument_consequence_classification.json", "shot": 5, "num": 86, "correct": 49, "acc": 0.5697674418604651}
{"task": "task514_argument_consequence_classification.json", "shot": 0, "num": 86, "correct": 0, "acc": 0.0}
{"task": "task082_babi_t1_single_supporting_fact_question_generation.json", "shot": 0, "num": 496, "correct": 0, "acc": 0.0}
{"task": "task082_babi_t1_single_supporting_fact_question_generation.json", "shot": 5, "num": 496, "correct": 69, "acc": 0.13911290322580644}
{"task": "task583_udeps_eng_coarse_pos_tagging.json", "shot": 0, "num": 3031, "correct": 0, "acc": 0.0}
{"task": "task583_udeps_eng_coarse_pos_tagging.json", "shot": 5, "num": 3031, "correct": 927, "acc": 0.30583965687891784}
{"task": "task075_squad1.1_answer_generation.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task075_squad1.1_answer_generation.json", "shot": 5, "num": 3249, "correct": 1448, "acc": 0.44567559248999694}
{"task": "task1189_check_char_in_string.json", "shot": 0, "num": 98, "correct": 0, "acc": 0.0}
{"task": "task1189_check_char_in_string.json", "shot": 5, "num": 98, "correct": 45, "acc": 0.45918367346938777}
{"task": "task1559_blimp_binary_classification.json", "shot": 0, "num": 500, "correct": 7, "acc": 0.014}
{"task": "task1559_blimp_binary_classification.json", "shot": 5, "num": 500, "correct": 234, "acc": 0.468}
{"task": "task454_swag_incorrect_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task454_swag_incorrect_answer_generation.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task691_mmmlu_answer_generation_college_physics.json", "shot": 0, "num": 48, "correct": 1, "acc": 0.020833333333333332}
{"task": "task691_mmmlu_answer_generation_college_physics.json", "shot": 5, "num": 48, "correct": 10, "acc": 0.20833333333333334}
{"task": "task1503_hatexplain_classification.json", "shot": 0, "num": 449, "correct": 0, "acc": 0.0}
{"task": "task1503_hatexplain_classification.json", "shot": 5, "num": 449, "correct": 136, "acc": 0.3028953229398664}
{"task": "task092_check_prime_classification.json", "shot": 0, "num": 3250, "correct": 1352, "acc": 0.416}
{"task": "task092_check_prime_classification.json", "shot": 5, "num": 3250, "correct": 1653, "acc": 0.5086153846153846}
{"task": "task116_com2sense_commonsense_reasoning.json", "shot": 0, "num": 974, "correct": 244, "acc": 0.25051334702258726}
{"task": "task116_com2sense_commonsense_reasoning.json", "shot": 5, "num": 974, "correct": 573, "acc": 0.5882956878850103}
{"task": "task094_conala_calculate_mean.json", "shot": 0, "num": 2500, "correct": 0, "acc": 0.0}
{"task": "task094_conala_calculate_mean.json", "shot": 5, "num": 2500, "correct": 4, "acc": 0.0016}
{"task": "task622_replace_alphabets_in_a_list_by_their_position_in_english_alphabet.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task622_replace_alphabets_in_a_list_by_their_position_in_english_alphabet.json", "shot": 5, "num": 3250, "correct": 81, "acc": 0.024923076923076923}
{"task": "task579_socialiqa_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task579_socialiqa_classification.json", "shot": 5, "num": 3250, "correct": 1505, "acc": 0.46307692307692305}
{"task": "task1214_atomic_classification_xwant.json", "shot": 0, "num": 3247, "correct": 441, "acc": 0.1358176778564829}
{"task": "task1214_atomic_classification_xwant.json", "shot": 5, "num": 3247, "correct": 1855, "acc": 0.5712965814598091}
{"task": "task927_yelp_negative_to_positive_style_transfer.json", "shot": 0, "num": 247, "correct": 0, "acc": 0.0}
{"task": "task927_yelp_negative_to_positive_style_transfer.json", "shot": 5, "num": 247, "correct": 0, "acc": 0.0}
{"task": "task358_casino_classification_negotiation_uv_part.json", "shot": 0, "num": 129, "correct": 0, "acc": 0.0}
{"task": "task358_casino_classification_negotiation_uv_part.json", "shot": 5, "num": 129, "correct": 62, "acc": 0.4806201550387597}
{"task": "task489_mwsc_question_generation.json", "shot": 0, "num": 50, "correct": 0, "acc": 0.0}
{"task": "task489_mwsc_question_generation.json", "shot": 5, "num": 50, "correct": 11, "acc": 0.22}
{"task": "task044_essential_terms_identifying_essential_words.json", "shot": 0, "num": 1117, "correct": 0, "acc": 0.0}
{"task": "task044_essential_terms_identifying_essential_words.json", "shot": 5, "num": 1117, "correct": 86, "acc": 0.07699194270367055}
{"task": "task853_hippocorpus_long_text_generation.json", "shot": 0, "num": 52, "correct": 0, "acc": 0.0}
{"task": "task853_hippocorpus_long_text_generation.json", "shot": 5, "num": 52, "correct": 0, "acc": 0.0}
{"task": "task1346_glue_cola_grammatical_correctness_classification.json", "shot": 0, "num": 2518, "correct": 19, "acc": 0.007545671167593328}
{"task": "task1346_glue_cola_grammatical_correctness_classification.json", "shot": 5, "num": 2518, "correct": 1726, "acc": 0.6854646544876887}
{"task": "task367_synthetic_remove_floats.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task367_synthetic_remove_floats.json", "shot": 5, "num": 3250, "correct": 323, "acc": 0.09938461538461539}
{"task": "task728_mmmlu_answer_generation_professional_accounting.json", "shot": 0, "num": 92, "correct": 3, "acc": 0.03260869565217391}
{"task": "task728_mmmlu_answer_generation_professional_accounting.json", "shot": 5, "num": 92, "correct": 32, "acc": 0.34782608695652173}
{"task": "task505_count_all_numerical_elements_in_list.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task505_count_all_numerical_elements_in_list.json", "shot": 5, "num": 3250, "correct": 603, "acc": 0.18553846153846154}
{"task": "task246_dream_question_generation.json", "shot": 0, "num": 3214, "correct": 0, "acc": 0.0}
{"task": "task246_dream_question_generation.json", "shot": 5, "num": 3214, "correct": 0, "acc": 0.0}
{"task": "task297_storycloze_incorrect_end_classification.json", "shot": 0, "num": 942, "correct": 0, "acc": 0.0}
{"task": "task297_storycloze_incorrect_end_classification.json", "shot": 5, "num": 942, "correct": 272, "acc": 0.28874734607218683}
{"task": "task324_jigsaw_classification_disagree.json", "shot": 0, "num": 194, "correct": 1, "acc": 0.005154639175257732}
{"task": "task324_jigsaw_classification_disagree.json", "shot": 5, "num": 194, "correct": 105, "acc": 0.5412371134020618}
{"task": "task476_cls_english_books_classification.json", "shot": 0, "num": 993, "correct": 1, "acc": 0.0010070493454179255}
{"task": "task476_cls_english_books_classification.json", "shot": 5, "num": 993, "correct": 837, "acc": 0.8429003021148036}
{"task": "task1406_kth_smallest_element.json", "shot": 0, "num": 98, "correct": 0, "acc": 0.0}
{"task": "task1406_kth_smallest_element.json", "shot": 5, "num": 98, "correct": 7, "acc": 0.07142857142857142}
{"task": "task067_abductivenli_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task067_abductivenli_answer_generation.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1553_cnn_dailymail_summarization.json", "shot": 5, "num": 475, "correct": 0, "acc": 0.0}
{"task": "task1553_cnn_dailymail_summarization.json", "shot": 0, "num": 475, "correct": 0, "acc": 0.0}
{"task": "task1580_eqasc-perturbed_question_generation.json", "shot": 0, "num": 411, "correct": 1, "acc": 0.0024330900243309003}
{"task": "task1580_eqasc-perturbed_question_generation.json", "shot": 5, "num": 411, "correct": 60, "acc": 0.145985401459854}
{"task": "task499_extract_and_add_all_numbers_from_list.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task499_extract_and_add_all_numbers_from_list.json", "shot": 5, "num": 3250, "correct": 243, "acc": 0.07476923076923077}
{"task": "task874_opus_xhosanavy_sr.json", "shot": 0, "num": 56, "correct": 0, "acc": 0.0}
{"task": "task874_opus_xhosanavy_sr.json", "shot": 5, "num": 56, "correct": 28, "acc": 0.5}
{"task": "task955_wiki_auto_style_transfer.json", "shot": 0, "num": 881, "correct": 0, "acc": 0.0}
{"task": "task955_wiki_auto_style_transfer.json", "shot": 5, "num": 881, "correct": 0, "acc": 0.0}
{"task": "task456_matres_intention_classification.json", "shot": 0, "num": 1328, "correct": 458, "acc": 0.34487951807228917}
{"task": "task456_matres_intention_classification.json", "shot": 5, "num": 1328, "correct": 719, "acc": 0.5414156626506024}
{"task": "task754_svamp_common-division_question_answering.json", "shot": 0, "num": 83, "correct": 0, "acc": 0.0}
{"task": "task754_svamp_common-division_question_answering.json", "shot": 5, "num": 83, "correct": 52, "acc": 0.6265060240963856}
{"task": "task073_commonsenseqa_answer_generation.json", "shot": 0, "num": 610, "correct": 12, "acc": 0.019672131147540985}
{"task": "task073_commonsenseqa_answer_generation.json", "shot": 5, "num": 610, "correct": 326, "acc": 0.5344262295081967}
{"task": "task027_drop_answer_type_generation.json", "shot": 0, "num": 2087, "correct": 0, "acc": 0.0}
{"task": "task027_drop_answer_type_generation.json", "shot": 5, "num": 2087, "correct": 870, "acc": 0.4168663152850982}
{"task": "task682_online_privacy_policy_text_classification.json", "shot": 0, "num": 331, "correct": 0, "acc": 0.0}
{"task": "task682_online_privacy_policy_text_classification.json", "shot": 5, "num": 331, "correct": 224, "acc": 0.676737160120846}
{"task": "task733_mmmlu_answer_generation_security_studies.json", "shot": 0, "num": 90, "correct": 8, "acc": 0.08888888888888889}
{"task": "task733_mmmlu_answer_generation_security_studies.json", "shot": 5, "num": 90, "correct": 28, "acc": 0.3111111111111111}
{"task": "task244_count_elements_in_set_union.json", "shot": 0, "num": 500, "correct": 0, "acc": 0.0}
{"task": "task244_count_elements_in_set_union.json", "shot": 5, "num": 500, "correct": 98, "acc": 0.196}
{"task": "task091_all_elements_from_index_i_to_j.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task091_all_elements_from_index_i_to_j.json", "shot": 5, "num": 3250, "correct": 117, "acc": 0.036}
{"task": "task708_mmmlu_answer_generation_high_school_physics.json", "shot": 0, "num": 85, "correct": 9, "acc": 0.10588235294117647}
{"task": "task708_mmmlu_answer_generation_high_school_physics.json", "shot": 5, "num": 85, "correct": 26, "acc": 0.3058823529411765}
{"task": "task1405_find_median.json", "shot": 0, "num": 98, "correct": 0, "acc": 0.0}
{"task": "task1405_find_median.json", "shot": 5, "num": 98, "correct": 16, "acc": 0.16326530612244897}
{"task": "task286_olid_offense_judgment.json", "shot": 0, "num": 2993, "correct": 3, "acc": 0.0010023387905111927}
{"task": "task286_olid_offense_judgment.json", "shot": 5, "num": 2993, "correct": 1994, "acc": 0.6662211827597728}
{"task": "task182_duorc_question_generation.json", "shot": 5, "num": 2629, "correct": 0, "acc": 0.0}
{"task": "task182_duorc_question_generation.json", "shot": 0, "num": 2629, "correct": 0, "acc": 0.0}
{"task": "task493_review_polarity_classification.json", "shot": 0, "num": 3250, "correct": 14, "acc": 0.0043076923076923075}
{"task": "task493_review_polarity_classification.json", "shot": 5, "num": 3250, "correct": 2966, "acc": 0.9126153846153846}
{"task": "task605_find_the_longest_common_subsequence_in_two_lists.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task605_find_the_longest_common_subsequence_in_two_lists.json", "shot": 5, "num": 3250, "correct": 70, "acc": 0.021538461538461538}
{"task": "task739_lhoestq_question_generation.json", "shot": 0, "num": 59, "correct": 0, "acc": 0.0}
{"task": "task739_lhoestq_question_generation.json", "shot": 5, "num": 59, "correct": 0, "acc": 0.0}
{"task": "task079_conala_concat_strings.json", "shot": 0, "num": 2497, "correct": 0, "acc": 0.0}
{"task": "task079_conala_concat_strings.json", "shot": 5, "num": 2497, "correct": 1041, "acc": 0.4169002803364037}
{"task": "task071_abductivenli_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task071_abductivenli_answer_generation.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task274_overruling_legal_classification.json", "shot": 0, "num": 1198, "correct": 5, "acc": 0.004173622704507512}
{"task": "task274_overruling_legal_classification.json", "shot": 5, "num": 1198, "correct": 917, "acc": 0.7654424040066778}
{"task": "task1560_blimp_binary_classification.json", "shot": 5, "num": 500, "correct": 265, "acc": 0.53}
{"task": "task1560_blimp_binary_classification.json", "shot": 0, "num": 500, "correct": 0, "acc": 0.0}
{"task": "task180_intervention_extraction.json", "shot": 0, "num": 212, "correct": 3, "acc": 0.014150943396226415}
{"task": "task180_intervention_extraction.json", "shot": 5, "num": 212, "correct": 130, "acc": 0.6132075471698113}
{"task": "task292_storycommonsense_character_text_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task292_storycommonsense_character_text_generation.json", "shot": 5, "num": 3250, "correct": 693, "acc": 0.21323076923076922}
{"task": "task477_cls_english_dvd_classification.json", "shot": 0, "num": 982, "correct": 3, "acc": 0.003054989816700611}
{"task": "task477_cls_english_dvd_classification.json", "shot": 5, "num": 982, "correct": 812, "acc": 0.8268839103869654}
{"task": "task350_winomt_classification_gender_identifiability_pro.json", "shot": 0, "num": 1579, "correct": 0, "acc": 0.0}
{"task": "task350_winomt_classification_gender_identifiability_pro.json", "shot": 5, "num": 1579, "correct": 759, "acc": 0.48068397720075995}
{"task": "task1207_atomic_classification_atlocation.json", "shot": 0, "num": 3247, "correct": 130, "acc": 0.04003695719125346}
{"task": "task1207_atomic_classification_atlocation.json", "shot": 5, "num": 3247, "correct": 2235, "acc": 0.6883276870957807}
{"task": "task963_librispeech_asr_next_word_prediction.json", "shot": 0, "num": 75, "correct": 0, "acc": 0.0}
{"task": "task963_librispeech_asr_next_word_prediction.json", "shot": 5, "num": 75, "correct": 9, "acc": 0.12}
{"task": "task162_count_words_starting_with_letter.json", "shot": 0, "num": 1864, "correct": 169, "acc": 0.09066523605150215}
{"task": "task162_count_words_starting_with_letter.json", "shot": 5, "num": 1864, "correct": 607, "acc": 0.32564377682403434}
{"task": "task665_mmmlu_answer_generation_anatomy.json", "shot": 0, "num": 76, "correct": 11, "acc": 0.14473684210526316}
{"task": "task665_mmmlu_answer_generation_anatomy.json", "shot": 5, "num": 76, "correct": 38, "acc": 0.5}
{"task": "task714_mmmlu_answer_generation_human_sexuality.json", "shot": 0, "num": 72, "correct": 19, "acc": 0.2638888888888889}
{"task": "task714_mmmlu_answer_generation_human_sexuality.json", "shot": 5, "num": 72, "correct": 34, "acc": 0.4722222222222222}
{"task": "task967_ruletaker_incorrect_fact_generation_based_on_given_paragraph.json", "shot": 0, "num": 148, "correct": 0, "acc": 0.0}
{"task": "task967_ruletaker_incorrect_fact_generation_based_on_given_paragraph.json", "shot": 5, "num": 148, "correct": 18, "acc": 0.12162162162162163}
{"task": "task127_scan_long_text_generation_action_command_all.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task127_scan_long_text_generation_action_command_all.json", "shot": 5, "num": 3249, "correct": 2, "acc": 0.0006155740227762388}
{"task": "task355_casino_classification_negotiation_other_need.json", "shot": 0, "num": 407, "correct": 0, "acc": 0.0}
{"task": "task355_casino_classification_negotiation_other_need.json", "shot": 5, "num": 407, "correct": 211, "acc": 0.5184275184275184}
{"task": "task207_max_element_lists.json", "shot": 0, "num": 2500, "correct": 0, "acc": 0.0}
{"task": "task207_max_element_lists.json", "shot": 5, "num": 2500, "correct": 39, "acc": 0.0156}
{"task": "task155_count_nouns_verbs.json", "shot": 0, "num": 3220, "correct": 0, "acc": 0.0}
{"task": "task155_count_nouns_verbs.json", "shot": 5, "num": 3220, "correct": 1105, "acc": 0.34316770186335405}
{"task": "task766_craigslist_bargains_classification.json", "shot": 0, "num": 100, "correct": 0, "acc": 0.0}
{"task": "task766_craigslist_bargains_classification.json", "shot": 5, "num": 100, "correct": 100, "acc": 1.0}
{"task": "task078_all_elements_except_last_i.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task078_all_elements_except_last_i.json", "shot": 5, "num": 3250, "correct": 192, "acc": 0.059076923076923075}
{"task": "task1495_adverse_drug_event_classification.json", "shot": 0, "num": 999, "correct": 0, "acc": 0.0}
{"task": "task1495_adverse_drug_event_classification.json", "shot": 5, "num": 999, "correct": 722, "acc": 0.7227227227227228}
{"task": "task322_jigsaw_classification_threat.json", "shot": 0, "num": 3193, "correct": 1, "acc": 0.00031318509238960227}
{"task": "task322_jigsaw_classification_threat.json", "shot": 5, "num": 3193, "correct": 2309, "acc": 0.7231443783275916}
{"task": "task1449_disease_entity_extraction_bc5cdr_dataset.json", "shot": 0, "num": 39, "correct": 0, "acc": 0.0}
{"task": "task1449_disease_entity_extraction_bc5cdr_dataset.json", "shot": 5, "num": 39, "correct": 24, "acc": 0.6153846153846154}
{"task": "task1606_ethos_text_classification.json", "shot": 0, "num": 93, "correct": 0, "acc": 0.0}
{"task": "task1606_ethos_text_classification.json", "shot": 5, "num": 93, "correct": 59, "acc": 0.6344086021505376}
{"task": "task1291_multi_news_summarization.json", "shot": 5, "num": 3247, "correct": 0, "acc": 0.0}
{"task": "task1291_multi_news_summarization.json", "shot": 0, "num": 3247, "correct": 0, "acc": 0.0}
{"task": "task745_ai2_arithmetic_questions_arithmetic.json", "shot": 0, "num": 189, "correct": 0, "acc": 0.0}
{"task": "task745_ai2_arithmetic_questions_arithmetic.json", "shot": 5, "num": 189, "correct": 98, "acc": 0.5185185185185185}
{"task": "task144_subjqa_question_answering.json", "shot": 0, "num": 327, "correct": 0, "acc": 0.0}
{"task": "task144_subjqa_question_answering.json", "shot": 5, "num": 327, "correct": 18, "acc": 0.05504587155963303}
{"task": "task325_jigsaw_classification_identity_attack.json", "shot": 0, "num": 3178, "correct": 0, "acc": 0.0}
{"task": "task325_jigsaw_classification_identity_attack.json", "shot": 5, "num": 3178, "correct": 1726, "acc": 0.5431088735053493}
{"task": "task377_remove_words_of_given_length.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task377_remove_words_of_given_length.json", "shot": 5, "num": 3250, "correct": 56, "acc": 0.01723076923076923}
{"task": "task1590_diplomacy_text_generation.json", "shot": 0, "num": 79, "correct": 0, "acc": 0.0}
{"task": "task1590_diplomacy_text_generation.json", "shot": 5, "num": 79, "correct": 0, "acc": 0.0}
{"task": "task1197_atomic_classification_oreact.json", "shot": 0, "num": 3247, "correct": 1631, "acc": 0.5023098244533415}
{"task": "task1197_atomic_classification_oreact.json", "shot": 5, "num": 3247, "correct": 1530, "acc": 0.4712041884816754}
{"task": "task1320_country_domain_tld.json", "shot": 5, "num": 126, "correct": 118, "acc": 0.9365079365079365}
{"task": "task1320_country_domain_tld.json", "shot": 0, "num": 126, "correct": 0, "acc": 0.0}
{"task": "task1327_qa_zre_answer_generation_from_question.json", "shot": 0, "num": 3031, "correct": 4, "acc": 0.0013196964698119432}
{"task": "task1327_qa_zre_answer_generation_from_question.json", "shot": 5, "num": 3031, "correct": 2100, "acc": 0.6928406466512702}
{"task": "task077_splash_explanation_to_sql.json", "shot": 0, "num": 1802, "correct": 0, "acc": 0.0}
{"task": "task077_splash_explanation_to_sql.json", "shot": 5, "num": 1802, "correct": 197, "acc": 0.10932297447280799}
{"task": "task028_drop_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task028_drop_answer_generation.json", "shot": 5, "num": 3250, "correct": 695, "acc": 0.21384615384615385}
{"task": "task196_sentiment140_answer_generation.json", "shot": 0, "num": 3250, "correct": 5, "acc": 0.0015384615384615385}
{"task": "task196_sentiment140_answer_generation.json", "shot": 5, "num": 3250, "correct": 2368, "acc": 0.7286153846153847}
{"task": "task1135_xcsr_en_commonsense_mc_classification.json", "shot": 0, "num": 497, "correct": 0, "acc": 0.0}
{"task": "task1135_xcsr_en_commonsense_mc_classification.json", "shot": 5, "num": 497, "correct": 123, "acc": 0.24748490945674045}
{"task": "task378_reverse_words_of_given_length.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task378_reverse_words_of_given_length.json", "shot": 5, "num": 3250, "correct": 161, "acc": 0.04953846153846154}
{"task": "task462_qasper_classification.json", "shot": 0, "num": 669, "correct": 0, "acc": 0.0}
{"task": "task462_qasper_classification.json", "shot": 5, "num": 669, "correct": 226, "acc": 0.33781763826606875}
{"task": "task100_concatenate_all_elements_from_index_i_to_j.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task100_concatenate_all_elements_from_index_i_to_j.json", "shot": 5, "num": 3250, "correct": 92, "acc": 0.028307692307692308}
{"task": "task716_mmmlu_answer_generation_jurisprudence.json", "shot": 0, "num": 61, "correct": 12, "acc": 0.19672131147540983}
{"task": "task716_mmmlu_answer_generation_jurisprudence.json", "shot": 5, "num": 61, "correct": 28, "acc": 0.45901639344262296}
{"task": "task374_synthetic_pos_or_neg_calculation.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task374_synthetic_pos_or_neg_calculation.json", "shot": 5, "num": 3249, "correct": 64, "acc": 0.019698368728839642}
{"task": "task1382_quarel_write_correct_answer.json", "shot": 0, "num": 399, "correct": 0, "acc": 0.0}
{"task": "task1382_quarel_write_correct_answer.json", "shot": 5, "num": 399, "correct": 163, "acc": 0.40852130325814534}
{"task": "task616_cola_classification.json", "shot": 0, "num": 3250, "correct": 250, "acc": 0.07692307692307693}
{"task": "task616_cola_classification.json", "shot": 5, "num": 3250, "correct": 1630, "acc": 0.5015384615384615}
{"task": "task1148_maximum_ascii_value.json", "shot": 0, "num": 100, "correct": 0, "acc": 0.0}
{"task": "task1148_maximum_ascii_value.json", "shot": 5, "num": 100, "correct": 17, "acc": 0.17}
{"task": "task137_detoxifying-lms_classification_toxicity.json", "shot": 0, "num": 195, "correct": 0, "acc": 0.0}
{"task": "task137_detoxifying-lms_classification_toxicity.json", "shot": 5, "num": 195, "correct": 1, "acc": 0.005128205128205128}
{"task": "task1151_swap_max_min.json", "shot": 0, "num": 100, "correct": 0, "acc": 0.0}
{"task": "task1151_swap_max_min.json", "shot": 5, "num": 100, "correct": 0, "acc": 0.0}
{"task": "task429_senteval_tense.json", "shot": 0, "num": 3243, "correct": 114, "acc": 0.03515263644773358}
{"task": "task429_senteval_tense.json", "shot": 5, "num": 3243, "correct": 1901, "acc": 0.5861856305889609}
{"task": "task683_online_privacy_policy_text_purpose_answer_generation.json", "shot": 0, "num": 1540, "correct": 1, "acc": 0.0006493506493506494}
{"task": "task683_online_privacy_policy_text_purpose_answer_generation.json", "shot": 5, "num": 1540, "correct": 893, "acc": 0.5798701298701299}
{"task": "task592_sciq_incorrect_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task592_sciq_incorrect_answer_generation.json", "shot": 5, "num": 3250, "correct": 1, "acc": 0.0003076923076923077}
{"task": "task1316_remove_duplicates_string.json", "shot": 0, "num": 100, "correct": 0, "acc": 0.0}
{"task": "task1316_remove_duplicates_string.json", "shot": 5, "num": 100, "correct": 3, "acc": 0.03}
{"task": "task195_sentiment140_classification.json", "shot": 0, "num": 3250, "correct": 723, "acc": 0.22246153846153846}
{"task": "task195_sentiment140_classification.json", "shot": 5, "num": 3250, "correct": 2328, "acc": 0.7163076923076923}
{"task": "task139_detoxifying-lms_classification_topicality.json", "shot": 0, "num": 195, "correct": 0, "acc": 0.0}
{"task": "task139_detoxifying-lms_classification_topicality.json", "shot": 5, "num": 195, "correct": 17, "acc": 0.08717948717948718}
{"task": "task1398_obqa_question_generation.json", "shot": 0, "num": 473, "correct": 0, "acc": 0.0}
{"task": "task1398_obqa_question_generation.json", "shot": 5, "num": 473, "correct": 2, "acc": 0.004228329809725159}
{"task": "task1315_find_range_array.json", "shot": 0, "num": 100, "correct": 0, "acc": 0.0}
{"task": "task1315_find_range_array.json", "shot": 5, "num": 100, "correct": 4, "acc": 0.04}
{"task": "task305_jeopardy_answer_generation_normal.json", "shot": 0, "num": 3250, "correct": 361, "acc": 0.11107692307692307}
{"task": "task305_jeopardy_answer_generation_normal.json", "shot": 5, "num": 3250, "correct": 1298, "acc": 0.3993846153846154}
{"task": "task1313_amazonreview_polarity_classification.json", "shot": 0, "num": 1511, "correct": 541, "acc": 0.35804103242885504}
{"task": "task1313_amazonreview_polarity_classification.json", "shot": 5, "num": 1511, "correct": 1329, "acc": 0.8795499669093315}
{"task": "task755_find_longest_substring_and_replace_its_sorted_lowercase_version_in_both_lists.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task755_find_longest_substring_and_replace_its_sorted_lowercase_version_in_both_lists.json", "shot": 5, "num": 3250, "correct": 70, "acc": 0.021538461538461538}
{"task": "task381_boolq_question_generation.json", "shot": 0, "num": 703, "correct": 0, "acc": 0.0}
{"task": "task381_boolq_question_generation.json", "shot": 5, "num": 703, "correct": 0, "acc": 0.0}
{"task": "task327_jigsaw_classification_toxic.json", "shot": 0, "num": 2783, "correct": 0, "acc": 0.0}
{"task": "task327_jigsaw_classification_toxic.json", "shot": 5, "num": 2783, "correct": 1863, "acc": 0.6694214876033058}
{"task": "task1714_convai3_sentence_generation.json", "shot": 0, "num": 1148, "correct": 0, "acc": 0.0}
{"task": "task1714_convai3_sentence_generation.json", "shot": 5, "num": 1148, "correct": 7, "acc": 0.006097560975609756}
{"task": "task689_mmmlu_answer_generation_college_mathematics.json", "shot": 0, "num": 57, "correct": 0, "acc": 0.0}
{"task": "task689_mmmlu_answer_generation_college_mathematics.json", "shot": 5, "num": 57, "correct": 13, "acc": 0.22807017543859648}
{"task": "task857_inquisitive_question_generation.json", "shot": 0, "num": 81, "correct": 0, "acc": 0.0}
{"task": "task857_inquisitive_question_generation.json", "shot": 5, "num": 81, "correct": 1, "acc": 0.012345679012345678}
{"task": "task1661_super_glue_classification.json", "shot": 0, "num": 2000, "correct": 0, "acc": 0.0}
{"task": "task1661_super_glue_classification.json", "shot": 5, "num": 2000, "correct": 1530, "acc": 0.765}
{"task": "task146_afs_argument_similarity_gun_control.json", "shot": 0, "num": 1000, "correct": 0, "acc": 0.0}
{"task": "task146_afs_argument_similarity_gun_control.json", "shot": 5, "num": 1000, "correct": 424, "acc": 0.424}
{"task": "task1366_healthfact_classification.json", "shot": 5, "num": 123, "correct": 2, "acc": 0.016260162601626018}
{"task": "task1366_healthfact_classification.json", "shot": 0, "num": 123, "correct": 0, "acc": 0.0}
{"task": "task1328_qa_zre_relation_generation_from_question.json", "shot": 0, "num": 2920, "correct": 0, "acc": 0.0}
{"task": "task1328_qa_zre_relation_generation_from_question.json", "shot": 5, "num": 2920, "correct": 2304, "acc": 0.7890410958904109}
{"task": "task478_cls_english_music_classification.json", "shot": 0, "num": 985, "correct": 20, "acc": 0.02030456852791878}
{"task": "task478_cls_english_music_classification.json", "shot": 5, "num": 985, "correct": 831, "acc": 0.8436548223350254}
{"task": "task703_mmmlu_answer_generation_high_school_geography.json", "shot": 0, "num": 87, "correct": 15, "acc": 0.1724137931034483}
{"task": "task703_mmmlu_answer_generation_high_school_geography.json", "shot": 5, "num": 87, "correct": 45, "acc": 0.5172413793103449}
{"task": "task1601_webquestions_answer_generation.json", "shot": 0, "num": 1888, "correct": 0, "acc": 0.0}
{"task": "task1601_webquestions_answer_generation.json", "shot": 5, "num": 1888, "correct": 343, "acc": 0.1816737288135593}
{"task": "task1379_quarel_incorrect_answer_generation.json", "shot": 0, "num": 399, "correct": 0, "acc": 0.0}
{"task": "task1379_quarel_incorrect_answer_generation.json", "shot": 5, "num": 399, "correct": 179, "acc": 0.44862155388471175}
{"task": "task639_multi_woz_user_utterance_generation.json", "shot": 0, "num": 89, "correct": 0, "acc": 0.0}
{"task": "task639_multi_woz_user_utterance_generation.json", "shot": 5, "num": 89, "correct": 0, "acc": 0.0}
{"task": "task1722_civil_comments_threat_classification.json", "shot": 0, "num": 499, "correct": 0, "acc": 0.0}
{"task": "task1722_civil_comments_threat_classification.json", "shot": 5, "num": 499, "correct": 283, "acc": 0.5671342685370742}
{"task": "task243_count_elements_in_set_intersection.json", "shot": 0, "num": 499, "correct": 0, "acc": 0.0}
{"task": "task243_count_elements_in_set_intersection.json", "shot": 5, "num": 499, "correct": 72, "acc": 0.14428857715430862}
{"task": "task125_conala_pair_differences.json", "shot": 0, "num": 2499, "correct": 0, "acc": 0.0}
{"task": "task125_conala_pair_differences.json", "shot": 5, "num": 2499, "correct": 21, "acc": 0.008403361344537815}
{"task": "task922_event2mind_word_generation.json", "shot": 0, "num": 218, "correct": 0, "acc": 0.0}
{"task": "task922_event2mind_word_generation.json", "shot": 5, "num": 218, "correct": 14, "acc": 0.06422018348623854}
{"task": "task115_help_advice_classification.json", "shot": 0, "num": 2873, "correct": 318, "acc": 0.11068569439610164}
{"task": "task115_help_advice_classification.json", "shot": 5, "num": 2873, "correct": 1652, "acc": 0.5750087017055343}
{"task": "task626_xlwic_sentence_based_on_given_word_sentence_generation.json", "shot": 0, "num": 134, "correct": 0, "acc": 0.0}
{"task": "task626_xlwic_sentence_based_on_given_word_sentence_generation.json", "shot": 5, "num": 134, "correct": 0, "acc": 0.0}
{"task": "task1191_food_veg_nonveg.json", "shot": 5, "num": 51, "correct": 37, "acc": 0.7254901960784313}
{"task": "task1191_food_veg_nonveg.json", "shot": 0, "num": 51, "correct": 4, "acc": 0.0784313725490196}
{"task": "task1210_atomic_classification_madeupof.json", "shot": 0, "num": 2650, "correct": 3, "acc": 0.0011320754716981133}
{"task": "task1210_atomic_classification_madeupof.json", "shot": 5, "num": 2650, "correct": 1841, "acc": 0.6947169811320755}
{"task": "task131_scan_long_text_generation_action_command_long.json", "shot": 0, "num": 918, "correct": 0, "acc": 0.0}
{"task": "task131_scan_long_text_generation_action_command_long.json", "shot": 5, "num": 918, "correct": 2, "acc": 0.002178649237472767}
{"task": "task886_quail_question_generation.json", "shot": 0, "num": 250, "correct": 0, "acc": 0.0}
{"task": "task886_quail_question_generation.json", "shot": 5, "num": 250, "correct": 1, "acc": 0.004}
{"task": "task1444_round_power_of_two.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1444_round_power_of_two.json", "shot": 5, "num": 3250, "correct": 10, "acc": 0.003076923076923077}
{"task": "task581_socialiqa_question_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task581_socialiqa_question_generation.json", "shot": 5, "num": 3250, "correct": 170, "acc": 0.052307692307692305}
{"task": "task1354_sent_comp_classification.json", "shot": 0, "num": 500, "correct": 311, "acc": 0.622}
{"task": "task1354_sent_comp_classification.json", "shot": 5, "num": 500, "correct": 434, "acc": 0.868}
{"task": "task900_freebase_qa_category_classification.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task900_freebase_qa_category_classification.json", "shot": 5, "num": 3249, "correct": 608, "acc": 0.1871345029239766}
{"task": "task284_imdb_classification.json", "shot": 0, "num": 3250, "correct": 8, "acc": 0.0024615384615384616}
{"task": "task284_imdb_classification.json", "shot": 5, "num": 3250, "correct": 2184, "acc": 0.672}
{"task": "task307_jeopardy_answer_generation_final.json", "shot": 0, "num": 1783, "correct": 58, "acc": 0.03252944475602917}
{"task": "task307_jeopardy_answer_generation_final.json", "shot": 5, "num": 1783, "correct": 383, "acc": 0.2148065058889512}
{"task": "task767_craigslist_bargains_classification.json", "shot": 0, "num": 600, "correct": 0, "acc": 0.0}
{"task": "task767_craigslist_bargains_classification.json", "shot": 5, "num": 600, "correct": 413, "acc": 0.6883333333333334}
{"task": "task299_storycloze_sentence_generation.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task299_storycloze_sentence_generation.json", "shot": 5, "num": 3249, "correct": 1, "acc": 0.0003077870113881194}
{"task": "task863_asdiv_multiop_question_answering.json", "shot": 0, "num": 138, "correct": 0, "acc": 0.0}
{"task": "task863_asdiv_multiop_question_answering.json", "shot": 5, "num": 138, "correct": 11, "acc": 0.07971014492753623}
{"task": "task966_ruletaker_fact_checking_based_on_given_context.json", "shot": 0, "num": 150, "correct": 1, "acc": 0.006666666666666667}
{"task": "task966_ruletaker_fact_checking_based_on_given_context.json", "shot": 5, "num": 150, "correct": 117, "acc": 0.78}
{"task": "task1426_country_independence_year.json", "shot": 0, "num": 95, "correct": 0, "acc": 0.0}
{"task": "task1426_country_independence_year.json", "shot": 5, "num": 95, "correct": 66, "acc": 0.6947368421052632}
{"task": "task1325_qa_zre_question_generation_on_subject_relation.json", "shot": 0, "num": 2566, "correct": 1, "acc": 0.0003897116134060795}
{"task": "task1325_qa_zre_question_generation_on_subject_relation.json", "shot": 5, "num": 2566, "correct": 45, "acc": 0.017537022603273576}
{"task": "task1283_hrngo_quality_classification.json", "shot": 0, "num": 1988, "correct": 0, "acc": 0.0}
{"task": "task1283_hrngo_quality_classification.json", "shot": 5, "num": 1988, "correct": 974, "acc": 0.48993963782696176}
{"task": "task080_piqa_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task080_piqa_answer_generation.json", "shot": 5, "num": 3250, "correct": 1, "acc": 0.0003076923076923077}
{"task": "task280_stereoset_classification_stereotype_type.json", "shot": 0, "num": 1854, "correct": 0, "acc": 0.0}
{"task": "task280_stereoset_classification_stereotype_type.json", "shot": 5, "num": 1854, "correct": 966, "acc": 0.5210355987055016}
{"task": "task293_storycommonsense_emotion_text_generation.json", "shot": 0, "num": 3250, "correct": 6, "acc": 0.0018461538461538461}
{"task": "task293_storycommonsense_emotion_text_generation.json", "shot": 5, "num": 3250, "correct": 256, "acc": 0.07876923076923077}
{"task": "task565_circa_answer_generation.json", "shot": 0, "num": 1472, "correct": 0, "acc": 0.0}
{"task": "task565_circa_answer_generation.json", "shot": 5, "num": 1472, "correct": 1, "acc": 0.0006793478260869565}
{"task": "task279_stereoset_classification_stereotype.json", "shot": 0, "num": 3245, "correct": 0, "acc": 0.0}
{"task": "task279_stereoset_classification_stereotype.json", "shot": 5, "num": 3245, "correct": 1470, "acc": 0.4530046224961479}
{"task": "task113_count_frequency_of_letter.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task113_count_frequency_of_letter.json", "shot": 5, "num": 3250, "correct": 772, "acc": 0.23753846153846153}
{"task": "task1381_quarel_incorrect_option_generation.json", "shot": 0, "num": 399, "correct": 0, "acc": 0.0}
{"task": "task1381_quarel_incorrect_option_generation.json", "shot": 5, "num": 399, "correct": 200, "acc": 0.5012531328320802}
{"task": "task901_freebase_qa_category_question_generation.json", "shot": 0, "num": 35, "correct": 0, "acc": 0.0}
{"task": "task901_freebase_qa_category_question_generation.json", "shot": 5, "num": 35, "correct": 0, "acc": 0.0}
{"task": "task206_collatz_conjecture.json", "shot": 0, "num": 2499, "correct": 0, "acc": 0.0}
{"task": "task206_collatz_conjecture.json", "shot": 5, "num": 2499, "correct": 5, "acc": 0.0020008003201280513}
{"task": "task306_jeopardy_answer_generation_double.json", "shot": 0, "num": 3250, "correct": 321, "acc": 0.09876923076923078}
{"task": "task306_jeopardy_answer_generation_double.json", "shot": 5, "num": 3250, "correct": 1281, "acc": 0.39415384615384613}
{"task": "task1665_trainglecopa_question_generation.json", "shot": 5, "num": 50, "correct": 5, "acc": 0.1}
{"task": "task1665_trainglecopa_question_generation.json", "shot": 0, "num": 50, "correct": 0, "acc": 0.0}
{"task": "task1487_organism_substance_extraction_anem_dataset.json", "shot": 0, "num": 53, "correct": 0, "acc": 0.0}
{"task": "task1487_organism_substance_extraction_anem_dataset.json", "shot": 5, "num": 53, "correct": 27, "acc": 0.5094339622641509}
{"task": "task074_squad1.1_question_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task074_squad1.1_question_generation.json", "shot": 5, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task889_goemotions_classification.json", "shot": 0, "num": 250, "correct": 36, "acc": 0.144}
{"task": "task889_goemotions_classification.json", "shot": 5, "num": 250, "correct": 149, "acc": 0.596}
{"task": "task095_conala_max_absolute_value.json", "shot": 0, "num": 2488, "correct": 0, "acc": 0.0}
{"task": "task095_conala_max_absolute_value.json", "shot": 5, "num": 2488, "correct": 1299, "acc": 0.5221061093247589}
{"task": "task1720_civil_comments_toxicity_classification.json", "shot": 0, "num": 500, "correct": 9, "acc": 0.018}
{"task": "task1720_civil_comments_toxicity_classification.json", "shot": 5, "num": 500, "correct": 254, "acc": 0.508}
{"task": "task929_products_reviews_classification.json", "shot": 0, "num": 3245, "correct": 0, "acc": 0.0}
{"task": "task929_products_reviews_classification.json", "shot": 5, "num": 3245, "correct": 2737, "acc": 0.8434514637904469}
{"task": "task638_multi_woz_classification.json", "shot": 0, "num": 2290, "correct": 0, "acc": 0.0}
{"task": "task638_multi_woz_classification.json", "shot": 5, "num": 2290, "correct": 962, "acc": 0.4200873362445415}
{"task": "task147_afs_argument_similarity_gay_marriage.json", "shot": 0, "num": 767, "correct": 0, "acc": 0.0}
{"task": "task147_afs_argument_similarity_gay_marriage.json", "shot": 5, "num": 767, "correct": 317, "acc": 0.4132985658409387}
{"task": "task248_dream_classification.json", "shot": 0, "num": 413, "correct": 0, "acc": 0.0}
{"task": "task248_dream_classification.json", "shot": 5, "num": 413, "correct": 78, "acc": 0.18886198547215496}
{"task": "task1484_gene_extraction_linnaeus_dataset.json", "shot": 0, "num": 71, "correct": 0, "acc": 0.0}
{"task": "task1484_gene_extraction_linnaeus_dataset.json", "shot": 5, "num": 71, "correct": 59, "acc": 0.8309859154929577}
{"task": "task752_svamp_multiplication_question_answering.json", "shot": 0, "num": 54, "correct": 0, "acc": 0.0}
{"task": "task752_svamp_multiplication_question_answering.json", "shot": 5, "num": 54, "correct": 17, "acc": 0.3148148148148148}
{"task": "task096_conala_list_index_subtraction.json", "shot": 0, "num": 2459, "correct": 0, "acc": 0.0}
{"task": "task096_conala_list_index_subtraction.json", "shot": 5, "num": 2459, "correct": 28, "acc": 0.011386742578283855}
{"task": "task400_paws_paraphrase_classification.json", "shot": 0, "num": 3243, "correct": 36, "acc": 0.011100832562442183}
{"task": "task400_paws_paraphrase_classification.json", "shot": 5, "num": 3243, "correct": 1775, "acc": 0.5473327166204132}
{"task": "task1321_country_continent.json", "shot": 0, "num": 119, "correct": 2, "acc": 0.01680672268907563}
{"task": "task1321_country_continent.json", "shot": 5, "num": 119, "correct": 115, "acc": 0.9663865546218487}
{"task": "task212_logic2text_classification.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task212_logic2text_classification.json", "shot": 5, "num": 3249, "correct": 918, "acc": 0.28254847645429365}
{"task": "task383_matres_classification.json", "shot": 0, "num": 959, "correct": 0, "acc": 0.0}
{"task": "task383_matres_classification.json", "shot": 5, "num": 959, "correct": 534, "acc": 0.556830031282586}
{"task": "task340_winomt_classification_gender_pro.json", "shot": 0, "num": 788, "correct": 0, "acc": 0.0}
{"task": "task340_winomt_classification_gender_pro.json", "shot": 5, "num": 788, "correct": 752, "acc": 0.9543147208121827}
{"task": "task428_senteval_inversion.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task428_senteval_inversion.json", "shot": 5, "num": 3250, "correct": 1836, "acc": 0.564923076923077}
{"task": "task917_coqa_question_generation.json", "shot": 0, "num": 59, "correct": 0, "acc": 0.0}
{"task": "task917_coqa_question_generation.json", "shot": 5, "num": 59, "correct": 0, "acc": 0.0}
{"task": "task1566_propara_structured_text_generation.json", "shot": 0, "num": 100, "correct": 0, "acc": 0.0}
{"task": "task1566_propara_structured_text_generation.json", "shot": 5, "num": 100, "correct": 3, "acc": 0.03}
{"task": "task366_synthetic_return_primes.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task366_synthetic_return_primes.json", "shot": 5, "num": 3250, "correct": 116, "acc": 0.03569230769230769}
{"task": "task1518_limit_answer_generation.json", "shot": 0, "num": 498, "correct": 0, "acc": 0.0}
{"task": "task1518_limit_answer_generation.json", "shot": 5, "num": 498, "correct": 113, "acc": 0.22690763052208834}
{"task": "task210_logic2text_structured_text_generation.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task210_logic2text_structured_text_generation.json", "shot": 5, "num": 3249, "correct": 60, "acc": 0.018467220683287166}
{"task": "task143_odd-man-out_classification_generate_category.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task143_odd-man-out_classification_generate_category.json", "shot": 5, "num": 3250, "correct": 1314, "acc": 0.4043076923076923}
{"task": "task389_torque_generate_temporal_question.json", "shot": 0, "num": 1134, "correct": 0, "acc": 0.0}
{"task": "task389_torque_generate_temporal_question.json", "shot": 5, "num": 1134, "correct": 0, "acc": 0.0}
{"task": "task141_odd-man-out_classification_category.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task141_odd-man-out_classification_category.json", "shot": 5, "num": 3250, "correct": 1265, "acc": 0.3892307692307692}
{"task": "task181_outcome_extraction.json", "shot": 0, "num": 212, "correct": 0, "acc": 0.0}
{"task": "task181_outcome_extraction.json", "shot": 5, "num": 212, "correct": 85, "acc": 0.4009433962264151}
{"task": "task618_amazonreview_summary_text_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task618_amazonreview_summary_text_generation.json", "shot": 5, "num": 3250, "correct": 30, "acc": 0.009230769230769232}
{"task": "task732_mmmlu_answer_generation_public_relations.json", "shot": 0, "num": 60, "correct": 13, "acc": 0.21666666666666667}
{"task": "task732_mmmlu_answer_generation_public_relations.json", "shot": 5, "num": 60, "correct": 31, "acc": 0.5166666666666667}
{"task": "task460_qasper_answer_generation.json", "shot": 0, "num": 1336, "correct": 0, "acc": 0.0}
{"task": "task460_qasper_answer_generation.json", "shot": 5, "num": 1336, "correct": 139, "acc": 0.10404191616766467}
{"task": "task1504_hatexplain_answer_generation.json", "shot": 0, "num": 2052, "correct": 0, "acc": 0.0}
{"task": "task1504_hatexplain_answer_generation.json", "shot": 5, "num": 2052, "correct": 202, "acc": 0.09844054580896686}
{"task": "task093_conala_normalize_lists.json", "shot": 0, "num": 2500, "correct": 0, "acc": 0.0}
{"task": "task093_conala_normalize_lists.json", "shot": 5, "num": 2500, "correct": 1, "acc": 0.0004}
{"task": "task1542_every_ith_element_from_starting.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1542_every_ith_element_from_starting.json", "shot": 5, "num": 3250, "correct": 77, "acc": 0.023692307692307693}
{"task": "task145_afs_argument_similarity_death_penalty.json", "shot": 0, "num": 1000, "correct": 0, "acc": 0.0}
{"task": "task145_afs_argument_similarity_death_penalty.json", "shot": 5, "num": 1000, "correct": 465, "acc": 0.465}
{"task": "task712_mmmlu_answer_generation_high_school_world_history.json", "shot": 0, "num": 89, "correct": 0, "acc": 0.0}
{"task": "task712_mmmlu_answer_generation_high_school_world_history.json", "shot": 5, "num": 89, "correct": 28, "acc": 0.3146067415730337}
{"task": "task598_cuad_answer_generation.json", "shot": 0, "num": 204, "correct": 0, "acc": 0.0}
{"task": "task598_cuad_answer_generation.json", "shot": 5, "num": 204, "correct": 0, "acc": 0.0}
{"task": "task897_freebase_qa_topic_question_generation.json", "shot": 0, "num": 1945, "correct": 0, "acc": 0.0}
{"task": "task897_freebase_qa_topic_question_generation.json", "shot": 5, "num": 1945, "correct": 2, "acc": 0.0010282776349614395}
{"task": "task588_amazonfood_rating_classification.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task588_amazonfood_rating_classification.json", "shot": 5, "num": 3250, "correct": 2012, "acc": 0.6190769230769231}
{"task": "task275_enhanced_wsc_paraphrase_generation.json", "shot": 0, "num": 589, "correct": 0, "acc": 0.0}
{"task": "task275_enhanced_wsc_paraphrase_generation.json", "shot": 5, "num": 589, "correct": 10, "acc": 0.01697792869269949}
{"task": "task149_afs_argument_quality_death_penalty.json", "shot": 0, "num": 933, "correct": 0, "acc": 0.0}
{"task": "task149_afs_argument_quality_death_penalty.json", "shot": 5, "num": 933, "correct": 634, "acc": 0.6795284030010718}
{"task": "task333_hateeval_classification_hate_en.json", "shot": 0, "num": 2997, "correct": 4, "acc": 0.001334668001334668}
{"task": "task333_hateeval_classification_hate_en.json", "shot": 5, "num": 2997, "correct": 1799, "acc": 0.600266933600267}
{"task": "task1404_date_conversion.json", "shot": 0, "num": 98, "correct": 0, "acc": 0.0}
{"task": "task1404_date_conversion.json", "shot": 5, "num": 98, "correct": 96, "acc": 0.9795918367346939}
{"task": "task919_coqa_incorrect_answer_generation.json", "shot": 0, "num": 59, "correct": 0, "acc": 0.0}
{"task": "task919_coqa_incorrect_answer_generation.json", "shot": 5, "num": 59, "correct": 0, "acc": 0.0}
{"task": "task1452_location_entity_extraction_btc_corpus.json", "shot": 0, "num": 88, "correct": 0, "acc": 0.0}
{"task": "task1452_location_entity_extraction_btc_corpus.json", "shot": 5, "num": 88, "correct": 47, "acc": 0.5340909090909091}
{"task": "task183_rhyme_generation.json", "shot": 0, "num": 500, "correct": 0, "acc": 0.0}
{"task": "task183_rhyme_generation.json", "shot": 5, "num": 500, "correct": 0, "acc": 0.0}
{"task": "task1311_amazonreview_rating_classification.json", "shot": 0, "num": 2275, "correct": 1359, "acc": 0.5973626373626374}
{"task": "task1311_amazonreview_rating_classification.json", "shot": 5, "num": 2275, "correct": 1671, "acc": 0.7345054945054945}
{"task": "task158_count_frequency_of_words.json", "shot": 0, "num": 1024, "correct": 0, "acc": 0.0}
{"task": "task158_count_frequency_of_words.json", "shot": 5, "num": 1024, "correct": 598, "acc": 0.583984375}
{"task": "task495_semeval_headline_classification.json", "shot": 0, "num": 3248, "correct": 0, "acc": 0.0}
{"task": "task495_semeval_headline_classification.json", "shot": 5, "num": 3248, "correct": 953, "acc": 0.29341133004926107}
{"task": "task1445_closest_integers.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task1445_closest_integers.json", "shot": 5, "num": 3250, "correct": 206, "acc": 0.06338461538461539}
{"task": "task628_xlwic_word_with_different_meaning_sentence_generation.json", "shot": 0, "num": 20, "correct": 0, "acc": 0.0}
{"task": "task628_xlwic_word_with_different_meaning_sentence_generation.json", "shot": 5, "num": 20, "correct": 0, "acc": 0.0}
{"task": "task1500_dstc3_classification.json", "shot": 0, "num": 476, "correct": 0, "acc": 0.0}
{"task": "task1500_dstc3_classification.json", "shot": 5, "num": 476, "correct": 244, "acc": 0.5126050420168067}
{"task": "task1592_yahoo_answers_topics_classfication.json", "shot": 0, "num": 497, "correct": 0, "acc": 0.0}
{"task": "task1592_yahoo_answers_topics_classfication.json", "shot": 5, "num": 497, "correct": 151, "acc": 0.3038229376257545}
{"task": "task761_app_review_classification.json", "shot": 0, "num": 149, "correct": 27, "acc": 0.18120805369127516}
{"task": "task761_app_review_classification.json", "shot": 5, "num": 149, "correct": 146, "acc": 0.9798657718120806}
{"task": "task084_babi_t1_single_supporting_fact_identify_relevant_fact.json", "shot": 0, "num": 496, "correct": 0, "acc": 0.0}
{"task": "task084_babi_t1_single_supporting_fact_identify_relevant_fact.json", "shot": 5, "num": 496, "correct": 249, "acc": 0.5020161290322581}
{"task": "task1481_gene_extraction_bc2gm_dataset.json", "shot": 0, "num": 531, "correct": 0, "acc": 0.0}
{"task": "task1481_gene_extraction_bc2gm_dataset.json", "shot": 5, "num": 531, "correct": 158, "acc": 0.2975517890772128}
{"task": "task1706_ljspeech_classification.json", "shot": 0, "num": 50, "correct": 0, "acc": 0.0}
{"task": "task1706_ljspeech_classification.json", "shot": 5, "num": 50, "correct": 47, "acc": 0.94}
{"task": "task316_crows-pairs_classification_stereotype.json", "shot": 0, "num": 1505, "correct": 0, "acc": 0.0}
{"task": "task316_crows-pairs_classification_stereotype.json", "shot": 5, "num": 1505, "correct": 766, "acc": 0.5089700996677741}
{"task": "task908_dialogre_identify_familial_relationships.json", "shot": 0, "num": 45, "correct": 0, "acc": 0.0}
{"task": "task908_dialogre_identify_familial_relationships.json", "shot": 5, "num": 45, "correct": 9, "acc": 0.2}
{"task": "task627_xlwic_word_with_same_meaning_sentence_generation.json", "shot": 0, "num": 78, "correct": 0, "acc": 0.0}
{"task": "task627_xlwic_word_with_same_meaning_sentence_generation.json", "shot": 5, "num": 78, "correct": 1, "acc": 0.01282051282051282}
{"task": "task1206_atomic_classification_isbefore.json", "shot": 0, "num": 3247, "correct": 159, "acc": 0.048968278410840775}
{"task": "task1206_atomic_classification_isbefore.json", "shot": 5, "num": 3247, "correct": 2079, "acc": 0.6402833384662766}
{"task": "task887_quail_answer_generation.json", "shot": 0, "num": 250, "correct": 0, "acc": 0.0}
{"task": "task887_quail_answer_generation.json", "shot": 5, "num": 250, "correct": 60, "acc": 0.24}
{"task": "task1489_sarcasmdetection_tweet_classification.json", "shot": 0, "num": 59, "correct": 0, "acc": 0.0}
{"task": "task1489_sarcasmdetection_tweet_classification.json", "shot": 5, "num": 59, "correct": 40, "acc": 0.6779661016949152}
{"task": "task368_synthetic_even_or_odd_calculation.json", "shot": 0, "num": 3248, "correct": 0, "acc": 0.0}
{"task": "task368_synthetic_even_or_odd_calculation.json", "shot": 5, "num": 3248, "correct": 6, "acc": 0.0018472906403940886}
{"task": "task1167_penn_treebank_coarse_pos_tagging.json", "shot": 0, "num": 3240, "correct": 0, "acc": 0.0}
{"task": "task1167_penn_treebank_coarse_pos_tagging.json", "shot": 5, "num": 3240, "correct": 1152, "acc": 0.35555555555555557}
{"task": "task430_senteval_subject_count.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task430_senteval_subject_count.json", "shot": 5, "num": 3250, "correct": 1975, "acc": 0.6076923076923076}
{"task": "task122_conala_list_index_addition.json", "shot": 0, "num": 2500, "correct": 0, "acc": 0.0}
{"task": "task122_conala_list_index_addition.json", "shot": 5, "num": 2500, "correct": 0, "acc": 0.0}
{"task": "task1428_country_surface_area.json", "shot": 0, "num": 119, "correct": 0, "acc": 0.0}
{"task": "task1428_country_surface_area.json", "shot": 5, "num": 119, "correct": 23, "acc": 0.19327731092436976}
{"task": "task1200_atomic_classification_xeffect.json", "shot": 0, "num": 3247, "correct": 1798, "acc": 0.5537419156144133}
{"task": "task1200_atomic_classification_xeffect.json", "shot": 5, "num": 3247, "correct": 1714, "acc": 0.527871881736988}
{"task": "task1317_country_calling_code.json", "shot": 0, "num": 137, "correct": 99, "acc": 0.7226277372262774}
{"task": "task1317_country_calling_code.json", "shot": 5, "num": 137, "correct": 116, "acc": 0.8467153284671532}
{"task": "task898_freebase_qa_answer_generation.json", "shot": 0, "num": 3250, "correct": 172, "acc": 0.05292307692307692}
{"task": "task898_freebase_qa_answer_generation.json", "shot": 5, "num": 3250, "correct": 1821, "acc": 0.5603076923076923}
{"task": "task844_financial_phrasebank_classification.json", "shot": 0, "num": 2419, "correct": 866, "acc": 0.3579991732120711}
{"task": "task844_financial_phrasebank_classification.json", "shot": 5, "num": 2419, "correct": 1612, "acc": 0.666391070690368}
{"task": "task472_haspart_classification.json", "shot": 0, "num": 2952, "correct": 0, "acc": 0.0}
{"task": "task472_haspart_classification.json", "shot": 5, "num": 2952, "correct": 1570, "acc": 0.5318428184281843}
{"task": "task1519_qa_srl_question_generation.json", "shot": 0, "num": 172, "correct": 0, "acc": 0.0}
{"task": "task1519_qa_srl_question_generation.json", "shot": 5, "num": 172, "correct": 0, "acc": 0.0}
{"task": "task123_conala_sort_dictionary.json", "shot": 0, "num": 2500, "correct": 0, "acc": 0.0}
{"task": "task123_conala_sort_dictionary.json", "shot": 5, "num": 2500, "correct": 0, "acc": 0.0}
{"task": "task1312_amazonreview_polarity_classification.json", "shot": 0, "num": 3250, "correct": 43, "acc": 0.01323076923076923}
{"task": "task1312_amazonreview_polarity_classification.json", "shot": 5, "num": 3250, "correct": 3162, "acc": 0.9729230769230769}
{"task": "task062_bigbench_repeat_copy_logic.json", "shot": 0, "num": 15, "correct": 0, "acc": 0.0}
{"task": "task062_bigbench_repeat_copy_logic.json", "shot": 5, "num": 15, "correct": 3, "acc": 0.2}
{"task": "task024_cosmosqa_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task024_cosmosqa_answer_generation.json", "shot": 5, "num": 3250, "correct": 17, "acc": 0.005230769230769231}
{"task": "task384_socialiqa_question_classification.json", "shot": 0, "num": 2438, "correct": 0, "acc": 0.0}
{"task": "task384_socialiqa_question_classification.json", "shot": 5, "num": 2438, "correct": 1214, "acc": 0.49794913863822804}
{"task": "task177_para-nmt_paraphrasing.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task177_para-nmt_paraphrasing.json", "shot": 5, "num": 3250, "correct": 3, "acc": 0.0009230769230769231}
{"task": "task723_mmmlu_answer_generation_moral_disputes.json", "shot": 0, "num": 95, "correct": 14, "acc": 0.14736842105263157}
{"task": "task723_mmmlu_answer_generation_moral_disputes.json", "shot": 5, "num": 95, "correct": 41, "acc": 0.43157894736842106}
{"task": "task819_pec_sentiment_classification.json", "shot": 0, "num": 38, "correct": 0, "acc": 0.0}
{"task": "task819_pec_sentiment_classification.json", "shot": 5, "num": 38, "correct": 30, "acc": 0.7894736842105263}
{"task": "task597_cuad_answer_generation.json", "shot": 0, "num": 204, "correct": 0, "acc": 0.0}
{"task": "task597_cuad_answer_generation.json", "shot": 5, "num": 204, "correct": 0, "acc": 0.0}
{"task": "task865_mawps_addsub_question_answering.json", "shot": 0, "num": 584, "correct": 0, "acc": 0.0}
{"task": "task865_mawps_addsub_question_answering.json", "shot": 5, "num": 584, "correct": 223, "acc": 0.3818493150684932}
{"task": "task1609_xquad_en_question_generation.json", "shot": 0, "num": 120, "correct": 0, "acc": 0.0}
{"task": "task1609_xquad_en_question_generation.json", "shot": 5, "num": 120, "correct": 0, "acc": 0.0}
{"task": "task111_asset_sentence_simplification.json", "shot": 0, "num": 1000, "correct": 0, "acc": 0.0}
{"task": "task111_asset_sentence_simplification.json", "shot": 5, "num": 1000, "correct": 0, "acc": 0.0}
{"task": "task159_check_frequency_of_words_in_sentence_pair.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task159_check_frequency_of_words_in_sentence_pair.json", "shot": 5, "num": 3250, "correct": 1983, "acc": 0.6101538461538462}
{"task": "task156_codah_classification_adversarial.json", "shot": 0, "num": 1386, "correct": 0, "acc": 0.0}
{"task": "task156_codah_classification_adversarial.json", "shot": 5, "num": 1386, "correct": 687, "acc": 0.49567099567099565}
{"task": "task688_mmmlu_answer_generation_college_computer_science.json", "shot": 0, "num": 57, "correct": 3, "acc": 0.05263157894736842}
{"task": "task688_mmmlu_answer_generation_college_computer_science.json", "shot": 5, "num": 57, "correct": 15, "acc": 0.2631578947368421}
{"task": "task729_mmmlu_answer_generation_professional_law.json", "shot": 0, "num": 151, "correct": 11, "acc": 0.0728476821192053}
{"task": "task729_mmmlu_answer_generation_professional_law.json", "shot": 5, "num": 151, "correct": 41, "acc": 0.271523178807947}
{"task": "task615_moviesqa_answer_generation.json", "shot": 0, "num": 3250, "correct": 298, "acc": 0.0916923076923077}
{"task": "task615_moviesqa_answer_generation.json", "shot": 5, "num": 3250, "correct": 459, "acc": 0.14123076923076924}
{"task": "task722_mmmlu_answer_generation_random_topic.json", "shot": 0, "num": 119, "correct": 47, "acc": 0.3949579831932773}
{"task": "task722_mmmlu_answer_generation_random_topic.json", "shot": 5, "num": 119, "correct": 70, "acc": 0.5882352941176471}
{"task": "task494_review_polarity_answer_generation.json", "shot": 0, "num": 2508, "correct": 982, "acc": 0.3915470494417863}
{"task": "task494_review_polarity_answer_generation.json", "shot": 5, "num": 2508, "correct": 1568, "acc": 0.6251993620414673}
{"task": "task072_abductivenli_answer_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task072_abductivenli_answer_generation.json", "shot": 5, "num": 3250, "correct": 9, "acc": 0.002769230769230769}
{"task": "task899_freebase_qa_topic_generation.json", "shot": 0, "num": 3248, "correct": 0, "acc": 0.0}
{"task": "task899_freebase_qa_topic_generation.json", "shot": 5, "num": 3248, "correct": 646, "acc": 0.19889162561576354}
{"task": "task1420_mathqa_general.json", "shot": 0, "num": 3226, "correct": 0, "acc": 0.0}
{"task": "task1420_mathqa_general.json", "shot": 5, "num": 3226, "correct": 560, "acc": 0.1735895846249225}
{"task": "task875_emotion_classification.json", "shot": 0, "num": 1615, "correct": 0, "acc": 0.0}
{"task": "task875_emotion_classification.json", "shot": 5, "num": 1615, "correct": 677, "acc": 0.41919504643962846}
{"task": "task1369_healthfact_sentence_generation.json", "shot": 5, "num": 1661, "correct": 0, "acc": 0.0}
{"task": "task1369_healthfact_sentence_generation.json", "shot": 0, "num": 1661, "correct": 0, "acc": 0.0}
{"task": "task858_inquisitive_span_detection.json", "shot": 0, "num": 81, "correct": 0, "acc": 0.0}
{"task": "task858_inquisitive_span_detection.json", "shot": 5, "num": 81, "correct": 10, "acc": 0.12345679012345678}
{"task": "task1507_boolean_temporal_reasoning.json", "shot": 0, "num": 3230, "correct": 1597, "acc": 0.49442724458204335}
{"task": "task1507_boolean_temporal_reasoning.json", "shot": 5, "num": 3230, "correct": 1796, "acc": 0.5560371517027863}
{"task": "task584_udeps_eng_fine_pos_tagging.json", "shot": 0, "num": 2990, "correct": 0, "acc": 0.0}
{"task": "task584_udeps_eng_fine_pos_tagging.json", "shot": 5, "num": 2990, "correct": 267, "acc": 0.08929765886287626}
{"task": "task854_hippocorpus_classification.json", "shot": 0, "num": 60, "correct": 0, "acc": 0.0}
{"task": "task854_hippocorpus_classification.json", "shot": 5, "num": 60, "correct": 17, "acc": 0.2833333333333333}
{"task": "task163_count_words_ending_with_letter.json", "shot": 0, "num": 2881, "correct": 104, "acc": 0.03609857688302673}
{"task": "task163_count_words_ending_with_letter.json", "shot": 5, "num": 2881, "correct": 1050, "acc": 0.3644567858382506}
{"task": "task1378_quarel_correct_answer_generation.json", "shot": 0, "num": 399, "correct": 0, "acc": 0.0}
{"task": "task1378_quarel_correct_answer_generation.json", "shot": 5, "num": 399, "correct": 173, "acc": 0.43358395989974935}
{"task": "task1479_organization_entity_extraction_btc_corpus.json", "shot": 0, "num": 100, "correct": 0, "acc": 0.0}
{"task": "task1479_organization_entity_extraction_btc_corpus.json", "shot": 5, "num": 100, "correct": 53, "acc": 0.53}
{"task": "task1721_civil_comments_obscenity_classification.json", "shot": 0, "num": 500, "correct": 3, "acc": 0.006}
{"task": "task1721_civil_comments_obscenity_classification.json", "shot": 5, "num": 500, "correct": 229, "acc": 0.458}
{"task": "task1443_string_to_number.json", "shot": 0, "num": 3249, "correct": 0, "acc": 0.0}
{"task": "task1443_string_to_number.json", "shot": 5, "num": 3249, "correct": 349, "acc": 0.10741766697445368}
{"task": "task295_semeval_2020_task4_commonsense_reasoning.json", "shot": 0, "num": 2998, "correct": 0, "acc": 0.0}
{"task": "task295_semeval_2020_task4_commonsense_reasoning.json", "shot": 5, "num": 2998, "correct": 1201, "acc": 0.40060040026684457}
{"task": "task1726_mathqa_correct_answer_generation.json", "shot": 0, "num": 2238, "correct": 0, "acc": 0.0}
{"task": "task1726_mathqa_correct_answer_generation.json", "shot": 5, "num": 2238, "correct": 38, "acc": 0.016979445933869526}
{"task": "task1380_quarel_correct_option_generation.json", "shot": 0, "num": 399, "correct": 0, "acc": 0.0}
{"task": "task1380_quarel_correct_option_generation.json", "shot": 5, "num": 399, "correct": 226, "acc": 0.5664160401002506}
{"task": "task928_yelp_positive_to_negative_style_transfer.json", "shot": 0, "num": 246, "correct": 0, "acc": 0.0}
{"task": "task928_yelp_positive_to_negative_style_transfer.json", "shot": 5, "num": 246, "correct": 0, "acc": 0.0}
{"task": "task1593_yahoo_answers_topics_classification.json", "shot": 0, "num": 497, "correct": 0, "acc": 0.0}
{"task": "task1593_yahoo_answers_topics_classification.json", "shot": 5, "num": 497, "correct": 97, "acc": 0.19517102615694165}
{"task": "task933_wiki_auto_style_transfer.json", "shot": 0, "num": 882, "correct": 0, "acc": 0.0}
{"task": "task933_wiki_auto_style_transfer.json", "shot": 5, "num": 882, "correct": 2, "acc": 0.0022675736961451248}
{"task": "task594_sciq_question_generation.json", "shot": 0, "num": 3250, "correct": 0, "acc": 0.0}
{"task": "task594_sciq_question_generation.json", "shot": 5, "num": 3250, "correct": 84, "acc": 0.025846153846153845}
{"task": "task1599_smcalflow_classification.json", "shot": 0, "num": 1905, "correct": 1191, "acc": 0.6251968503937008}
{"task": "task1599_smcalflow_classification.json", "shot": 5, "num": 1905, "correct": 1039, "acc": 0.5454068241469816}
{"task": "task1168_brown_coarse_pos_tagging.json", "shot": 0, "num": 3237, "correct": 0, "acc": 0.0}
{"task": "task1168_brown_coarse_pos_tagging.json", "shot": 5, "num": 3237, "correct": 1487, "acc": 0.45937596540006176}

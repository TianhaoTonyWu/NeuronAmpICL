{"mod": "LAPE", "n_shot": 0, "dataset": "sni", "task": "task242", "multiplier": 2.0, "percentage": 0.35, "num": 2997, "correct": 695, "acc": 0.23189856523189856}
{"mod": "GV", "n_shot": 0, "dataset": "sni", "task": "task242", "multiplier": 1.6, "percentage": 0.5, "num": 2997, "correct": 16, "acc": 0.005338672005338672}
{"mod": "GV_last", "n_shot": 0, "dataset": "sni", "task": "task242", "multiplier": 1.5, "percentage": 0.5, "num": 2997, "correct": 815, "acc": 0.2719386052719386}
{"mod": "GV_final", "n_shot": 0, "dataset": "sni", "task": "task242", "multiplier": 1.5, "percentage": 0.25, "num": 2997, "correct": 595, "acc": 0.19853186519853186}

{"mod": "LAPE", "n_shot": 0, "dataset": "sni", "task": "task274", "multiplier": 1.4, "percentage": 0.1, "num": 1198, "correct": 17, "acc": 0.014190317195325543}
{"mod": "GV", "n_shot": 0, "dataset": "sni", "task": "task274", "multiplier": 1.8, "percentage": 0.05, "num": 1198, "correct": 458, "acc": 0.3823038397328882}
{"mod": "GV_last", "n_shot": 0, "dataset": "sni", "task": "task274", "multiplier": 2.0, "percentage": 0.1, "num": 1198, "correct": 170, "acc": 0.1419031719532554}
{"mod": "GV_final", "n_shot": 0, "dataset": "sni", "task": "task274", "multiplier": 1.9, "percentage": 0.1, "num": 1198, "correct": 291, "acc": 0.24290484140233723}

{"mod": "LAPE", "n_shot": 0, "dataset": "sni", "task": "task1447", "multiplier": 2.1, "percentage": 0.1, "num": 1359, "correct": 66, "acc": 0.04856512141280353}
{"mod": "GV", "n_shot": 0, "dataset": "sni", "task": "task1447", "multiplier": 1.3, "percentage": 0.25, "num": 1359, "correct": 38, "acc": 0.027961736571008096}
{"mod": "GV_last", "n_shot": 0, "dataset": "sni", "task": "task1447", "multiplier": 2.5, "percentage": 0.05, "num": 1359, "correct": 588, "acc": 0.4326710816777042}
{"mod": "GV_final", "n_shot": 0, "dataset": "sni", "task": "task1447", "multiplier": 1.9, "percentage": 0.05, "num": 1359, "correct": 214, "acc": 0.15746872700515085}

{"mod": "LAPE", "n_shot": 0, "dataset": "sni", "task": "task403", "multiplier": 1.1, "percentage": 0.25, "num": 3004, "correct": 340, "acc": 0.11318242343541944}
{"mod": "GV", "n_shot": 0, "dataset": "sni", "task": "task403", "multiplier": 2.0, "percentage": 0.15, "num": 3004, "correct": 572, "acc": 0.1904127829560586}
{"mod": "GV_last", "n_shot": 0, "dataset": "sni", "task": "task403", "multiplier": 1.1, "percentage": 0.4, "num": 3004, "correct": 257, "acc": 0.0855525965379494}
{"mod": "GV_final", "n_shot": 0, "dataset": "sni", "task": "task403", "multiplier": 1.1, "percentage": 0.5, "num": 3004, "correct": 242, "acc": 0.08055925432756325}

{"mod": "LAPE", "n_shot": 0, "dataset": "sni", "task": "task645", "multiplier": 2.2, "percentage": 0.15, "num": 1000, "correct": 14, "acc": 0.014}
{"mod": "GV", "n_shot": 0, "dataset": "sni", "task": "task645", "multiplier": 2.4, "percentage": 0.15, "num": 1000, "correct": 149, "acc": 0.149}
{"mod": "GV_last", "n_shot": 0, "dataset": "sni", "task": "task645", "multiplier": 1.6, "percentage": 0.15, "num": 1000, "correct": 64, "acc": 0.064}
{"mod": "GV_final", "n_shot": 0, "dataset": "sni", "task": "task645", "multiplier": 2.2, "percentage": 0.05, "num": 1000, "correct": 93, "acc": 0.093}

{"mod": "LAPE", "n_shot": 0, "dataset": "sni", "task": "task475", "multiplier": 2.0, "percentage": 0.05, "num": 3250, "correct": 1316, "acc": 0.40492307692307694}
{"mod": "GV", "n_shot": 0, "dataset": "sni", "task": "task475", "multiplier": 1.6, "percentage": 0.15, "num": 3250, "correct": 445, "acc": 0.13692307692307693}
{"mod": "GV_last", "n_shot": 0, "dataset": "sni", "task": "task475", "multiplier": 1.6, "percentage": 0.2, "num": 3250, "correct": 573, "acc": 0.1763076923076923}
{"mod": "GV_final", "n_shot": 0, "dataset": "sni", "task": "task475", "multiplier": 1.5, "percentage": 0.25, "num": 3250, "correct": 564, "acc": 0.17353846153846153}

{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task242", "multiplier": 1.7, "percentage": 0.05, "num": 2997, "correct": 43, "acc": 0.01434768101434768}
{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task242", "multiplier": 1.7, "percentage": 0.05, "num": 2997, "correct": 65, "acc": 0.021688355021688355}
{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task242", "multiplier": 1.7, "percentage": 0.05, "num": 2997, "correct": 1, "acc": 0.000333667000333667}

{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task274", "multiplier": 1.7, "percentage": 0.05, "num": 1198, "correct": 39, "acc": 0.0325542570951586}
{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task274", "multiplier": 1.7, "percentage": 0.05, "num": 1198, "correct": 5, "acc": 0.004173622704507512}
{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task274", "multiplier": 1.7, "percentage": 0.05, "num": 1198, "correct": 0, "acc": 0.0}

{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task1447", "multiplier": 1.7, "percentage": 0.05, "num": 1359, "correct": 17, "acc": 0.012509197939661517}
{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task1447", "multiplier": 1.7, "percentage": 0.05, "num": 1359, "correct": 2, "acc": 0.0014716703458425313}
{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task1447", "multiplier": 1.7, "percentage": 0.05, "num": 1359, "correct": 4, "acc": 0.0029433406916850625}

{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task403", "multiplier": 1.7, "percentage": 0.05, "num": 3004, "correct": 616, "acc": 0.20505992010652463}
{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task403", "multiplier": 1.7, "percentage": 0.05, "num": 3004, "correct": 452, "acc": 0.15046604527296936}
{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task403", "multiplier": 1.7, "percentage": 0.05, "num": 3004, "correct": 125, "acc": 0.04161118508655127}

{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task645", "multiplier": 1.7, "percentage": 0.05, "num": 1000, "correct": 2, "acc": 0.002}
{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task645", "multiplier": 1.7, "percentage": 0.05, "num": 1000, "correct": 1, "acc": 0.001}
{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task645", "multiplier": 1.7, "percentage": 0.05, "num": 1000, "correct": 0, "acc": 0.0}

{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task475", "multiplier": 1.7, "percentage": 0.05, "num": 3250, "correct": 364, "acc": 0.112}
{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task475", "multiplier": 1.7, "percentage": 0.05, "num": 3250, "correct": 25, "acc": 0.007692307692307693}
{"mod": "Random", "n_shot": 0, "dataset": "sni", "task": "task475", "multiplier": 1.7, "percentage": 0.05, "num": 3250, "correct": 54, "acc": 0.016615384615384615}

{"mod": "Random", "n_shot": 0, "dataset": "gsm", "task": "ALL", "multiplier": 1.7, "percentage": 0.05, "num": 1319, "correct": 4, "acc": 0.003032600454890068}
{"mod": "Random", "n_shot": 0, "dataset": "gsm", "task": "ALL", "multiplier": 1.7, "percentage": 0.05, "num": 1319, "correct": 5, "acc": 0.0037907505686125853}
{"mod": "Random", "n_shot": 0, "dataset": "gsm", "task": "ALL", "multiplier": 1.7, "percentage": 0.05, "num": 1319, "correct": 26, "acc": 0.019711902956785442}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "abstract_algebra", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 1, "acc": 0.02}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "abstract_algebra", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 5, "acc": 0.1}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "abstract_algebra", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 9, "acc": 0.18}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "anatomy", "multiplier": 1.7, "percentage": 0.05, "num": 68, "correct": 22, "acc": 0.3235294117647059}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "anatomy", "multiplier": 1.7, "percentage": 0.05, "num": 68, "correct": 29, "acc": 0.4264705882352941}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "anatomy", "multiplier": 1.7, "percentage": 0.05, "num": 68, "correct": 28, "acc": 0.4117647058823529}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "astronomy", "multiplier": 1.7, "percentage": 0.05, "num": 76, "correct": 19, "acc": 0.25}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "astronomy", "multiplier": 1.7, "percentage": 0.05, "num": 76, "correct": 24, "acc": 0.3157894736842105}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "astronomy", "multiplier": 1.7, "percentage": 0.05, "num": 76, "correct": 16, "acc": 0.21052631578947367}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "business_ethics", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 12, "acc": 0.24}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "business_ethics", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 15, "acc": 0.3}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "business_ethics", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 22, "acc": 0.44}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "clinical_knowledge", "multiplier": 1.7, "percentage": 0.05, "num": 133, "correct": 40, "acc": 0.3007518796992481}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "clinical_knowledge", "multiplier": 1.7, "percentage": 0.05, "num": 133, "correct": 57, "acc": 0.42857142857142855}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "clinical_knowledge", "multiplier": 1.7, "percentage": 0.05, "num": 133, "correct": 37, "acc": 0.2781954887218045}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_biology", "multiplier": 1.7, "percentage": 0.05, "num": 72, "correct": 25, "acc": 0.3472222222222222}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_biology", "multiplier": 1.7, "percentage": 0.05, "num": 72, "correct": 21, "acc": 0.2916666666666667}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_biology", "multiplier": 1.7, "percentage": 0.05, "num": 72, "correct": 25, "acc": 0.3472222222222222}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_chemistry", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 6, "acc": 0.12}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_chemistry", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 8, "acc": 0.16}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_chemistry", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 3, "acc": 0.06}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_computer_science", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 15, "acc": 0.3}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_computer_science", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 10, "acc": 0.2}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_computer_science", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 14, "acc": 0.28}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_mathematics", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 4, "acc": 0.08}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_mathematics", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 9, "acc": 0.18}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_mathematics", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 15, "acc": 0.3}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_medicine", "multiplier": 1.7, "percentage": 0.05, "num": 87, "correct": 20, "acc": 0.22988505747126436}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_medicine", "multiplier": 1.7, "percentage": 0.05, "num": 87, "correct": 14, "acc": 0.16091954022988506}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_medicine", "multiplier": 1.7, "percentage": 0.05, "num": 87, "correct": 24, "acc": 0.27586206896551724}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_physics", "multiplier": 1.7, "percentage": 0.05, "num": 51, "correct": 6, "acc": 0.11764705882352941}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_physics", "multiplier": 1.7, "percentage": 0.05, "num": 51, "correct": 3, "acc": 0.058823529411764705}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "college_physics", "multiplier": 1.7, "percentage": 0.05, "num": 51, "correct": 4, "acc": 0.0784313725490196}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "computer_security", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 18, "acc": 0.36}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "computer_security", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 20, "acc": 0.4}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "computer_security", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 25, "acc": 0.5}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "conceptual_physics", "multiplier": 1.7, "percentage": 0.05, "num": 118, "correct": 23, "acc": 0.19491525423728814}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "conceptual_physics", "multiplier": 1.7, "percentage": 0.05, "num": 118, "correct": 42, "acc": 0.3559322033898305}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "conceptual_physics", "multiplier": 1.7, "percentage": 0.05, "num": 118, "correct": 29, "acc": 0.2457627118644068}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "econometrics", "multiplier": 1.7, "percentage": 0.05, "num": 57, "correct": 14, "acc": 0.24561403508771928}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "econometrics", "multiplier": 1.7, "percentage": 0.05, "num": 57, "correct": 9, "acc": 0.15789473684210525}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "econometrics", "multiplier": 1.7, "percentage": 0.05, "num": 57, "correct": 9, "acc": 0.15789473684210525}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "electrical_engineering", "multiplier": 1.7, "percentage": 0.05, "num": 73, "correct": 6, "acc": 0.0821917808219178}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "electrical_engineering", "multiplier": 1.7, "percentage": 0.05, "num": 73, "correct": 7, "acc": 0.0958904109589041}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "electrical_engineering", "multiplier": 1.7, "percentage": 0.05, "num": 73, "correct": 20, "acc": 0.273972602739726}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "elementary_mathematics", "multiplier": 1.7, "percentage": 0.05, "num": 189, "correct": 34, "acc": 0.17989417989417988}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "elementary_mathematics", "multiplier": 1.7, "percentage": 0.05, "num": 189, "correct": 42, "acc": 0.2222222222222222}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "elementary_mathematics", "multiplier": 1.7, "percentage": 0.05, "num": 189, "correct": 17, "acc": 0.08994708994708994}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "formal_logic", "multiplier": 1.7, "percentage": 0.05, "num": 63, "correct": 3, "acc": 0.047619047619047616}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "formal_logic", "multiplier": 1.7, "percentage": 0.05, "num": 63, "correct": 7, "acc": 0.1111111111111111}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "formal_logic", "multiplier": 1.7, "percentage": 0.05, "num": 63, "correct": 15, "acc": 0.23809523809523808}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "global_facts", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 20, "acc": 0.4}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "global_facts", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 3, "acc": 0.06}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "global_facts", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 2, "acc": 0.04}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_biology", "multiplier": 1.7, "percentage": 0.05, "num": 155, "correct": 50, "acc": 0.3225806451612903}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_biology", "multiplier": 1.7, "percentage": 0.05, "num": 155, "correct": 44, "acc": 0.2838709677419355}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_biology", "multiplier": 1.7, "percentage": 0.05, "num": 155, "correct": 50, "acc": 0.3225806451612903}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_chemistry", "multiplier": 1.7, "percentage": 0.05, "num": 102, "correct": 19, "acc": 0.18627450980392157}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_chemistry", "multiplier": 1.7, "percentage": 0.05, "num": 102, "correct": 17, "acc": 0.16666666666666666}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_chemistry", "multiplier": 1.7, "percentage": 0.05, "num": 102, "correct": 15, "acc": 0.14705882352941177}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_computer_science", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 11, "acc": 0.22}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_computer_science", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 12, "acc": 0.24}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_computer_science", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 11, "acc": 0.22}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_european_history", "multiplier": 1.7, "percentage": 0.05, "num": 83, "correct": 31, "acc": 0.37349397590361444}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_european_history", "multiplier": 1.7, "percentage": 0.05, "num": 83, "correct": 33, "acc": 0.39759036144578314}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_european_history", "multiplier": 1.7, "percentage": 0.05, "num": 83, "correct": 35, "acc": 0.42168674698795183}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_geography", "multiplier": 1.7, "percentage": 0.05, "num": 99, "correct": 29, "acc": 0.29292929292929293}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_geography", "multiplier": 1.7, "percentage": 0.05, "num": 99, "correct": 39, "acc": 0.3939393939393939}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_geography", "multiplier": 1.7, "percentage": 0.05, "num": 99, "correct": 28, "acc": 0.2828282828282828}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_government_and_politics", "multiplier": 1.7, "percentage": 0.05, "num": 97, "correct": 40, "acc": 0.41237113402061853}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_government_and_politics", "multiplier": 1.7, "percentage": 0.05, "num": 97, "correct": 32, "acc": 0.32989690721649484}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_government_and_politics", "multiplier": 1.7, "percentage": 0.05, "num": 97, "correct": 24, "acc": 0.24742268041237114}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_macroeconomics", "multiplier": 1.7, "percentage": 0.05, "num": 195, "correct": 46, "acc": 0.2358974358974359}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_macroeconomics", "multiplier": 1.7, "percentage": 0.05, "num": 195, "correct": 32, "acc": 0.1641025641025641}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_macroeconomics", "multiplier": 1.7, "percentage": 0.05, "num": 195, "correct": 32, "acc": 0.1641025641025641}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_mathematics", "multiplier": 1.7, "percentage": 0.05, "num": 135, "correct": 8, "acc": 0.05925925925925926}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_mathematics", "multiplier": 1.7, "percentage": 0.05, "num": 135, "correct": 23, "acc": 0.17037037037037037}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_mathematics", "multiplier": 1.7, "percentage": 0.05, "num": 135, "correct": 25, "acc": 0.18518518518518517}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_microeconomics", "multiplier": 1.7, "percentage": 0.05, "num": 119, "correct": 22, "acc": 0.18487394957983194}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_microeconomics", "multiplier": 1.7, "percentage": 0.05, "num": 119, "correct": 17, "acc": 0.14285714285714285}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_microeconomics", "multiplier": 1.7, "percentage": 0.05, "num": 119, "correct": 21, "acc": 0.17647058823529413}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_physics", "multiplier": 1.7, "percentage": 0.05, "num": 76, "correct": 5, "acc": 0.06578947368421052}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_physics", "multiplier": 1.7, "percentage": 0.05, "num": 76, "correct": 9, "acc": 0.11842105263157894}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_physics", "multiplier": 1.7, "percentage": 0.05, "num": 76, "correct": 12, "acc": 0.15789473684210525}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_psychology", "multiplier": 1.7, "percentage": 0.05, "num": 273, "correct": 60, "acc": 0.21978021978021978}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_psychology", "multiplier": 1.7, "percentage": 0.05, "num": 273, "correct": 93, "acc": 0.34065934065934067}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_psychology", "multiplier": 1.7, "percentage": 0.05, "num": 273, "correct": 61, "acc": 0.22344322344322345}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_statistics", "multiplier": 1.7, "percentage": 0.05, "num": 108, "correct": 9, "acc": 0.08333333333333333}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_statistics", "multiplier": 1.7, "percentage": 0.05, "num": 108, "correct": 7, "acc": 0.06481481481481481}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_statistics", "multiplier": 1.7, "percentage": 0.05, "num": 108, "correct": 10, "acc": 0.09259259259259259}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_us_history", "multiplier": 1.7, "percentage": 0.05, "num": 102, "correct": 25, "acc": 0.24509803921568626}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_us_history", "multiplier": 1.7, "percentage": 0.05, "num": 102, "correct": 29, "acc": 0.28431372549019607}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_us_history", "multiplier": 1.7, "percentage": 0.05, "num": 102, "correct": 38, "acc": 0.37254901960784315}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_world_history", "multiplier": 1.7, "percentage": 0.05, "num": 119, "correct": 38, "acc": 0.31932773109243695}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_world_history", "multiplier": 1.7, "percentage": 0.05, "num": 119, "correct": 42, "acc": 0.35294117647058826}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "high_school_world_history", "multiplier": 1.7, "percentage": 0.05, "num": 119, "correct": 46, "acc": 0.3865546218487395}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "human_aging", "multiplier": 1.7, "percentage": 0.05, "num": 112, "correct": 17, "acc": 0.15178571428571427}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "human_aging", "multiplier": 1.7, "percentage": 0.05, "num": 112, "correct": 12, "acc": 0.10714285714285714}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "human_aging", "multiplier": 1.7, "percentage": 0.05, "num": 112, "correct": 52, "acc": 0.4642857142857143}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "human_sexuality", "multiplier": 1.7, "percentage": 0.05, "num": 66, "correct": 5, "acc": 0.07575757575757576}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "human_sexuality", "multiplier": 1.7, "percentage": 0.05, "num": 66, "correct": 7, "acc": 0.10606060606060606}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "human_sexuality", "multiplier": 1.7, "percentage": 0.05, "num": 66, "correct": 17, "acc": 0.25757575757575757}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "international_law", "multiplier": 1.7, "percentage": 0.05, "num": 61, "correct": 26, "acc": 0.4262295081967213}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "international_law", "multiplier": 1.7, "percentage": 0.05, "num": 61, "correct": 26, "acc": 0.4262295081967213}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "international_law", "multiplier": 1.7, "percentage": 0.05, "num": 61, "correct": 25, "acc": 0.4098360655737705}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "jurisprudence", "multiplier": 1.7, "percentage": 0.05, "num": 54, "correct": 19, "acc": 0.35185185185185186}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "jurisprudence", "multiplier": 1.7, "percentage": 0.05, "num": 54, "correct": 25, "acc": 0.46296296296296297}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "jurisprudence", "multiplier": 1.7, "percentage": 0.05, "num": 54, "correct": 19, "acc": 0.35185185185185186}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "logical_fallacies", "multiplier": 1.7, "percentage": 0.05, "num": 82, "correct": 16, "acc": 0.1951219512195122}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "logical_fallacies", "multiplier": 1.7, "percentage": 0.05, "num": 82, "correct": 13, "acc": 0.15853658536585366}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "logical_fallacies", "multiplier": 1.7, "percentage": 0.05, "num": 82, "correct": 25, "acc": 0.3048780487804878}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "machine_learning", "multiplier": 1.7, "percentage": 0.05, "num": 56, "correct": 7, "acc": 0.125}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "machine_learning", "multiplier": 1.7, "percentage": 0.05, "num": 56, "correct": 15, "acc": 0.26785714285714285}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "machine_learning", "multiplier": 1.7, "percentage": 0.05, "num": 56, "correct": 6, "acc": 0.10714285714285714}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "management", "multiplier": 1.7, "percentage": 0.05, "num": 52, "correct": 14, "acc": 0.2692307692307692}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "management", "multiplier": 1.7, "percentage": 0.05, "num": 52, "correct": 22, "acc": 0.4230769230769231}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "management", "multiplier": 1.7, "percentage": 0.05, "num": 52, "correct": 14, "acc": 0.2692307692307692}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "marketing", "multiplier": 1.7, "percentage": 0.05, "num": 117, "correct": 65, "acc": 0.5555555555555556}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "marketing", "multiplier": 1.7, "percentage": 0.05, "num": 117, "correct": 26, "acc": 0.2222222222222222}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "marketing", "multiplier": 1.7, "percentage": 0.05, "num": 117, "correct": 34, "acc": 0.2905982905982906}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "medical_genetics", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 9, "acc": 0.18}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "medical_genetics", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 5, "acc": 0.1}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "medical_genetics", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 20, "acc": 0.4}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "miscellaneous", "multiplier": 1.7, "percentage": 0.05, "num": 392, "correct": 213, "acc": 0.5433673469387755}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "miscellaneous", "multiplier": 1.7, "percentage": 0.05, "num": 392, "correct": 133, "acc": 0.3392857142857143}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "miscellaneous", "multiplier": 1.7, "percentage": 0.05, "num": 392, "correct": 190, "acc": 0.4846938775510204}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "moral_disputes", "multiplier": 1.7, "percentage": 0.05, "num": 173, "correct": 46, "acc": 0.2658959537572254}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "moral_disputes", "multiplier": 1.7, "percentage": 0.05, "num": 173, "correct": 59, "acc": 0.34104046242774566}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "moral_disputes", "multiplier": 1.7, "percentage": 0.05, "num": 173, "correct": 33, "acc": 0.1907514450867052}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "moral_scenarios", "multiplier": 1.7, "percentage": 0.05, "num": 448, "correct": 108, "acc": 0.24107142857142858}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "moral_scenarios", "multiplier": 1.7, "percentage": 0.05, "num": 448, "correct": 81, "acc": 0.18080357142857142}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "moral_scenarios", "multiplier": 1.7, "percentage": 0.05, "num": 448, "correct": 82, "acc": 0.18303571428571427}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "nutrition", "multiplier": 1.7, "percentage": 0.05, "num": 153, "correct": 27, "acc": 0.17647058823529413}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "nutrition", "multiplier": 1.7, "percentage": 0.05, "num": 153, "correct": 29, "acc": 0.1895424836601307}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "nutrition", "multiplier": 1.7, "percentage": 0.05, "num": 153, "correct": 26, "acc": 0.16993464052287582}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "philosophy", "multiplier": 1.7, "percentage": 0.05, "num": 156, "correct": 30, "acc": 0.19230769230769232}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "philosophy", "multiplier": 1.7, "percentage": 0.05, "num": 156, "correct": 57, "acc": 0.36538461538461536}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "philosophy", "multiplier": 1.7, "percentage": 0.05, "num": 156, "correct": 27, "acc": 0.17307692307692307}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "prehistory", "multiplier": 1.7, "percentage": 0.05, "num": 162, "correct": 61, "acc": 0.3765432098765432}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "prehistory", "multiplier": 1.7, "percentage": 0.05, "num": 162, "correct": 18, "acc": 0.1111111111111111}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "prehistory", "multiplier": 1.7, "percentage": 0.05, "num": 162, "correct": 22, "acc": 0.13580246913580246}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "professional_accounting", "multiplier": 1.7, "percentage": 0.05, "num": 141, "correct": 36, "acc": 0.2553191489361702}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "professional_accounting", "multiplier": 1.7, "percentage": 0.05, "num": 141, "correct": 38, "acc": 0.2695035460992908}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "professional_accounting", "multiplier": 1.7, "percentage": 0.05, "num": 141, "correct": 29, "acc": 0.20567375886524822}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "professional_law", "multiplier": 1.7, "percentage": 0.05, "num": 767, "correct": 163, "acc": 0.21251629726205998}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "professional_law", "multiplier": 1.7, "percentage": 0.05, "num": 767, "correct": 137, "acc": 0.17861799217731422}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "professional_law", "multiplier": 1.7, "percentage": 0.05, "num": 767, "correct": 166, "acc": 0.21642764015645372}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "professional_medicine", "multiplier": 1.7, "percentage": 0.05, "num": 136, "correct": 36, "acc": 0.2647058823529412}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "professional_medicine", "multiplier": 1.7, "percentage": 0.05, "num": 136, "correct": 34, "acc": 0.25}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "professional_medicine", "multiplier": 1.7, "percentage": 0.05, "num": 136, "correct": 36, "acc": 0.2647058823529412}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "professional_psychology", "multiplier": 1.7, "percentage": 0.05, "num": 306, "correct": 89, "acc": 0.2908496732026144}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "professional_psychology", "multiplier": 1.7, "percentage": 0.05, "num": 306, "correct": 72, "acc": 0.23529411764705882}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "professional_psychology", "multiplier": 1.7, "percentage": 0.05, "num": 306, "correct": 60, "acc": 0.19607843137254902}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "public_relations", "multiplier": 1.7, "percentage": 0.05, "num": 55, "correct": 19, "acc": 0.34545454545454546}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "public_relations", "multiplier": 1.7, "percentage": 0.05, "num": 55, "correct": 23, "acc": 0.41818181818181815}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "public_relations", "multiplier": 1.7, "percentage": 0.05, "num": 55, "correct": 13, "acc": 0.23636363636363636}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "security_studies", "multiplier": 1.7, "percentage": 0.05, "num": 123, "correct": 17, "acc": 0.13821138211382114}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "security_studies", "multiplier": 1.7, "percentage": 0.05, "num": 123, "correct": 25, "acc": 0.2032520325203252}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "security_studies", "multiplier": 1.7, "percentage": 0.05, "num": 123, "correct": 33, "acc": 0.2682926829268293}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "sociology", "multiplier": 1.7, "percentage": 0.05, "num": 101, "correct": 43, "acc": 0.42574257425742573}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "sociology", "multiplier": 1.7, "percentage": 0.05, "num": 101, "correct": 38, "acc": 0.37623762376237624}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "sociology", "multiplier": 1.7, "percentage": 0.05, "num": 101, "correct": 26, "acc": 0.25742574257425743}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "us_foreign_policy", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 23, "acc": 0.46}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "us_foreign_policy", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 19, "acc": 0.38}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "us_foreign_policy", "multiplier": 1.7, "percentage": 0.05, "num": 50, "correct": 14, "acc": 0.28}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "virology", "multiplier": 1.7, "percentage": 0.05, "num": 83, "correct": 26, "acc": 0.3132530120481928}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "virology", "multiplier": 1.7, "percentage": 0.05, "num": 83, "correct": 19, "acc": 0.2289156626506024}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "virology", "multiplier": 1.7, "percentage": 0.05, "num": 83, "correct": 27, "acc": 0.3253012048192771}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "world_religions", "multiplier": 1.7, "percentage": 0.05, "num": 86, "correct": 46, "acc": 0.5348837209302325}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "world_religions", "multiplier": 1.7, "percentage": 0.05, "num": 86, "correct": 15, "acc": 0.1744186046511628}
{"mod": "Random", "n_shot": 0, "dataset": "mmlu", "task": "world_religions", "multiplier": 1.7, "percentage": 0.05, "num": 86, "correct": 23, "acc": 0.26744186046511625}
